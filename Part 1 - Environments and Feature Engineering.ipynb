{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Only needed on colab or on a fresh setup\n",
    "# Won't work on Windows!\n",
    "!pip install -r requirements.txt\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environments and Feature Engineering\n",
    "\n",
    "As we will discuss later, there is a plethora of reinforcement learning frameworks, with a new library appearing almost each week.\n",
    "Fortunately, when it comes to environments, one standard is far more common than any other: openAI's gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we don't count the inbuilt environments, gym is a rather small package with a few simple helpers\n",
    "for creating custom environments.\n",
    "The documentation on these helpers is, somehow surprisingly, non-existent. Also, best practices for feature engineering and\n",
    "general processes concerning environments are not easy to come by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We suspect that this lack of best-practices and the general non-industrial focus of most RL tools\n",
    "can be attributed to a stark difference between the RL research and the RL-for-industry focuses:\n",
    "\n",
    "The main RL research goal is to find algorithms that perform well across multiple environments.\n",
    "Improvements in performance due to feature engineering or specific learning architectures are\n",
    "not the overarching aim of most researchers, and hence not a focus of most tools either. Instead, the tools\n",
    "focus on being well integrated with some setup of multiple environments and on clarity of the implementations\n",
    "(often sacrificing modularity because researchers are expected to modify the source code directly).\n",
    "\n",
    "On the other hand, the RL industry application goal is usually to solve some specific problem with\n",
    "the help of RL. Whether the solution of this problem is useful in other circumstances is of lesser\n",
    "importance. Thus, there is a need for feature engineering, for incorporating problem-specific tricks\n",
    "into the learning algorithms, for parallelization, for support of common logging and tracking frameworks\n",
    "as well as for improved training speed and so on. Moreover, in such projects the developers would\n",
    "typically not want to modify source code of a tool they are using.\n",
    "Unfortunately, these needs are rarely met by RL libraries that focus on the research question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook we introduce you to gym's basic interfaces and to strategies for feature engineering\n",
    "and reward shaping. As you will see, the software design is quite different to non-RL machine learning.\n",
    "The topics presented here will be relevant for almost any kind of RL project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Basic interaction with environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# rendering envs directly in notebook is recommended only on colab\n",
    "in_notebook = \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from gym.envs.classic_control import PendulumEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "from visualization import demo_model, ModelProtocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = PendulumEnv(g=10)\n",
    "env = TimeLimit(env, max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class PushRightModel(ModelProtocol):\n",
    "    def predict(self, observation: np.ndarray, **kwargs):\n",
    "        return np.array([env.max_torque]), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demo_model(env, PushRightModel(), num_steps=800, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One can easily define human-designed agents, they can serve as simple benchmarks, and it is often a good idea to start an RL project with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Manipulating environments\n",
    "\n",
    "Let us see what happens if we increase gravity and keep pushing to the right with the maximal torque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"initial g: \" + str(env.g))\n",
    "env.g = 20\n",
    "print(\"modified g: \" + str(env.g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demo_model(env, PushRightModel(), num_steps=800, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Huh, nothing changed? Gotcha!\n",
    "The reason is a very funny behaviour of gym wrappers that took me hours to find for the first time: *getattr* is overridden but *setattr* is not!\n",
    "This causes creation of a new attribute in the wrapper instance shadowing the name of the (still unchanged) attribute in the wrapped environment.\n",
    "\n",
    "The quick solution here: unwrap and adjust g. The real solution in custom environments is to define setters like\n",
    "\n",
    "```python\n",
    "    def set_g(self, g: float):\n",
    "        self.g = g\n",
    "```\n",
    "for all quantities of interest and never to set attributes directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.env.g = 20\n",
    "demo_model(env, PushRightModel(), num_steps=800, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Processing and enriching observations\n",
    "\n",
    "In non-RL machine learning, data processing is usually happening in an abstraction bound to the model. Examples for this are scikit-learn\n",
    "pipelines and similar architectures allowing the implementation of a `.predict()` method acting on raw or slightly processed inputs. Such models\n",
    "can then be easily deployed to user-facing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In RL on the other hand, data processing is bound to the environment and not the agent - everything happens with wrappers.\n",
    "Let's say, we want to add the square of the velocity to the observations. For that, we will use an observation wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "\n",
    "class AddThetadotSquaredWrapper(ObservationWrapper):\n",
    "    def __init__(self, env: PendulumEnv):\n",
    "        super().__init__(env)\n",
    "        high, low = list(env.observation_space.high), list(env.observation_space.low)\n",
    "        max_speed = high[-1]\n",
    "        high.append(max_speed ** 2)\n",
    "        low.append(0)\n",
    "        self.observation_space = Box(\n",
    "            low=np.array(low), high=np.array(high), dtype=env.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        thetadot = observation[-1]\n",
    "        result = list(observation)\n",
    "        result.append(thetadot ** 2)\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "extended_env = AddThetadotSquaredWrapper(env)\n",
    "extended_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This suggests the following architecture: one should have a basic environment that makes as few assumptions about data processing\n",
    "as possible and then do all processing through a combination of observation wrappers when the environment is instantiated.\n",
    "\n",
    "How to do the latter is not entirely straightforward. Since the wrappers contain an instance of the environment, they can be easily\n",
    "chained but not concatenated (i.e. executed simultaneously, possibly in parallel).\n",
    "It is thus not directly possible to create multiple feature extractors and select which ones should be\n",
    "used when the environment is created. However, this kind of flexible feature extractor design is often *crucial* for\n",
    "a project! Rarely do we know in advance which combination of features will work best for solving the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This problem can be circumvented by an architectural trick - we create an observation class not bound to\n",
    "the environment which can be concatenated and use it within a single observation wrapper. Here is a sketch\n",
    "for how such a software design could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "extended_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from typing import List\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ScalarObservation(ABC):\n",
    "    \"\"\"\n",
    "    Base class for observations based on greyscale images, e.g. as provided by ScanningEMEnv\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def observation_space(self) -> Box:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def observation(self, arr: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __str__(self):\n",
    "        name = self.__class__.__name__\n",
    "        if name.endswith(\"Observation\"):\n",
    "            name = name.replace(\"Observation\", \"\")\n",
    "        return name\n",
    "\n",
    "\n",
    "class ScalarObservationWrapper(ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Wraps an environment and an ScalarObservation into a single, gym-compatible object. See docu of\n",
    "    ScalarObservation for more details about the intended usage of this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        scalar_observation: ScalarObservation,\n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.scalar_observation = scalar_observation\n",
    "        self.observation_space = self.scalar_observation.observation_space\n",
    "\n",
    "    def observation(self, image: np.ndarray):\n",
    "        return self.scalar_observation.observation(image)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<{self.scalar_observation}{self.env}>\"\n",
    "\n",
    "\n",
    "def concatenate_boxes(boxes: List[Box]):\n",
    "    result_lows = []\n",
    "    result_highs = []\n",
    "    for b in boxes:\n",
    "        if len(b.shape) != 1:\n",
    "            raise ValueError(f\"Can only concatenate flat boxes but got shape {b.shape}\")\n",
    "        result_lows.extend(b.low)\n",
    "        result_highs.extend(b.high)\n",
    "    return Box(np.array(result_lows), np.array(result_highs))\n",
    "\n",
    "\n",
    "class ConcatenatedScalarObservation(ScalarObservation):\n",
    "    def __init__(self, *observations: ScalarObservation):\n",
    "        self.observations = observations\n",
    "        self._observation_space = concatenate_boxes(\n",
    "            [obs.observation_space for obs in self.observations]\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> Box:\n",
    "        return self._observation_space\n",
    "\n",
    "    def observation(self, image: np.ndarray) -> np.ndarray:\n",
    "        return np.concatenate([obs.observation(image) for obs in self.observations])\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"_\".join([obs.__str__() for obs in self.observations])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's look at this pattern in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BaseObservation(ScalarObservation):\n",
    "    @property\n",
    "    def observation_space(self) -> Box:\n",
    "        return env.observation_space\n",
    "\n",
    "    def observation(self, arr: np.ndarray) -> np.ndarray:\n",
    "        return arr\n",
    "\n",
    "\n",
    "class ThetadotSquaredObservation(ScalarObservation):\n",
    "    @property\n",
    "    def observation_space(self) -> Box:\n",
    "        return Box(low=0, high=env.max_speed ** 2, shape=(1,))\n",
    "\n",
    "    def observation(self, arr: np.ndarray):\n",
    "        thetadot = arr[-1]\n",
    "        return np.array([thetadot ** 2])\n",
    "\n",
    "\n",
    "class ThetadotCubeObservation(ScalarObservation):\n",
    "    @property\n",
    "    def observation_space(self) -> Box:\n",
    "        return Box(low=0, high=env.max_speed ** 3, shape=(1,))\n",
    "\n",
    "    def observation(self, arr: np.ndarray):\n",
    "        thetadot = arr[-1]\n",
    "        return np.array([thetadot ** 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "obs1 = BaseObservation()\n",
    "obs2 = ThetadotSquaredObservation()\n",
    "obs3 = ThetadotCubeObservation()\n",
    "\n",
    "full_obs = ConcatenatedScalarObservation(obs1, obs2, obs3)\n",
    "enhanced_env = ScalarObservationWrapper(env, full_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "enhanced_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "enhanced_env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reward shaping\n",
    "\n",
    "As discussed before, finding a good reward is often the most important as well as the most challenging part of an RL project.\n",
    "Therefore, a lot of engineering and experimenting with different rewards might be needed. Defining new rewards should be as painless\n",
    "as possible.\n",
    "\n",
    "One way to do reward engineering is to use gym's RewardWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym import RewardWrapper\n",
    "\n",
    "\n",
    "class CustomPendulumReward(RewardWrapper):\n",
    "    def reward(self, reward):\n",
    "        #      do something with the reward\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This has two downsides:\n",
    "\n",
    "    1. as with observations, reward wrappers cannot be concatenated\n",
    "    2. the reward of the base env will always be computed, which might be expensive\n",
    "\n",
    "The first downside might not be a problem if concatenation of different rewards is not something you need. However,\n",
    "the second point is more serious and has no solution within the realm of RewardWrappers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When reward computations are extracted from observations in a straightforward manner, modularity in reward shaping may be\n",
    "achieved by passing a `reward_function` to the environment. To get even more generality and maintainability, we propose using\n",
    "the [strategy pattern](https://en.wikipedia.org/wiki/Strategy_pattern) for injecting rewards into environments. Due to python's\n",
    "great flexibility, one can in fact have both at the same time. Below we sketch how the strategy pattern could look like for rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym.envs.classic_control.pendulum import angle_normalize\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "\n",
    "class PendulumRewardMetric(ABC):\n",
    "    \"\"\"\n",
    "    RewardMetrics follow a strategy pattern. They are injected into environments and are used for computing\n",
    "    the reward from an environment's current state.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, env: PendulumEnv, action: np.ndarray) -> float:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ModularPendulumEnv(PendulumEnv):\n",
    "    def __init__(\n",
    "        self, reward_metric: Callable[[PendulumEnv, np.ndarray], float] = None, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        if reward_metric is None:\n",
    "            reward_metric = ModularPendulumEnv._default_reward_metric\n",
    "        self.reward_metric = reward_metric\n",
    "        if hasattr(reward_metric, \"range\"):\n",
    "            self.reward_metric = reward_metric.range\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_reward_metric(env: PendulumEnv, action: np.ndarray):\n",
    "        th, thdot = env.state  # th := theta\n",
    "        u = action\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot ** 2 + 0.001 * (u ** 2)\n",
    "        return -costs\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        reward = self.reward_metric(self, u)\n",
    "\n",
    "        newthdot = (\n",
    "            thdot\n",
    "            + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3.0 / (m * l ** 2) * u) * dt\n",
    "        )\n",
    "        newth = th + newthdot * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "        return self._get_obs(), reward, False, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With this kind of flexibility, we can easily define and inject all kinds of rewards. For example, a sparse reward giving 1 only if\n",
    "the pendulum is within a certain angle range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WithinRegionRewardMetric(PendulumRewardMetric):\n",
    "    def __init__(self, theta_min: float, theta_max: float):\n",
    "        self.theta_max = theta_max\n",
    "        self.theta_min = theta_min\n",
    "\n",
    "    def __call__(self, env: PendulumEnv, action: np.ndarray) -> float:\n",
    "        theta, thetadot = env.state\n",
    "        return 0 if self.theta_min < theta < self.theta_max else -1\n",
    "\n",
    "    @property\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        return 0, 1\n",
    "\n",
    "\n",
    "within_region_reward = WithinRegionRewardMetric(-np.pi / 4, np.pi / 4)\n",
    "\n",
    "target_region_pendulum_env = ModularPendulumEnv(reward_metric=within_region_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The same kind of pattern can be applied to episode termination criteria, which sometimes form an important part of the environment.\n",
    "For the pendulum environment this is rather not the case, so we will not highlight them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Composite rewards (and some syntactic sugar)\n",
    "\n",
    "Once the strategy pattern is implemented, it is easy to build new reward metrics out of old ones because the rewards are callables with the same,\n",
    "fixed signature. Thus, they can be multiplied, added, negated and so on. Since python allows operator overloading, this becomes particularly\n",
    "neat. Here is a small example for implementing combinations of reward metrics.\n",
    "\n",
    "*NB*: At some point I got sad that operator overloading is only allowed for regular objects but not functions, and that despite functions being\n",
    "\"first class citizens\" in python... So I implemented a way around it by coercing functions into classes and thereby permitting operations on them\n",
    "(prior to execution, of course). Have a look at [this gist](https://gist.github.com/MischaPanch/30b25d82093cdef6577146af75badcff)\n",
    "if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from numbers import Number\n",
    "from typing import TypeVar, Generic, Sequence\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "EnvType = TypeVar(\"EnvType\", bound=gym.Env)\n",
    "\n",
    "\n",
    "class RewardMetric(ABC, Generic[EnvType]):\n",
    "    \"\"\"\n",
    "    RewardMetrics follow a strategy pattern. They are injected into environments and are used for computing\n",
    "    the reward from an environment's current state.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, env: EnvType, action: np.ndarray) -> float:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        pass\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Number):\n",
    "            other = ConstantRewardMetric(other)\n",
    "        if not isinstance(other, RewardMetric):\n",
    "            raise ValueError(\n",
    "                f\"Can only multiply a RewardMetric with another RewardMetric but got: {other.__class__.__name__}\"\n",
    "            )\n",
    "        return RewardMetricProduct([self, other])\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if not isinstance(other, RewardMetric):\n",
    "            raise ValueError(\n",
    "                f\"Can only add a RewardMetric to another RewardMetric but got: {other.__class__.__name__}\"\n",
    "            )\n",
    "        # two is necessary because RewardMetricSum creates a weighted sum. We need to undo that for expected behaviour\n",
    "        return 2 * RewardMetricSum([self, other])\n",
    "\n",
    "    def __abs__(self):\n",
    "        return AbsRewardMetricWrapper(self)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return (-1) * self\n",
    "\n",
    "\n",
    "class RewardMetricSum(RewardMetric):\n",
    "    def __call__(self, env: EnvType, action: np.ndarray) -> float:\n",
    "        result = 0\n",
    "        for metric, weight in zip(self.reward_metrics, self.weights):\n",
    "            if weight > 0:\n",
    "                result += weight * metric(env, action)\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        return self._range\n",
    "\n",
    "    def __init__(\n",
    "        self, reward_metrics: List[RewardMetric], weights: Sequence[float] = None\n",
    "    ):\n",
    "        self.reward_metrics = reward_metrics\n",
    "        n_metrics = len(reward_metrics)\n",
    "        weights = np.array(weights) if weights is not None else np.ones(n_metrics)\n",
    "        if not np.all(weights >= 0):\n",
    "            raise ValueError(f\"Weights should be greater equal zero but got: {weights}\")\n",
    "        self.weights = weights / np.sum(weights)\n",
    "        lower_range, upper_range = 0, 0\n",
    "        for metric, weight in zip(self.reward_metrics, self.weights):\n",
    "            if weight > 0:\n",
    "                lower, upper = metric.range\n",
    "                lower_range += weight * lower\n",
    "                upper_range += weight * upper\n",
    "            else:\n",
    "                log.debug(\n",
    "                    f\"Ignoring {metric.__class__.__name__} for sum of rewards since weight is 0\"\n",
    "                )\n",
    "        self._range = (lower_range, upper_range)\n",
    "\n",
    "\n",
    "class ConstantRewardMetric(RewardMetric):\n",
    "    def __init__(self, value: Number):\n",
    "        self.value = float(value)\n",
    "\n",
    "    def __call__(self, env: EnvType, action: np.ndarray):\n",
    "        return self.value\n",
    "\n",
    "    @property\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        return self.value, self.value\n",
    "\n",
    "\n",
    "class AbsRewardMetricWrapper(RewardMetric):\n",
    "    def __init__(self, reward_metric: RewardMetric):\n",
    "        self.reward_metric = reward_metric\n",
    "        low, high = self.reward_metric.range\n",
    "        new_low, new_high = sorted([abs(low), abs(high)])\n",
    "        self._range = (new_low, new_high)\n",
    "\n",
    "    def __call__(self, env: EnvType, action: np.ndarray) -> float:\n",
    "        return abs(self.reward_metric(env, action))\n",
    "\n",
    "    @property\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        return self._range\n",
    "\n",
    "\n",
    "class RewardMetricProduct(RewardMetric):\n",
    "    def __init__(self, reward_metrics: List[RewardMetric]):\n",
    "        self.reward_metrics = reward_metrics\n",
    "\n",
    "        # NOTE: finding the right range in the general case requires quite complicated logic b/c of changes in sign.\n",
    "        # E.g. multiplying rewards with ranges (-3, 0), (-1, 2) results in the range (-6, 3)\n",
    "        def get_product_range(range1, range2):\n",
    "            range1, range2 = np.array(range1), np.array(range2)\n",
    "            boundary_candidates_matrix = range1.reshape((2, 1)) * range2\n",
    "            return boundary_candidates_matrix.min(), boundary_candidates_matrix.max()\n",
    "\n",
    "        ranges = [m.range for m in reward_metrics]\n",
    "        self._range = reduce(get_product_range, ranges)\n",
    "\n",
    "    def __call__(self, env: EnvType, action: np.ndarray) -> float:\n",
    "        result = 1\n",
    "        for reward_metric in self.reward_metrics:\n",
    "            value = reward_metric(env, action)\n",
    "            if value == 0:\n",
    "                return 0\n",
    "            result *= value\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def range(self) -> Tuple[float, float]:\n",
    "        return self._range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Since this notebook was mainly an introduction to useful ideas for approaching an RL project, the constructions\n",
    "here can be applied in many situations. You could for example:\n",
    "\n",
    "    1. Make an abstract base environment class with modularity along the lines of `ModularPendulumEnv` permitting simple injection\n",
    "       of rewardsa as well as an injection episode termination criteria.\n",
    "    2. Extend the concatenation of observation to images. Implement this for an atari game - one could envision that adding\n",
    "       additional channels to the input (e.g. mirroring or passing through an edge detector) could boost performance of\n",
    "       RL agents acting directly on pixel\n",
    "    3. Try reproducing some gym environments with the modular interfaces\n",
    "    4. Think about how a curriculum could be added to an environment - an adaptation of the task based on time or on the agent's\n",
    "       current performance\n",
    "    5. Try to develop a scheme for creating environments, including observations, rewards and so on, from configuration\n",
    "       (json, yaml or some format of your choice). How would you approach it? Think about versioning of configuration schemas\n",
    "       and issues of backwards compatibility.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
