{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Offline-RL open source datasets </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Source Datasets for offline RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of offline RL or imitation learning is to learn a policy from a fixed dataset. This approach has gained significant attention because it allows RL methods to utilize vast, pre-collected datasets, somewhat similar to how large datasets have propelled advances in supervised learning.**\n",
    "\n",
    "Challenges:\n",
    "\n",
    "- Existing benchmarks designed for online RL are not well-suited for the offline RL \n",
    "  setting.\n",
    "- RL benchmarks relying on data generated by partially-trained agents or environment-\n",
    "  dependent are not robust.\n",
    "\n",
    "These challenges makes it difficult to accurately measure progress in offline RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINARI Dataset\n",
    "(previously called D4RL from UC Berkeley/Google Brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library is emerging as the standard in the field. While previously reliant on D4RL, the community is currently transitioning to Minari. Its scope is to address the gap of representative datasets in offline RL, by introducing open-source datasets explicitly crafted for the offline setting. These datasets are shaped by the critical properties that real-world applications of offline RL demand. Minari provides datasets collected with random, medium, and expert policies in some environments, allowing us to evaluate whether an algorithm can extract meaning from noise.\n",
    "\n",
    "In particular the provided datasets focus mainly on the following properties that appear often in realistic situations:\n",
    "\n",
    "\n",
    "1 - **Narrow and biased data distributions**: e.g. from deterministic policies: Narrow datasets may arise in human demonstrations or in hand-crafted policies.\n",
    "\n",
    "2 - **Undirected and multitask data**: Undirected in the sense that is not directed towards the specific task one is trying to accomplish. E.g.: recording user interactions on the internet or recording videos of a car for autonomous driving. The main purpose it to test how good is the offline agent to be used for \"trajectory stitching\", i.e. combining trajectories from different tasks to achieve new objectives, instead of searching for out-of-distribution trajectories.\n",
    "\n",
    "<img src=\"_static/images/stiching.png\" alt=\"stich_traj\" style=\"width:200px;\">\n",
    "\n",
    "3 - **Sparse rewards**: Sparse rewards are challenging in online settings due to their close correlation with exploration. In offline RL, we exclusively explore within the dataset, making it an ideal framework to study the algorithm's response to sparse rewards.\n",
    "Note that crafting effective rewards can be challenging, and overly complex rewards may inadvertently push solutions towards suboptimal outcomes. In contrast, designing sparse rewards is often more straightforward as it merely involves specifying the task's success criteria, making it an attractive property to work with.\n",
    "\n",
    "4 - **Suboptimal data**: Give a clear task the data could not contain any optimal trajectory so this is a realistic scenario in general and still the offline agent should be able to find a suboptimal solution.\n",
    "\n",
    "5 - **Non-representable behavior policies**: non-Markovian behavior policies, and partial observability. For instance, if the data is collected with a classical control algorithm that have access to a window of previous states.\n",
    "\n",
    "\n",
    "6 - **Realistic domains**: Different Mujoco tasks as robot manipulation or multi-tasking.\n",
    "\n",
    "Let's give a look to [Minari](https://minari.farama.org/main/content/basic_usage/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Unplugged dataset\n",
    "\n",
    "(Deep Mind - Google Brain) [website](https://www.deepmind.com/blog/rl-unplugged-benchmarks-for-offline-reinforcement-learning) and [blog](https://www.deepmind.com/blog/rl-unplugged-benchmarks-for-offline-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes a nice set of tasks, but the crucial point is that all datasets come from behavior policies trained online, so the collected data may not be representative of realistic situations where human experts and non-RL policies are typically used to collect data. Additionally, most of the data comes from medium to expert policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open X-Embodiment Repository\n",
    "October 2023 - Partners from 33 academic labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [library](https://robotics-transformer-x.github.io/) introduced the **Open X-Embodiment Repository** that includes a dataset with 22 different robot types for **X-embodiment learning**, i.e. to learn from diverse and large-scale datasets from multiple robots for better transfer learning and improved generalization.\n",
    "\n",
    "[Let's give a look](https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[ \\[Fu.Justin et. al. '2021 \\] D4RL: Datasets for Deep Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "\n",
    "[ MINARI: A dataset API for Offline Reinforcement Learning ](https://minari.farama.org/main/content/basic_usage/) \n",
    "\n",
    "[ C. Gulcehre et al. '2021, â€œRL unplugged: A suite of benchmarks for offline\n",
    "reinforcement learning](https://arxiv.org/abs/2006.13888)\n",
    "\n",
    "[ A. Padalkar et. al. '2023 Open X-Embodiment: Robotic Learning Datasets and RT-X Models ](https://robotics-transformer-x.github.io/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
