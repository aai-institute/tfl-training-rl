{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline RL challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of offline RL is simple: find the optimal policy from a given dataset. The dataset doesn't need to be collected from experts as in imitation learning and neither from the same task but in the last case it should be representative of our problem at hand. So offline RL covers most of the realistic scenarios, but also it could be applied to a lot of data coming from different sources as we saw before ....\n",
    "\n",
    "\n",
    "The key distinction between online RL and offline RL lies in their exploration capabilities. In online RL, we actively explore the state-action space, while offline RL operates solely within a fixed dataset (denoted as $D$). Going \"out of distribution\" beyond this dataset in offline RL can lead to severe issues.\n",
    "\n",
    "<img src=\"_static/imagesoffline_RL.jpg\" alt=\"offline_rl\" style=\"height:400px;\">\n",
    "\n",
    "Online RL involves interactive exploration to discover the highest-reward regions by gathering environmental feedback. In contrast, offline RL imposes strict limitations on exploration beyond the dataset. This constraint results in algorithms overestimating unknown areas and attempting to navigate beyond the dataset, as illustrated in the figure below where the dataset doesn't fully represent high-reward regions.\n",
    "\n",
    "<img src=\"_static/imagesoffline_RL_3.jpg\" alt=\"offline_rl_1\" style=\"height:400px;\">\n",
    "\n",
    "As shown in the figure above on the right, once you are out of distribution (o.o.d) (states $s$ and $s'$ in red in the figure), as you don't have any feedback it will be hard to come back to $D$, as the o.o.d errors will propagate. As we will see this is one of the main challenges of offline RL and there are different techniques to mitigate this wrong behavior.\n",
    "\n",
    "The o.o.d. issues are not the only distributional shift effect in offline RL.\n",
    "After computing the optimal policy, it typically operates within a subset of the original dataset distribution, creating a distinct form of distributional shift (D' subset in green in the figure below). Evaluating a policy substantially different from the behavior policy reduces the effective sample size (from D to D'), resulting in increased variance in the estimates. In simpler terms, the limited number of data points may not accurately represent the true data distribution. \n",
    "\n",
    "<img src=\"_static/imagesoffline_RL_2.jpg\" alt=\"offline_rl_2\" style=\"height:400px;\">\n",
    "\n",
    "\n",
    "Can we apply techniques from online RL, known for its effectiveness in solving complex problems, to offline RL? Yes, we can. We can adapt concepts from online RL, particularly off-policy RL algorithms.\n",
    "\n",
    "In both offline and off-policy RL, we train a policy using a batch of data from a replay buffer. The key difference is that in online RL, this data is constantly updated with new data generated by an improving policy through exploration. In offline RL, we cannot change our replay buffer; we must use the same dataset throughout training. This fundamental distinction in data collection and utilization presents both challenges and opportunities in offline RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1**:  Let's check how efficient are online algorithms with finite amount of data.\n",
    "\n",
    "\n",
    "Let's give a look to the exercise in notebook 6a. We will apply a simple offpolicy algorithm, Deep Q-Network (DQN), replacing the environment we typically use in online settings with a ReplyBuffer of state-action-rewards pairs that we will collect with two suboptimal policies that will be the case in realistic scenarios (i.e. the data could come from many different sources). \n",
    "\n",
    "**Important**: In contrast to Imitation Learning, now we will be interested in a reward function as in DQN we will compute the TD error, i.e.:\n",
    "\n",
    "Remember: \n",
    "\n",
    "$$ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\ldots \\mid s_0 = s, a_0 = a \\right]\n",
    "$$\n",
    "\n",
    "<img src=\"_static/imagesq_value_iterative.png\" alt=\"offline_rl\" style=\"height:200px;\">\n",
    "\n",
    "\n",
    "DQN is a simple method to reach the optimal policy by iteratively compute:\n",
    "\n",
    "$$ Q(s, a) = Q(s, a) + α * [r + γ * max_a'(Q(s', a')) - Q(s, a)]$$\n",
    "\n",
    "\n",
    "Let's go to the notebook exercise! ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - **Without too much data, online algorithms in general will struggle to find a good policy as they rely too much \n",
    "    on exploration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - **Why is DQN going out of distribution?** The reason is more general than DQN and is a pathology that affects many RL algorithms and in particular the ones following an approximate dynamical programming approach where an evaluation-improvement iterative process is used to reach the optimal policy, i.e.:\n",
    "\n",
    "$$\n",
    "{\\hat Q}^{\\pi}_{k+1} \\leftarrow \\arg \\min_Q \\mathbb{E}_{(s,a,s')\\sim D} \\left[ Q(s, a) - \\left( r(s, a) + \\gamma \\mathbb{E}_{a' \\sim\\pi_k(a'|s')}[{\\hat Q}^{\\pi}_k(s', a')] \\right)^2 \\right]  \\tag{Evaluation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}_{k+1}}(s, a) \\right] \\tag{Improvement}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that in an offline setup we restrict the states $s$, $s'$ and $a$ to belong to the dataset, $D$, but the action $a'$ sampled from the policy $\\pi_k$ in the evaluation step could be outside. If we don't have enough information around the state $s'$ (as is typically the case in offline RL) the uncertainty about those states could make $\\pi_k(a'|s')$ select actions not covered in $D$ and this error will propagate in the evaluation iteration process. In online RL such an overestimation will be fixed through exploration but this not an option in offline methods. **This is perhaps the most important distributional shift issue that suffers offline RL, as already mentioned in the introduction.**\n",
    "\n",
    "Note: We could have also DNN function approximation errors that could also produce overestimation of actions bringing the system to out of distribution data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE II**: Another desired property of offline RL, as discussed in the open source RL dataset section, is to reuse pieces of trajectories from different datasets in order to create an optimal one for the task in question!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
