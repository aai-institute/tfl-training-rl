{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:27.996795Z",
     "start_time": "2024-05-09T19:53:26.925119Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:29.266823Z",
     "start_time": "2024-05-09T19:53:29.254411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:29.481414Z",
     "start_time": "2024-05-09T19:53:29.471813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:29.780063Z",
     "start_time": "2024-05-09T19:53:29.765450Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:31.749589Z",
     "start_time": "2024-05-09T19:53:30.907833Z"
    }
   },
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "load_env_variables()\n",
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv, RenderMode, register_grid_envs, EnvFactory\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset \n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig, get_trained_policy_path\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import widget_list\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env)\n",
    "from training_rl.offline_rl.utils import load_buffer_minari\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import MinariDatasetConfig\n",
    "from training_rl.offline_rl.visualizations.utils import policy_rollout_torcs_env, compare_policy_decisions_vs_expert_suggestions\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import create_minari_datasets\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "register_grid_envs()\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Offline RL algorithms exercises </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Offline RL pipeline:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Offline RL algorithms exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Previously, we discussed that **off-policy methods cannot learn from data efficiently unless a significant amount of data covering a large portion of the environment states is available**. Only in such cases can the agent explore the environment and get feedback similar to what's done in an online approach. However, this scenario is rare and challenging to achieve in realistic applications, which is one of the reasons why we turn to offline RL, where only a small amount of data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also discussed one of the major issues when applying off-policy methods to collected data: the agent's tendency to go out-of-distribution (o.o.d.). More importantly, once it goes o.o.d., the policy becomes unpredictable, making it impossible to return to the in-distribution region. This unpredictability propagates errors in the policy evaluation process (i.e., the dynamic programming equations), destroying the algorithm's learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise I\n",
    "\n",
    "**Similar to our approach in the off-policy notebook (nb_95), we will collect a small amount of expert data and a larger amount of suboptimal data. We will then observe how two offline RL algorithms introduced earlier, BCQ and CQL, can recover the expert policy without going o.o.d. We will compare our results with the imitation learning approach, specifically the BC algorithm, which, as we discussed in the imitation learning section (nb_93), is another viable option when expert data is available.**\n",
    "\n",
    "\n",
    "In this exercise, we will collect two datasets: one with expert and another with suboptimal data. The goal of the agent will be to get as close as possible to the target.\n",
    "\n",
    "I - **expert policy**: collect ~ 1000 steps\n",
    "\n",
    "II  - **Suboptimal policy**:  collect ~ 2000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "obstacle_selected = widget_list([ObstacleTypes.obstacle_8x8_wall_with_door])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = obstacle_selected.value\n",
    "INITIAL_STATE = (7, 7)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Behavior policies and datasets configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_move_up_from_bottom_5_steps\n",
    "DATA_SET_IDENTIFIER_I = \"_expert_\"\n",
    "NUM_STEPS_I = 1000\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_move_left_with_noise\n",
    "DATA_SET_IDENTIFIER_II = \"_suboptimal_\"\n",
    "NUM_STEPS_II = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Collect data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_data_sets_offline_rl\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")\n",
    "\n",
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 3: Feed data into replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis\n",
    "\n",
    "Note that we have four peaks. The ones at (2,7) and (3,7) come from policy-I, which goes towards the target but stops before reaching it. The other two peaks at (6,0) and (7,0) are produced by policy-II, which drifts the agent to the left with noise. **It is important to notice that the amount of collected data at state (5,7) is very little, but this state is crucial if we want to approach the target.**\n",
    "\n",
    "What do you think a BC algorithm would do? What about an offline one?\n",
    "\n",
    "<div style=\"margin-top: 20px;\">\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "        <div style=\"width: 100%;\">\n",
    "            <img src=\"_static/images/nb_96_critical_state.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "        </div>\n",
    "        <div style=\"width: 100%;\">\n",
    "            <img src=\"_static/images/96_critical_action_states.png\" alt=\"KL divergence\" width=80%>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 4-5: Select offline policies and training\n",
    "\n",
    "In this part of the exercise you need to: \n",
    "\n",
    "a) Restore the policy configurations (through TrainedPolicyConfig) for three offline RL policies, namely **BCQ, CQL and BC**, i.e.:\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig( ... )\n",
    "\n",
    "Give a look to the policy parameteres in offline_rl/offline_policies. \n",
    "\n",
    "\n",
    "b) Train the policies on the **expert data**:\n",
    "\n",
    "NUM_EPOCHS =10\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "STEP_PER_EPOCH = len_buffer\n",
    "\n",
    "\n",
    "offline_training( ... )\n",
    "\n",
    "\n",
    "c) Visualize the policies:\n",
    "\n",
    "offpolicy_rendering( ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_rl_policies = [PolicyName.bcq_discrete, PolicyName.cql_discrete, PolicyName.imitation_learning]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0*len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "available_obstacles = [ ObstacleTypes.obstacle_8x8_wall_with_door]\n",
    "selected_obstacle = widget_list(available_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (7, 7)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(selected_data_set.value, selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, SAVED_POLICY_NAME), map_location=\"cpu\"))\n",
    "\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value)\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "#snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False,\n",
    "    inline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary and conclusions\n",
    "\n",
    "**1 - Are BCQ and the CQL policies able to learn the expert data?**\n",
    "\n",
    "**2 - As we saw before, imitation learning is a good option when you have expert data. How it compares with the offline algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now, we'll explore how BCQ and CQL, address the issue of connecting suboptimal trajectories in order to get new ones with higer rewards (stitching property). We will see how they compare with imitation learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will start again with the previous setup. So, as we did before, we will create again two datasets: one from a policy moving suboptimal from (0,0) to (2,4), and the other from another policy moving from (4,0) to (7,7). The goal is to find an agent capable of connecting trajectories coming from both datasets, in order to find the optimal path between (2,0) and (2,4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 1: Create the environment\n",
    "\n",
    "**Create the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_free_8x8\n",
    "INITIAL_STATE_POLICY_I = (0,0)\n",
    "INITIAL_STATE_POLICY_II = (2,0)\n",
    "FINAL_STATE_POLICY = (2, 4)\n",
    "\n",
    "\n",
    "env_2D_grid_initial_config_I = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_I,\n",
    "    target_state=FINAL_STATE_POLICY,\n",
    ")\n",
    "\n",
    "env_2D_grid_initial_config_II = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_II,\n",
    "    target_state=FINAL_STATE_POLICY,\n",
    ")\n",
    "\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode),\n",
    "                                          env_config=env_2D_grid_initial_config_I)\n",
    "snapshot_env(env)\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode),\n",
    "                                          env_config=env_2D_grid_initial_config_II)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 2: Create Minari datasets\n",
    "\n",
    "**Let's study how well offline RL algorithms can deal with the stitching property. We will examine some edge cases to compare them with some of the algorithms we have already studied.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "IDENTIFIER_COMBINED_DATASETS = \"_stiching_property_I\"\n",
    "\n",
    "# Dataset I with 2000 collected points\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_grid_deterministic_0_0_to_4_7\n",
    "DATA_SET_IDENTIFIER_I = \"_longer_path\"\n",
    "NUM_STEPS_I =2000\n",
    "\n",
    "# Dataset II with 1000 points\n",
    "#BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_move_right\n",
    "DATA_SET_IDENTIFIER_II = \"_short_path\"\n",
    "NUM_STEPS_II = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "select_policy_to_render = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config = env_2D_grid_initial_config_I if select_policy_to_render.value == BEHAVIOR_POLICY_I else env_2D_grid_initial_config_II\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=select_policy_to_render.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Create datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = (env_2D_grid_initial_config_I, env_2D_grid_initial_config_II),\n",
    ")\n",
    "buffer_data = load_buffer_minari(config_combined_data.data_set_name)\n",
    "data_size = len(buffer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 3: Feed data into replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env, normalized=False)\n",
    "\n",
    "if \"start_0_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((0,0))\n",
    "    snapshot_env(env)\n",
    "elif \"start_2_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((2,0))\n",
    "    snapshot_env(env)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 4: Select offline policies and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_rl_policies = [PolicyName.bcq_discrete, PolicyName.cql_discrete]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Offiline - Training\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0*len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### **Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (2, 4)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(selected_data_set.value, selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, SAVED_POLICY_NAME), map_location=\"cpu\"))\n",
    "\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "#snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Let's now change the dataset distribution. We'll collect 600 points with the first behavior policy and 100 with the second one. In this case, the probability of taking the suboptimal path will be higher. What paths are chosen by the algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "ToDo together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's train a BCQ policy using the TORCS data from our previous DAGGER exercise.\n",
    "\n",
    "Imitation learning failed to achieve autonomous driving, as it imitate the behavior policy that caused the car to crash after a few meters. Let's test if the BCQ algorithm is able to avoid the crash.\n",
    "\n",
    "Note that we won't be able to get results as good as DAGGER, as it not only introduced expert knowledge but also new states, which makes the problem much easier to solve. However access to human experts is quite rare in real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "os.system(\"torcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Create TORCS Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = EnvFactory.torcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Create Minari dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select TORCS behavior policies and visualize them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_selected_to_visualize = widget_list([\n",
    "    BehaviorPolicyType.torcs_drunk_driver_policy\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    behavior_policy_name=policy_selected_to_visualize.value,\n",
    "    num_frames=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the policy\n",
    "BEHAVIOR_POLICY = policy_selected_to_visualize.value\n",
    "DATA_SET_IDENTIFIER = \"torcs_crash\"\n",
    "NUM_STEPS = 4000\n",
    "\n",
    "config_torcs_data_set = create_minari_datasets(\n",
    "    env_name=ENV_NAME,\n",
    "    dataset_identifier=DATA_SET_IDENTIFIER,\n",
    "    num_colected_points=NUM_STEPS,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    ")\n",
    "\n",
    "_ = os.system(\"pkill torcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: Feed dataset to replay buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = config_torcs_data_set.data_set_name\n",
    "buffer_data = load_buffer_minari(DATA_SET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4-5: Select offline policy and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Select the policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:50.941612Z",
     "start_time": "2024-05-09T19:53:50.928500Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "NUMBER_TEST_ENVS = 1\n",
    "PERCENTAGE_DATA_PER_EPOCH = 1.0\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "OFFLINE_POLICY_NAME = PolicyName.bcq_continuous\n",
    "TRAINED_POLICY_NAME = \"policy_bcq.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Offline RL policy metadata config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:53.384296Z",
     "start_time": "2024-05-09T19:53:53.366403Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=PERCENTAGE_DATA_PER_EPOCH * len(buffer_data),\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_name=TRAINED_POLICY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:57.758669Z",
     "start_time": "2024-05-09T19:53:57.743789Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trained_policy_selected = widget_list([TRAINED_POLICY_NAME, \"policy_best_reward.pth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:54:10.427826Z",
     "start_time": "2024-05-09T19:54:06.754285Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trained_bcq_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bcq_policy.load_state_dict(torch.load(str(os.path.join(log_path, trained_policy_selected.value)), map_location=\"cpu\"))\n",
    "trained_bcq_policy\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=\"torcs\",\n",
    "    render_mode=None,\n",
    "    policy_model=trained_bcq_policy,\n",
    "    num_frames=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Analysis of results: Let's compare the actions taken by the BCQ policy against the ones that would have been taken by the expert.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:52:15.100086Z",
     "start_time": "2024-05-09T19:52:14.333241Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_STEPS_ROLLOUT = 4500 \n",
    "\n",
    "output_initial_phase = policy_rollout_torcs_env(\n",
    "    driver_policy=trained_bcq_policy,\n",
    "    advisor_policy=BehaviorPolicyType.torcs_expert_policy,\n",
    "    num_steps=NUM_STEPS_ROLLOUT,\n",
    ")\n",
    "\n",
    "compare_policy_decisions_vs_expert_suggestions(\n",
    "    policy_actions=output_initial_phase[\"actions_driver\"],\n",
    "    expert_suggestions=output_initial_phase[\"actions_advisor\"],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the BCQ decisions are very close to the expert ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's give a look to the BCQ vs data action distributions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 60\n",
    "actions_bcq_policy = output_initial_phase[\"actions_driver\"]\n",
    "_ = plt.hist(actions_bcq_policy, bins=num_bins, alpha=0.5, label='bcq actions')\n",
    "_ = plt.hist(buffer_data.act, bins=num_bins, alpha=0.5, label='data actions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that BCQ goes a bit o.o.d. to find the optimal path, but it doesn't create totally different actions, as we discussed before. It basically analyzes if it can produce similar actions to the collected ones, that bring the agent towards higher rewards.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, let's see which are the states corresponding to the o.o.d. actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_bcq = np.array(actions_bcq_policy)\n",
    "mask = ((actions_bcq>-0.1)&(actions_bcq<-0.05)) | ((actions_bcq>0.05)&(actions_bcq<0.1))\n",
    "indx_ood_states = np.where(mask)[0]\n",
    "_ = plt.hist(indx_ood_states, bins=100)\n",
    "plt.title(\"Histogram of states where BCQ decides to go o.o.d.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can check that these states are the ones corresponding to the curve where the behavior policy crashed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise IV \n",
    "\n",
    "[AdroitHandPen-v1](https://minari.farama.org/datasets/pen/expert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Offline RL proves valuable in various scenarios, especially when:\n",
    "\n",
    "a. Robots require intelligent behavior in complex open-world environments demanding extensive training data due to robust visual perception requirements. (complex environment modeling and extensive data collection)\n",
    "\n",
    "b. Robot grasping tasks, which involve expert data that cannot be accurately simulated, providing an opportunity to assess our BCQ algorithm.\n",
    "\n",
    "c. Robotic navigation tasks, where offline RL aids in crafting effective navigation policies using real-world data.\n",
    "\n",
    "d. Autonomous driving, where ample expert data and an offline approach enhance safety.\n",
    "\n",
    "e. Healthcare applications, where safety is paramount due to the potential serious consequences of inaccurate forecasts.\n",
    "\n",
    "... and many more.\n",
    "\n",
    "However, if you have access to an environment with abundant data, online Reinforcement Learning (RL) can be a powerful choice due to its potential for exploration and real-time feedback. Nevertheless, the landscape of RL is evolving, and a data-centric approach is gaining prominence, exemplified by vast datasets like X-Embodiment. It's becoming evident that robots trained with diverse data across various scenarios tend to outperform those solely focused on specific tasks. Furthermore, leveraging multitask trained agents for transfer learning can be a valuable strategy for addressing your specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "396.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
