{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:27.996795Z",
     "start_time": "2024-05-09T19:53:26.925119Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:29.266823Z",
     "start_time": "2024-05-09T19:53:29.254411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:29.481414Z",
     "start_time": "2024-05-09T19:53:29.471813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:29.780063Z",
     "start_time": "2024-05-09T19:53:29.765450Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:31.749589Z",
     "start_time": "2024-05-09T19:53:30.907833Z"
    }
   },
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "load_env_variables()\n",
    "\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv, RenderMode, register_grid_envs\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset \n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig, get_trained_policy_path\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import widget_list\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env)\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, state_action_histogram\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import MinariDatasetConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "register_grid_envs()\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Offline RL algorithms exercises </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Offline RL pipeline:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Offline RL algorithms exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Previously, we discussed that **off-policy methods cannot learn from data efficiently unless a significant amount of data covering a large portion of the environment states is available**. Only in such cases can the agent explore the environment and get feedback similar to what's done in an online approach. However, this scenario is rare and challenging to achieve in realistic applications, which is one of the reasons why we turn to offline RL, where only a small amount of data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also discussed one of the major issues when applying off-policy methods to collected data: the agent's tendency to go out-of-distribution (o.o.d.). More importantly, once it goes o.o.d., the policy becomes unpredictable, making it impossible to return to the in-distribution region. This unpredictability propagates errors in the policy evaluation process (i.e., the dynamic programming equations), hindering the algorithm's learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise I\n",
    "\n",
    "**Similar to our approach in the off-policy notebook (nb_95), we will collect a small amount of expert data and a larger amount of suboptimal data. We will then observe how two offline RL algorithms introduced earlier, BCQ and CQL, can recover the expert policy without going o.o.d. We will compare our results with the imitation learning approach, specifically the BC algorithm, which, as we discussed in the imitation learning section (nb_93), is another viable option when expert data is available.**\n",
    "\n",
    "\n",
    "In this exercise we will collect two datasets with expert and suboptimal data that tries to bring the agent from (3,0) to (0,7) .\n",
    "\n",
    "I  - **Suboptimal expert policy**:  collect ~ 500 steps\n",
    "\n",
    "II - **expert policy**: collect ~ 100 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "obstacle_selected = widget_list([ObstacleTypes.obstacle_8x8_top_right, ObstacleTypes.obst_free_8x8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = obstacle_selected.value\n",
    "INITIAL_STATE = (7, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Behavior policies and datasets configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_move_up_from_bottom_twice\n",
    "DATA_SET_IDENTIFIER_I = \"_suboptimal_for_offline_rl\"\n",
    "NUM_STEPS_I = 500\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_suboptimal_determ_initial_3_0_final_3_7\n",
    "DATA_SET_IDENTIFIER_II = \"_expert_for_offline_rl\"\n",
    "NUM_STEPS_II = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Collect data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_data_sets_offline_rl\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")\n",
    "\n",
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 3: Feed data into replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.01))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 4-5: Select offline policies and trained them\n",
    "\n",
    "In this part of the exercise you need to: \n",
    "\n",
    "a) Restore the policy configurations (through TrainedPolicyConfig) for three offline RL policies, namely **BCQ, CQL and BC**, i.e.:\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig( ... )\n",
    "\n",
    "Give a look to the policy parameteres in offline_rl/offline_policies. \n",
    "\n",
    "\n",
    "b) Train the policies on the **expert data**:\n",
    "\n",
    "NUM_EPOCHS =10\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "STEP_PER_EPOCH = len_buffer\n",
    "\n",
    "\n",
    "offline_training( ... )\n",
    "\n",
    "\n",
    "c) Visualize the policies:\n",
    "\n",
    "offpolicy_rendering( ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_rl_policies = [PolicyName.bcq_discrete, PolicyName.cql_discrete, PolicyName.imitation_learning]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0*len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "available_obstacles = [ObstacleTypes.obstacle_8x8_top_right, ObstacleTypes.obst_free_8x8]\n",
    "selected_obstacle = widget_list(available_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (7, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(selected_data_set.value, selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, SAVED_POLICY_NAME), map_location=\"cpu\"))\n",
    "\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value)\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "#snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary and conclusions\n",
    "\n",
    "**1 - Are BCQ and the CQL policies able to learn the expert data?**\n",
    "\n",
    "**2 - As we saw before, imitation learning is a good option when you have expert data. How it compares with the offline algorithms?**\n",
    "\n",
    "**3 - Now rollout the three policies from o.o.d. data. What do you observe?**\n",
    "\n",
    "**4 - Remove the obstacle and do a rollout of the three policies. What do you observe?**\n",
    "\n",
    "**5 - Remove now the obstacle and use the combined dataset that includes a fair amount of suboptimal data. What do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now, we'll explore how BCQ and CQL, address the issue of connecting suboptimal trajectories in order to get new ones with higer rewards (stitching property). We will see how they compare with imitation learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will start again with the previous setup. So, as we did before, we will create again two datasets: one from a policy moving suboptimal from (0,0) to (7,0), and the other from another policy moving from (4,0) to (7,7). The goal is to find an agent capable of connecting trajectories coming from both datasets, in order to find the optimal path between (0,0) and (7,7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 1: Create the environment\n",
    "\n",
    "**Create the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_free_8x8\n",
    "INITIAL_STATE_POLICY_I = (0,0)\n",
    "INITIAL_STATE_POLICY_II = (4,0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config_I = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_I,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env_2D_grid_initial_config_II = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_II,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode),\n",
    "                                          env_config=env_2D_grid_initial_config_I)\n",
    "snapshot_env(env)\n",
    "\n",
    "env.set_starting_point(INITIAL_STATE_POLICY_II)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "IDENTIFIER_COMBINED_DATASETS = \"_stiching_property_I\"\n",
    "\n",
    "# Dataset I\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_grid_deterministic_0_0_to_4_7\n",
    "DATA_SET_IDENTIFIER_I = \"_move_downwards\"\n",
    "NUM_STEPS_I = 200\n",
    "\n",
    "# Dataset II\n",
    "#BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "DATA_SET_IDENTIFIER_II = \"_move_deterministic\"\n",
    "NUM_STEPS_II = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "select_policy_to_render = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config = env_2D_grid_initial_config_I if select_policy_to_render.value == BEHAVIOR_POLICY_I else env_2D_grid_initial_config_II\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=select_policy_to_render.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Create datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = (env_2D_grid_initial_config_I, env_2D_grid_initial_config_II),\n",
    ")\n",
    "buffer_data = load_buffer_minari(config_combined_data.data_set_name)\n",
    "data_size = len(buffer_data)\n",
    "\n",
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 3: Feed data into replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.03))\n",
    "\n",
    "if \"start_0_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((0,0))\n",
    "    snapshot_env(env)\n",
    "elif \"start_4_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((4,0))\n",
    "    snapshot_env(env)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 4: Select offline policies and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_rl_policies = [PolicyName.bcq_discrete, PolicyName.cql_discrete, PolicyName.imitation_learning]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Offiline - Training\n",
    "\n",
    "NUM_EPOCHS =10\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0*len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "#offline_policy_config.policy_config[\"unlikely_action_threshold\"]=0.6\n",
    "#offline_policy_config.policy_config[\"min_q_weight\"]=15.0\n",
    "#offline_policy_config.policy_config[\"num_quantiles\"]=5\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(selected_data_set.value, selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, SAVED_POLICY_NAME), map_location=\"cpu\"))\n",
    "\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "#snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What do you observe? Try increasing the number of expert samples and run it again. What happens now?**\n",
    "\n",
    "**As we can see, the BCQ algorithm is able to stitch two trajectories together to create an optimal one.**\n",
    "\n",
    "**Now, trying to do the same with CQL and compare results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's train a BCQ policy using the TORCS data from our previous DAGGER exercise.\n",
    "\n",
    "Imitation learning failed to achieve autonomous driving, as it imitate the behavior policy that caused the car to crash after a few meters. Let's test if the BCQ algorithm can successfully navigate the entire race track.\n",
    "\n",
    "Note that we won't be able to get results as good as DAGGER, as it not only introduced expert knowledge but also new states, which makes the problem much easier to solve. However access to human experts is quite rare in real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "os.system(\"torcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Select the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:48.933903Z",
     "start_time": "2024-05-09T19:53:48.918712Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ee151303c9492ea37e96633a5b0c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('torcs-data-v0',), value='torcs-data-v0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collected_datasets_names = [\"torcs-data-v0\"]\n",
    "collected_datasets = widget_list(collected_datasets_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Select the policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:50.941612Z",
     "start_time": "2024-05-09T19:53:50.928500Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 13\n",
    "BATCH_SIZE = 128\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None  # 1626\n",
    "PERCENTAGE_DATA_PER_EPOCH = 1.0\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "OFFLINE_POLICY_NAME = PolicyName.bcq_continuous\n",
    "DATA_SET_NOISY_NAME = collected_datasets.value\n",
    "TRAINED_POLICY_NAME = \"policy_bcq.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Feed data into replay buffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:53.384296Z",
     "start_time": "2024-05-09T19:53:53.366403Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset /home/ivan/Documents/GIT_PROJECTS/tfl-training-rl/src/training_rl/offline_rl/data/offline_data/torcs-data-v0 downloaded. number of episodes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/envs/training_rl/lib/python3.11/site-packages/gymnasium/spaces/box.py:130: UserWarning: \u001B[33mWARN: Box bound precision lowered by casting to float32\u001B[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "buffer_data = load_buffer_minari(DATA_SET_NOISY_NAME)\n",
    "data_config = MinariDatasetConfig.load_from_file(DATA_SET_NOISY_NAME)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_NOISY_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=PERCENTAGE_DATA_PER_EPOCH * len(buffer_data),\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_name=TRAINED_POLICY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:53:57.758669Z",
     "start_time": "2024-05-09T19:53:57.743789Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trained_policy_selected = widget_list([\"policy_best_reward.pth\", TRAINED_POLICY_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:54:10.427826Z",
     "start_time": "2024-05-09T19:54:06.754285Z"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trained_bcq_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_NOISY_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bcq_policy.load_state_dict(torch.load(str(os.path.join(log_path, trained_policy_selected.value)), map_location=\"cpu\"))\n",
    "trained_bcq_policy\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=\"torcs\",\n",
    "    render_mode=None,\n",
    "    policy_model=trained_bcq_policy,\n",
    "    num_frames=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Analysis of results: Let's compare the expert, suboptimal and offline trained policies.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T19:52:15.100086Z",
     "start_time": "2024-05-09T19:52:14.333241Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.visualizations.utils import collect_data_from_rollout_torcs_policy\n",
    "NUM_STEPS_ROLLOUT = 5000 \n",
    "bcq_driver = collect_data_from_rollout_torcs_policy(\n",
    "    env_collected_quantity='angle',\n",
    "    driver_policy=trained_bcq_policy,\n",
    "    num_steps=NUM_STEPS_ROLLOUT\n",
    ")\n",
    "\n",
    "drunk_driver = collect_data_from_rollout_torcs_policy(\n",
    "    env_collected_quantity='angle',\n",
    "    driver_policy=BehaviorPolicyType.torcs_drunk_driver_policy,\n",
    "    num_steps=NUM_STEPS_ROLLOUT\n",
    ")\n",
    "\n",
    "expert_driver = collect_data_from_rollout_torcs_policy(\n",
    "    env_collected_quantity='angle',\n",
    "    driver_policy=BehaviorPolicyType.torcs_expert_policy,\n",
    "    num_steps=NUM_STEPS_ROLLOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_multiple_lines(expert_data, bcq_data, drunk_data, obs_name=\"actions_driver\"):\n",
    "    x = range(len(expert_data[obs_name]))\n",
    "    plt.plot(x, expert_data[obs_name], label='Expert Driver')\n",
    "    plt.plot(x, bcq_data[obs_name], label='BCQ Driver')\n",
    "    plt.plot(x, drunk_data[obs_name], label='Drunk Driver')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel(obs_name)\n",
    "    plt.title(f'Comparison of Drivers {obs_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_multiple_lines(expert_driver, bcq_driver, drunk_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lidar_value = 1\n",
    "\n",
    "expert_lidars_left = [lidar[lidar_value] for lidar in expert_driver[\"observations\"]]\n",
    "bcq_lidars_left = [lidar[lidar_value] for lidar in bcq_driver[\"observations\"]]\n",
    "drunk_lidars_left = [lidar[lidar_value] for lidar in drunk_driver[\"observations\"]]\n",
    "\n",
    "def hist_comparison(data_list, labels, x_title, bins=20, alpha=0.4, colors=None, y_range=None):\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = ['blue', 'orange','green']\n",
    "\n",
    "    for i, data in enumerate(data_list):\n",
    "        plt.hist(data, bins=bins, density=True, alpha=alpha, color=colors[i], label=labels[i])\n",
    "    \n",
    "        plt.xlabel(x_title)\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Histogram Comparison')\n",
    "        plt.legend()\n",
    "        if y_range:\n",
    "            plt.ylim(y_range[0], y_range[1])  # Change y-axis range here\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "data = [expert_lidars_left, bcq_lidars_left]\n",
    "labels=[\"bcq\",\"behavior\"]\n",
    "hist_comparison(data, labels, y_range=[0.0,10.5], x_title=\"observations\")\n",
    "\n",
    "data = [expert_driver[\"actions_driver\"], bcq_driver[\"actions_driver\"]]\n",
    "labels=[\"bcq\",\"behavior\"]\n",
    "hist_comparison(data, labels, y_range=[0.0,10.5], x_title=\"actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise IV \n",
    "\n",
    "Robotic hand: Homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Offline RL proves valuable in various scenarios, especially when:\n",
    "\n",
    "a. Robots require intelligent behavior in complex open-world environments demanding extensive training data due to robust visual perception requirements. (complex environment modeling and extensive data collection)\n",
    "\n",
    "b. Robot grasping tasks, which involve expert data that cannot be accurately simulated, providing an opportunity to assess our BCQ algorithm.\n",
    "\n",
    "c. Robotic navigation tasks, where offline RL aids in crafting effective navigation policies using real-world data.\n",
    "\n",
    "d. Autonomous driving, where ample expert data and an offline approach enhance safety.\n",
    "\n",
    "e. Healthcare applications, where safety is paramount due to the potential serious consequences of inaccurate forecasts.\n",
    "\n",
    "... and many more.\n",
    "\n",
    "However, if you have access to an environment with abundant data, online Reinforcement Learning (RL) can be a powerful choice due to its potential for exploration and real-time feedback. Nevertheless, the landscape of RL is evolving, and a data-centric approach is gaining prominence, exemplified by vast datasets like X-Embodiment. It's becoming evident that robots trained with diverse data across various scenarios tend to outperform those solely focused on specific tasks. Furthermore, leveraging multitask trained agents for transfer learning can be a valuable strategy for addressing your specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
