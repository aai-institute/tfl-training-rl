{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%autoreload\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import gymnasium as gym\n",
    "import minari\n",
    "import numpy as np\n",
    "from minari import DataCollectorV0, StepDataCallback\n",
    "\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.simple_grid import \\\n",
    "    Custom2DGridEnv\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, register_grid_envs)\n",
    "from training_rl.offline_rl.custom_envs.utils import (\n",
    "    Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.utils import \\\n",
    "    generate_compatible_minari_dataset_name\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import \\\n",
    "    offpolicy_rendering\n",
    "from training_rl.offline_rl.utils import (delete_minari_data_if_exists,\n",
    "                                          load_buffer_minari,\n",
    "                                          one_hot_to_integer,\n",
    "                                          state_action_histogram)\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env)\n",
    "\n",
    "load_env_variables()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# To get access to the registered environments.\n",
    "register_grid_envs()\n",
    "\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Minari Overview </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minari overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Unfortunately not supported in windows (maybe mainly because of Mujoco dependency)\n",
    "\n",
    "1 - **[Intro Minari](https://minari.farama.org/main/content/basic_usage/)**\n",
    "\n",
    "Can be used to collect data given a gymnasium environment:\n",
    "\n",
    "    from minari import DataCollectorV0\n",
    "    import gymnasium as gym\n",
    "\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env = DataCollectorV0(env, record_infos=True, max_buffer_steps=10000)\n",
    "\n",
    "\n",
    "Has different functionalities like to add useful metadata to the datasets, fuse data coming from different behavioral policies, custom preprocessing of the collected (observation, action, reward) data, add new data to existing datasets, restore the environments associated with your data, etc. We will use some of these functionalities in our exercises.\n",
    "\n",
    "However, is not a very robust library so in the possible it is better to have your own functions to restore your environment, load metadata, etc. And rely on Minari as little as possible. We will see that in the notebook exercises.\n",
    "\n",
    "\n",
    "Of course one of its main use is to Download the very useful datasets available online as well as to have access to the policies used to generate them: [datasets](https://minari.farama.org/main/datasets/pen/human/)\n",
    "\n",
    "\n",
    "Many interesting datasets uses the Mujoco C/C++ library.\n",
    "\n",
    "2 - **[Mujoco](https://github.com/google-deepmind/mujoco)**\n",
    "\n",
    "MuJoCo, short for Multi-Joint dynamics with Contact, is a versatile physics engine designed to support research and development across various fields, including robotics, bio-mechanics, graphics, animation, machine learning, and more. Originally created by Robotic LLC, it was later acquired by DeepMind and made freely accessible to the public in October 2021. Furthermore, it was open-sourced in May 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, we will create our own suboptimal policy and generate a Minari dataset using the collected data. The primary goal is to become familiar with the Minari library and learn how to collect data in the form of (step, action, reward) using it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will design a suboptimal behavioral policy, denoted as $\\pi_b$, to guide an agent through a 2-dimensional 8x8 grid world. The agent's starting position is at (0, 0), and the objective is to reach a target located at (0, 7). During this process, we will collect data from our policy.**\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "In our grid world environment, the agent's position is represented as $(x_1, x_2)$, where $x_1$ corresponds to the vertical coordinate, and $x_2$ corresponds to the horizontal coordinate. Additionally, we've placed obstacles within the environment to increase the task's complexity.\n",
    "\n",
    "Observations are represented as a 64-dimensional vector since we are working with a 8x8 grid. All vector elements are set to zero, except for the position where the agent is located, which is set to one.\n",
    "\n",
    "The action is represented by an integer in the range of [0, 1, 2, 3], each indicating a direction:\n",
    "\n",
    "    0: (-1, 0) - UP\n",
    "    1: (1, 0) - DOWN\n",
    "    2: (0, -1) - LEFT\n",
    "    3: (0, 1) - RIGHT\n",
    "\n",
    "Now, let's proceed to create the environment and take a closer look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have different 2-d grid environments registered.\n",
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Configure the environment initial conditions. We also have different obstacles registered\n",
    "# in ObstacleTypes. \n",
    "env_2d_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=ObstacleTypes.vertical_object_8x8,\n",
    "    initial_state=(0, 0),\n",
    "    target_state=(0, 7),\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(\n",
    "    gym.make(ENV_NAME, render_mode=render_mode),\n",
    "    env_config=env_2d_grid_initial_config\n",
    ")\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a suboptimal behavioral policy, $\\pi_b$:\n",
    "\n",
    "    def behavior_suboptimal_policy(state: np.ndarray, env: \n",
    "                                                     Custom2DGridEnv) -> int:\n",
    "\n",
    "        state_index = one_hot_to_integer(state)\n",
    "        state_xy = env.to_xy(state_index)\n",
    "\n",
    "        ...\n",
    "\n",
    "        return action\n",
    "        \n",
    "(Note: In practical scenarios, the environment would not be supplied to the policy. We include it here for simplicity.)\n",
    "\n",
    "This policy should meet the following criteria:\n",
    "\n",
    "1 - Guide the agent from its initial position at (0, 0) to a target located at (0, 7).\n",
    "\n",
    "2 - Incorporate stochastic movement with a restriction of not traversing more than 5 cells \n",
    "    vertically while introducing a bias towards the right direction to make it suboptimal.\n",
    "    \n",
    "Hint: To ensure that everything is working correctly, you can visualize the policy using the **offpolicy_rendering(...)** function:\n",
    "\n",
    "    offpolicy_rendering(\n",
    "        env_or_env_name=ENV_NAME,\n",
    "        render_mode=render_mode,\n",
    "        policy_model= behavior_suboptimal_policy,\n",
    "        env_2d_grid_initial_config=env_2d_grid_initial_config,\n",
    "        num_frames=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_suboptimal_policy(state: np.ndarray, env: Custom2DGridEnv) -> int:\n",
    "\n",
    "    state_index = one_hot_to_integer(state)\n",
    "    state_xy = env.to_xy(state_index)\n",
    "\n",
    "    possible_directions = [0, 1, 2, 3]\n",
    "    weights = [1, 1, 1, 2]\n",
    "\n",
    "    if state_xy[0] == 4:\n",
    "        weights = [2, 1, 2]\n",
    "        possible_directions = [0, 2, 3]\n",
    "\n",
    "    random_directions = random.choices(possible_directions, weights=weights)[0]\n",
    "\n",
    "    return random_directions\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    policy_model= behavior_suboptimal_policy,\n",
    "    env_2d_grid_initial_config=env_2d_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using our suboptimal policy, we will proceed to collect data and generate a custom Minari dataset.\n",
    "\n",
    "Give a look to [Minari documentation](https://minari.farama.org/main/content/basic_usage/) if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your own Minari dataset using the behavior_suboptimal_policy.\n",
    "\n",
    "Hint: Remember to use the DataCollectorV0 wrapper provided in the Minari library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER = \"_rl_workshop\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "\n",
    "DATA_SET_NAME += DATA_SET_IDENTIFIER\n",
    "minari_dataset_name = generate_compatible_minari_dataset_name(ENV_NAME, DATA_SET_NAME, VERSION_DATA_SET)\n",
    "\n",
    "delete_minari_data_if_exists(minari_dataset_name)\n",
    "\n",
    "# Useful for data preprocessing\n",
    "class CustomSubsetStepDataCallback(StepDataCallback):\n",
    "    def __call__(self, env, **kwargs):\n",
    "        step_data = super().__call__(env, **kwargs)\n",
    "        #del step_data[\"observations\"][\"achieved_goal\"]\n",
    "        return step_data\n",
    "\n",
    "env = DataCollectorV0(\n",
    "    env,\n",
    "    step_data_callback=CustomSubsetStepDataCallback,\n",
    "    record_infos=False,\n",
    ")\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "num_steps = 0\n",
    "for _ in range(BUFFER_SIZE):\n",
    "\n",
    "    action = behavior_suboptimal_policy(state, env)\n",
    "\n",
    "    next_state, reward, done, time_out, info = env.step(action)\n",
    "    num_steps += 1\n",
    "\n",
    "    if done or time_out:\n",
    "        state, _ = env.reset()\n",
    "        num_steps=0\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "dataset = minari.create_dataset_from_collector_env(dataset_id=minari_dataset_name, collector_env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the state-action distribution data.\n",
    "\n",
    "First, let's upload the Minari dataset in a Tianshou ReplyBuffer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = \"Grid_2D_8x8_discrete-data_rl_workshop-v0\"\n",
    "data = load_buffer_minari(NAME_EXPERT_DATA)\n",
    "\n",
    "state_action_count_data, _ = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(data, env)\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\",new_keys_for_state_action_count_plot=new_keys)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution makes sense. From states (5,0) to (7,7), our policy doesn't collect any data, neither around the obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[ \\[Fu.Justin et. al.\\] D4RL: Datasets for Deep Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "\n",
    "[ MINARI: A dataset API for Offline Reinforcement Learning ](https://minari.farama.org/main/content/basic_usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
