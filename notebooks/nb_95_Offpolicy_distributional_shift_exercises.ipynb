{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:12:17.451999Z",
     "start_time": "2024-04-09T16:12:16.443733Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:12:18.209514Z",
     "start_time": "2024-04-09T16:12:18.199068Z"
    }
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:12:18.897136Z",
     "start_time": "2024-04-09T16:12:18.882164Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:12:37.628132Z",
     "start_time": "2024-04-09T16:12:19.382783Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "load_env_variables()\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, register_grid_envs)\n",
    "from training_rl.offline_rl.custom_envs.utils import (\n",
    "    Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import \\\n",
    "    offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import \\\n",
    "    offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig, get_trained_policy_path)\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import \\\n",
    "    restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import (compare_state_action_histograms,\n",
    "                                          load_buffer_minari,\n",
    "                                          state_action_histogram)\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "register_grid_envs()\n",
    "\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Offline RL distributional shift exercises </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Policy Distributional Shift - I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise I:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we have two datasets that tries to bring the agent from (3,0) to (7,7) .\n",
    "\n",
    "<img src=\"_static/images/nb_95_env_image_1.png\" alt=\"grid environment configuration\" style=\"height:400px;\">\n",
    "\n",
    "One policy is suboptimal and the other is optimal.\n",
    "\n",
    "I  - **Suboptimal expert policy**:  collect ~ 8000 steps\n",
    "\n",
    "II - **expert policy**: collect ~ 1000 steps\n",
    "\n",
    "(This could be a realistic situation where you could have only a few human expert data available and the rest of the data is collected through a far from optimal policy.)\n",
    "\n",
    "**We will use in this example as off-policy RL policy, the Deep Q-Network (DQN) algorithm,** that we introduced in the online RL section. As the DQN agent cannot interact with the environment we will feed the collected data through a ReplyBuffer, similarly as we did before in the imitation learning example.\n",
    "\n",
    "\n",
    "Exercise: (Give a look to the imitation learning exercise)\n",
    "\n",
    "1 - **We will use the 8x8 grid environment with a vertical obstacle : ObstacleTypes.dqn_obstacle_8x8**.\n",
    "\n",
    "2 - **The policies: behavior_8x8_eps_greedy_4_0_to_7_7 and behavior_8x8_deterministic_4_0_to_7_7)**\n",
    "\n",
    "3 - **For the dataset collection give a look to the solution**, specifically to the points:\n",
    "\n",
    "- Configure the two datasets\n",
    "- Create Minari combined datasets \n",
    "\n",
    "as we will fuse the two datasets coming from different policies into a single one (as you should do in a real problem)\n",
    "\n",
    "4 - **Do the training using as policy BehaviorPolicyType.dqn to have a felling of how good (or how bad) is an offpolicy algorithm to deal with collected data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SOLUTION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:12:54.210464Z",
     "start_time": "2024-04-09T16:12:54.107118Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.dqn_obstacle_8x8\n",
    "INITIAL_STATE = (3, 0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure and rendering policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:13:12.623694Z",
     "start_time": "2024-04-09T16:13:12.607653Z"
    }
   },
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "# ToDo: CHANGE !!!!!!!!!\n",
    "#BEHAVIOR_POLICY_I = BehaviorPolicyType.random\n",
    "DATA_SET_IDENTIFIER_I = \"_suboptimal\"\n",
    "# TODO: CHANGE !!!!!!!\n",
    "NUM_STEPS_I = 100000\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "DATA_SET_IDENTIFIER_II = \"_expert\"\n",
    "NUM_STEPS_II = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:13:15.165099Z",
     "start_time": "2024-04-09T16:13:13.044646Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_I,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert policy\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create single Minari dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( give a look to **create_combined_minari_dataset(...)** ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:13:20.614814Z",
     "start_time": "2024-04-09T16:13:17.327554Z"
    }
   },
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_data_sets_random_walk\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check state-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:13:23.980999Z",
     "start_time": "2024-04-09T16:13:21.495025Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Buffers with minari datasets\n",
    "name_combined_dataset = config_combined_data.data_set_name\n",
    "\n",
    "buffer_data = load_buffer_minari(name_combined_dataset)\n",
    "data_size = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.3))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### offline policy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:20:43.643397Z",
     "start_time": "2024-04-09T16:20:43.629838Z"
    }
   },
   "outputs": [],
   "source": [
    "POLICY_NAME = PolicyName.dqn\n",
    "\n",
    "NAME_EXPERT_DATA = name_combined_dataset\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:31:16.966025Z",
     "start_time": "2024-04-09T16:28:52.960343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the training\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 0.1*data_size\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:31:16.967266Z",
     "start_time": "2024-04-09T16:31:16.967165Z"
    }
   },
   "outputs": [],
   "source": [
    "# ToDo: Delete or not delete ???\n",
    "\n",
    "from training_rl.offline_rl.online_trainings.online_training import online_training\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyType\n",
    "\n",
    "online_training(\n",
    "    trained_policy_config = offline_policy_config,\n",
    "    policy_type = PolicyType.offpolicy,\n",
    "    num_epochs=1,\n",
    "    batch_size=64,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:31:19.928380Z",
     "start_time": "2024-04-09T16:31:19.911411Z"
    }
   },
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy_best_reward.pth\"\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T16:31:25.927977Z",
     "start_time": "2024-04-09T16:31:22.224214Z"
    }
   },
   "outputs": [],
   "source": [
    "#env_2D_grid_initial_config.obstacles = OBSTACLE.obst_free_8x8\n",
    "#env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State-action policy distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 100 # as more episodes the better\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data,\n",
    "    env,\n",
    "    policy,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data)\n",
    "state_action_histogram(state_action_count_policy)\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Train again the DQN but now with random data (i.e. suboptimal random policy  and increase the expert data too,\n",
    "expert data ~ 30000).**\n",
    "\n",
    "Is the policy better? Why?\n",
    "\n",
    "Remove the walls and see what happens, i.e:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - Try now with the original problem and try one of the offline RL algorithms that we will introduce later:  BCQ.**\n",
    "\n",
    "What happens now?\n",
    "\n",
    "BCQ as all the offline RL algorithms has tuning parameters to control o.o.d data. They are typically tune to some empirical values that solve a huge range of tasks but in general these are important parameters you will need to tune in your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Policy Distributional Shift - II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise I\n",
    "\n",
    "**In this notebook we will deal with another important property that we should fulfill with a robust offline RL algorithm, the stitching, i.e. the reuse of different trajectories contain in the data to obtain the best trajectory in our dataset.**\n",
    "\n",
    "\n",
    "The goal will be to reach a target at (7,7) starting from (0,0). We will use again the 8x8 grid environment. Our dataset contains trajectories covering our space of interest but generated for different tasks (note that before we collected data for the same task) . One is a suboptimal policy that moves the agent from (0,0) to (7,0) and the other is a deterministic an optimal one (human expert) that brings the agent from (4,0) to (7,7). We have obviously much more data coming from the suboptimal policy than the expert one as it is cheaper.\n",
    "\n",
    "So we will create the two policies:\n",
    "\n",
    "I  - **Suboptimal expert policy** (behavior_8x8_moves_downwards_within_strip):  moves agent in suboptimal way downwards starting from (0,0) (collect 8000 steps)\n",
    "\n",
    "II - **Optimal expert policy**(behavior_8x8_deterministic_4_0_to_7_7): moves agent in the optimal path from (4,0) to (7,7) (collect 1000 steps)\n",
    "\n",
    "\n",
    "In this example we will use again as off-policy RL algorithm, the Deep Q-Network (DQN) algorithm.\n",
    "\n",
    "**Let's setup our configuration and create the environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.vertical_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_IDENTIFIER_I = \"_downwards_\"\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip\n",
    "NUM_STEPS_I = 8000\n",
    "\n",
    "DATA_SET_IDENTIFIER_II = \"_optimal_\"\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "NUM_STEPS_II = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create combined Minari dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"_stiching\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering behavioral policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_I,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert policy\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_combined_dataset = config_combined_data.data_set_name\n",
    "\n",
    "#Create Buffers with minari datasets\n",
    "buffer_data = load_buffer_minari(name_combined_dataset)\n",
    "data_size = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.012))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_NAME = PolicyName.dqn\n",
    "\n",
    "NAME_EXPERT_DATA = name_combined_dataset\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 0.1*data_size\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy_best_reward.pth\"\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1 - What do you notice? What happens if you increase the expert data? Is it better?\n",
    "\n",
    "2 - Try again with the BCQ algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
