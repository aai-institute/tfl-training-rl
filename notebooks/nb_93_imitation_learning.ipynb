{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:11.231023Z",
     "start_time": "2023-12-17T10:13:10.356212Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:11.245432Z",
     "start_time": "2023-12-17T10:13:11.232473Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:11.546984Z",
     "start_time": "2023-12-17T10:13:11.500639Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:12.958422Z",
     "start_time": "2023-12-17T10:13:12.424161Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from copy import copy\n",
    "\n",
    "import gymnasium as gym\n",
    "import minari\n",
    "import torch\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, register_grid_envs)\n",
    "from training_rl.offline_rl.custom_envs.utils import (\n",
    "    Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import (\n",
    "    MinariDatasetConfig, create_minari_datasets)\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import \\\n",
    "    offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import \\\n",
    "    offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig, get_trained_policy_path)\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import \\\n",
    "    restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import (compare_state_action_histograms,\n",
    "                                          load_buffer_minari,\n",
    "                                          state_action_histogram)\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env)\n",
    "\n",
    "load_env_variables()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "register_grid_envs()\n",
    "\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Imitation Learning </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Imitation learning is a supervise learning approach focuses on learning policies or behaviors by observing and imitating expert demonstrations**. Instead of learning from trial and error, imitation learning leverages existing expert knowledge to train agents.\n",
    "\n",
    "This makes these algorithms appealing as **you don't need to create a reward function for your task** like in situations where the manual approach becomes essential because creating a reward function directly is not feasible, such as when training a self-driving vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The easiest imitation learning algorithm is call BC (Behavioral Cloning) and is just supervised learning on the collected expert data, i.e.:\n",
    "\n",
    "$$ D = \\{(s_0, a_0), (s_1, a_1), \\ldots, (s_T, a_T^o)\\} \\quad \\tag{Dataset} $$\n",
    "\n",
    "$$ L_{BC}(\\theta) = \\frac{1}{2} \\left(\\pi_\\theta(s_t) - a_t\\right)^2 \\tag{Cost function}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are improve versions of BC like DAgger (Dataset Aggregation) where after BC the policy is being rollout and if new states appear a new feedback is to ask the human expert. This could produce a huge improvement, although it could be quite expensive.\n",
    "\n",
    "Pros and cons of these models:\n",
    "\n",
    "**pros**: If you have expert dataset, and you are not worry about safety (i.e. unexpected policy behavior in unknown states) this could be a fast approach.\n",
    "\n",
    "**cons**: In general we don't have access to expert data so this is one of the main issues, but even if we have we will have problems related with distributional shift between our clone policy and the provided dataset. We will see this in a moment in an exercise. Also, many of the properties of the Minari datasets (see exercise notebook) that could appear in reality cannot be handled with simple imitation learning approaches, like for instance the stitching property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are other interesting methods that combine imitation learning and the offline RL methods we will introduce later. Typically, they involve two steps:\n",
    "\n",
    "1 - Modeling data distribution (Imitation learning).\n",
    "\n",
    "2 - Applying offline RL for planning.\n",
    "\n",
    "In the first step, they use more sophisticated techniques for cloning, such as Transformers to generate new trajectories or normalizing flows to fit the state-action data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this exercise, we will work with a simple example of Behavioral cloning (BC) . The goal is to explore some of the issues with imitation learning and become familiar with the Tianshou library for offline RL training!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will create a Minari dataset, as we did previously, but this time we will use the function **create_minari_datasets(...)** which saves some useful metadata. This metadata is important for recreating the environment associated with the data when testing our trained policy.\n",
    "\n",
    "We will also utilize one of our registered behavioral policies, but let's first take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:16.944298Z",
     "start_time": "2023-12-17T10:13:16.912486Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "BEHAVIOR_POLICY = BehaviorPolicyType.behavior_8x8_random_towards_left_within_strip\n",
    "OFFLINE_POLICY = PolicyName.imitation_learning\n",
    "\n",
    "# Two dimensional grid world data\n",
    "OBSTACLE = ObstacleTypes.obst_big_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the policy is not random; it includes expert information, as it has a bias to reach the target**. This is, of course, the preferred situation in realistic problems as well and the perfect situation to apply Behavioral Cloning (BC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure our Minari dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:19.334538Z",
     "start_time": "2023-12-17T10:13:18.622404Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER = \"_exercise_5\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "NUM_STEPS = 10000\n",
    "\n",
    "\n",
    "\n",
    "minari_dataset = create_minari_datasets(\n",
    "    env_name=ENV_NAME,\n",
    "    dataset_name=DATA_SET_NAME,\n",
    "    dataset_identifier=DATA_SET_IDENTIFIER,\n",
    "    version_dataset=VERSION_DATA_SET,\n",
    "    num_colected_points=NUM_STEPS,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    ")\n",
    "\n",
    "data = minari.load_dataset(minari_dataset.data_set_name)\n",
    "print(\"number of episodes collected: \", len(data))\n",
    "#for elem in data:\n",
    "#    print(elem.actions, elem.truncations, elem.terminations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the state-action data distribution. Since we are going to use the Tianshou RL library, we will load the previous dataset into a Tianshou ReplayBuffer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:13:23.245110Z",
     "start_time": "2023-12-17T10:13:21.872463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data saved in /offline_data\n",
    "NAME_EXPERT_DATA = minari_dataset.data_set_name\n",
    "\n",
    "# fill the ReplyBuffer\n",
    "buffer_data = load_buffer_minari(NAME_EXPERT_DATA)\n",
    "data_size = len(buffer_data)\n",
    "\n",
    "# We will need the env for the plots. \n",
    "data_config = MinariDatasetConfig.load_from_file(NAME_EXPERT_DATA)\n",
    "env_config = data_config.initial_config_2d_grid_env\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_config)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.007))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-action distribution makes sense! The agent starts at (0,0), and since the episode length is finite (200 steps), the ball will more likely be around the upper-left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's train our first offline RL algorithm: Imitation Learning.**\n",
    "\n",
    "Before we proceed, let's take a moment to become a little more familiar with the code. Let's spend some minutes reviewing:\n",
    "\n",
    "    a - il_policy.py\n",
    "    b - policy_registry.py\n",
    "    c - training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-17T10:10:59.186992Z",
     "start_time": "2023-12-17T10:10:20.937274Z"
    }
   },
   "outputs": [],
   "source": [
    "#The model policy to be trained.\n",
    "\n",
    "POLICY_NAME = PolicyName.imitation_learning\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 0.1*data_size\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the state-action BC policy distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "NUM_EPISODES = 40 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.007))\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the learned policy has a very similar distribution to the dataset, so they will perform similarly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now visualize the policy**\n",
    "\n",
    "Below the imitation_policy_sampling=False arguments will give us the $\\arg \\max_a \\pi(s|a)$. By setting it to True you will be sampling actions from the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As our dataset includes a fair amount of expert data, by taking the $\\arg \\max_a \\pi(s|a)$, we are able to remove the noise from the data and obtain the expert policy! This is a nice property of imitation learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that in real life, the forbidden zone (black region) in the environment could be a playground or a garden, and there could be a good reason not to go there. So let's use our trained agent and see what happens when we remove the restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the obstacle and examine the state-action distribution. What do you observe? Can you explain it?**\n",
    "\n",
    "Hint: You can create a copy of the previous environment and use ObstacleTypes.obst_free_8x8 in your configuration, like this:\n",
    "\n",
    "    new_env_config = copy(env_config)\n",
    "    new_env_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "    env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), \n",
    "    env_config=new_env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now remove the forbidden region and recreate the environment\n",
    "new_env_config = copy(env_config)\n",
    "new_env_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=new_env_config)\n",
    "\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTRIBUTIONAL SHIFT EFFECT**: As we can see, there is a distributional shift effect in this case, mainly due to out-of-distribution state-action pairs as the agent attempts to move into unexplored regions. This is an undesired effect caused by function approximation, i.e., the DNN policy cannot perfectly capture the state-action distribution in regions where there is little or no data. This effect is highly undesirable and should be eliminated. When close to unexplored regions, the policy will behave unpredictably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a forbidden position to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_env_config = copy(env_config)\n",
    "new_env_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "new_env_config.initial_state = (4,0)\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=new_env_config)\n",
    "\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the behavior of the agent when it starts from out-of-distribution data is entirely unexpected. This is normal because the provided expert policy is only designed to guide the agent from any point within the allowed zone to the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Imitation learning becomes interesting when one has access to noisy expert data, as it provides a means to reduce noise from the expert data.**\n",
    "\n",
    "**Another noteworthy aspect of imitation learning is that it doesn't rely on rewards, making it a less complex solution to the problem of reward shaping in reinforcement learning.**\n",
    "\n",
    "However, in realistic applications, obtaining expert data is often a challenge. In many cases, we have access to only a limited number of trajectories, rendering this method less useful for extracting optimal policies.\n",
    "\n",
    "This is where offline RL algorithms come into play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### References\n",
    "\n",
    "[ Ross et al. 2012 - A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning](https://arxiv.org/abs/1011.0686)\n",
    "\n",
    "[Janner et al. 2021 - Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)\n",
    "\n",
    "[Prudencio et al. 2023 - A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems ](https://arxiv.org/pdf/2203.01387.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
