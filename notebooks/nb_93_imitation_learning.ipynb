{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:55.530895Z",
     "start_time": "2024-05-06T16:44:54.548693Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:55.543363Z",
     "start_time": "2024-05-06T16:44:55.532433Z"
    }
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:55.551636Z",
     "start_time": "2024-05-06T16:44:55.543959Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:56.191797Z",
     "start_time": "2024-05-06T16:44:55.552397Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "load_env_variables()\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import minari\n",
    "import torch\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, register_grid_envs, EnvFactory)\n",
    "from training_rl.offline_rl.custom_envs.utils import (\n",
    "    Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import (\n",
    "    MinariDatasetConfig, create_minari_datasets)\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import \\\n",
    "    offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import \\\n",
    "    offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig, get_trained_policy_path)\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import \\\n",
    "    restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import (compare_state_action_histograms,\n",
    "                                          load_buffer_minari,\n",
    "                                          state_action_histogram)\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env, trajectory_cumulative_rewards_plot)\n",
    "from training_rl.offline_rl.utils import widget_list\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, widget_list\n",
    "from tianshou.data import Batch\n",
    "from training_rl.offline_rl.visualizations.utils import compute_corrected_actions_from_policy_guided\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyRestorationConfigFactoryRegistry\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyFactoryRegistry\n",
    "from training_rl.offline_rl.offline_policies.dagger_torcs_policy import model_dagger_fit\n",
    "from training_rl.offline_rl.offline_trainings.training_decision_transformer import evaluate_on_env\n",
    "from training_rl.offline_rl.offline_policies.decision_transformer_policy import get_decision_transformer_default_config, \\\n",
    "    create_decision_transformer_policy_from_dict\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Imitation Learning </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imitation Learning Exercises\n",
    "\n",
    "**Imitation learning is a supervised learning approach that focuses on learning policies or behaviors by observing and imitating expert demonstrations**.\n",
    "\n",
    "This makes these algorithms appealing as, **you don't need to create a reward function for your task**.\n",
    "\n",
    "We will begin with Behavioral Cloning (BC), as introduced earlier. Later, we will cover the well-known improvement DAGGER, and if time permits, Decision Transformers (DT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our pipeline for offline learning:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:50%;\">\n",
    "\n",
    "In this exercise, we will work with the full pipeline, selecting some imitation learning algorithms in step 4. \n",
    "\n",
    "Note: For the offline RL part, we will use the exact same pipeline but plug in our offline RL algorithms in step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Behavioral Clonning (BC)\n",
    "\n",
    "It has been introduced previously, but let's remember the main ingredients:\n",
    "\n",
    "$$ D = \\{(s_0, a_0), (s_1, a_1), \\ldots, (s_T, a_T)\\} \\quad \\tag{Dataset} $$\n",
    "\n",
    "$$\\text{where} \\quad \\pi_{\\theta^*}(s_t) \\text{ s.t. } \\theta^* = argmin_\\theta  L_{BC}(\\theta) \\tag{learned policy}$$\n",
    "\n",
    "$$\\text{with} \\quad L_{BC}(\\theta) = \\sum_{t=0}^T \\left(\\pi_\\theta(s_t) - a_t\\right)^2 \\tag{Cost function}$$\n",
    "\n",
    "and, $\\pi_\\theta(s_t)$, typically a DNN. **Note that there is no trace of rewards here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise I\n",
    "\n",
    "In this exercise, we will:\n",
    "\n",
    "1 - Study the out-of-distribution (o.o.d.) effect, as mentioned before, in a simple 2D grid-world environment.\n",
    "\n",
    "<img src=\"_static/images/93_imitation_learning.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig.1 Once out-of-distribution BC agent is unlikely to recover </div>\n",
    "\n",
    "2 - Evaluate how well BC handles noisy data, which is a common issue when working with collected expert data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:57.550597Z",
     "start_time": "2024-05-06T16:44:57.449195Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete.value\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_big_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = EnvFactory[ENV_NAME].get_env(render_mode=RenderMode.RGB_ARRAY_LIST,grid_config=env_2D_grid_initial_config)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Behavior policies and dataset configurations**\n",
    "\n",
    "Note that we have more noisy data than expert information, as it is common in realistic problems, where expert (or close-to-expert) data is challenging to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:59.060561Z",
     "start_time": "2024-05-06T16:44:59.046108Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_suboptimal_initial_0_0_final_0_7\n",
    "DATA_SET_IDENTIFIER_I = \"_suboptimal_exercise_I_nb_93\"\n",
    "NUM_STEPS_I = 500\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.random\n",
    "DATA_SET_IDENTIFIER_II = \"_random_exercise_I_nb_93\"\n",
    "NUM_STEPS_II = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize behavior policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:59.773602Z",
     "start_time": "2024-05-06T16:44:59.760719Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "behavior_policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:45:50.361578Z",
     "start_time": "2024-05-06T16:45:03.336824Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=behavior_policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    fps=8.0,\n",
    "    inline=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Collect Minari dataset**\n",
    "\n",
    "We will use the 'create_combined_minari_dataset(...)' function, which utilizes Minari's 'combine_datasets(...)' method in the backend, to merge different datasets.Please, give a look to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create the two collected datasets and merge them\n",
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 3: Feed dataset to Tianshou ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# feed selected Minari dataset into ReplayBuffer\n",
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check that the state-action distributions make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 4-5: Select offline policy and training\n",
    "\n",
    "Before we proceed, let's take a moment to become a little more familiar with the code. Let's spend some minutes reviewing:\n",
    "\n",
    "    a - il_policy.py\n",
    "    b - policy_registry.py\n",
    "    c - offline_training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.imitation_learning\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = len_buffer\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**state-action BC policy distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "NUM_EPISODES = 40 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As expected, the learned policy has a very similar distribution to the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize BC trained policy**\n",
    "\n",
    "Below the imitation_policy_sampling=False arguments will give us the $\\arg \\max_a \\pi(a|s)$. By setting it to True you will be sampling actions from the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "**As our dataset includes a fair amount of expert data, by taking the $\\arg \\max_a \\pi(s|a)$, we are able to remove the noise from the data and obtain the expert policy. This is a nice property of imitation learning! But if the expert data is not enough the BC algorithm will imitate the noise and the policy will be far from optimal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try now the following:\n",
    "\n",
    "**a - Start the agent from a different position. What happens?.**\n",
    "\n",
    "**b - Remove the obstacle and examine the state-action distribution. What do you observe? Can you explain it?.\n",
    "      What do you think would happen if you start the agent from a previously forbidden position?**\n",
    "      \n",
    "**c -  Let's revisit Exercise I and see what happens if you increase the noise significantly, like to 40K-50K?**\n",
    "\n",
    "\n",
    "Hint: You can use a different ObstacleTypes.obst_free_8x8 in your configuration and change the initial state, like this:\n",
    "\n",
    "    NEW_INITIAL_STATE = (1,0)\n",
    "    env.set_new_obstacle_map(ObstacleTypes.obst_free_8x8.value)\n",
    "    env.set_starting_point(NEW_INITIAL_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "obstacle_availables = [ObstacleTypes.obst_free_8x8, ObstacleTypes.obst_big_8x8]\n",
    "selected_obstacle = widget_list(obstacle_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NEW_INITIAL_STATE = (2,2)\n",
    "env.set_starting_point(NEW_INITIAL_STATE)\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value) #ObstacleTypes.obst_big_8x8\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 20\n",
    "env.set_starting_point((0,0))\n",
    "env.set_new_obstacle_map(ObstacleTypes.obst_free_8x8.value)\n",
    "\n",
    "\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keep in mind that in real-life scenarios, the forbidden zone—represented by the black region in the original environment—might correspond to a playground or a garden. There could be valid reasons for avoiding this area, so it's crucial not to enter it. If an agent does venture into this forbidden zone, it should strive to return to in-distribution states. Offline reinforcement learning (RL) addresses this challenge, as we will explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION c**\n",
    "\n",
    "As you can see, increasing the noise level causes the trained policy to mimic the noise, resulting in a far from policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "**DISTRIBUTIONAL SHIFT EFFECT**: As we observed in the previous exercises, a distributional shift effect often occurs, primarily due to out-of-distribution state-action pairs as the agent explores unfamiliar areas. This undesired effect is caused by function approximation, meaning that the DNN policy cannot perfectly represent the state-action distribution in regions with limited or no data. Eliminating this effect is crucial. Near unexplored regions, the policy's behavior becomes unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BC pros and cons\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "a - BC becomes interesting when one has access to noisy expert data, as it provides a means to reduce noise from the expert data.\n",
    "\n",
    "b - Another noteworthy aspect of BC is that it doesn't rely on rewards, making it a less complex solution to the problem of reward shaping in reinforcement learning.\n",
    "\n",
    "**Cons**: \n",
    "\n",
    "a - in realistic applications, obtaining expert data is often a challenge. In many cases, we have access to only a limited number of trajectories, rendering this method less useful for extracting optimal policies. \n",
    "\n",
    "b - as we've observed, encountering o.o.d. scenarios (common during inference when visiting state-actions not included in the training dataset) causes BC to behave unpredictably. This unpredictability arises due to the lack of feedback typically present in online RL settings.\n",
    "\n",
    "c - Additionally, simple imitation learning approaches often struggle to address an important property mentioned in the Minari datasets introduction (nb_91), known as the stitching property. This property refers to the ability of the learned policy to combine suboptimal trajectories into better ones, which is crucial in real-world problems.\n",
    "\n",
    "We will explore later how offline RL provides a strategic approach to address these drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DAGGER (Dataset Aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Improved versions of Behavioral Cloning (BC), such as DAgger (Dataset Aggregation), involve **rolling out the policy after initial BC training**. If **new states** emerge during rollout, **additional feedback is sought from human experts**. While this approach can lead to significant improvements, it can also incur substantial costs.\n",
    "\n",
    "<img src=\"_static/images/93_DAGGER.jpg\" alt=\"Snow\" style=\"width:50%;\">\n",
    "<div class=\"slide title\"> Fig.2: DAGGER idea </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/93_dagger_pseudocode.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig.2: DAGGER pseudocde </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error in Dagger is: $$ \\mathbb{E} \\left[ \\sum_t c(s_t, a_t) \\right] \\sim O(\\epsilon H)$$\n",
    "\n",
    "\n",
    "In summary:\n",
    "\n",
    "**-DAGGER is powerful but impractical: it gradually expands the dataset by incorporating new states and actions with expert knowledge in each iteration.**\n",
    "\n",
    "**-DAGGER aligns more closely with online RL methodologies as in offline RL, states not included in the dataset are inaccessible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TORCS environment\n",
    "\n",
    "TORCS (The Open Racing Car Simulator) is a free 3D car racing game made in C++ (it includes aerodynamics, wheel rotation, car damage, fuel, etc.)\n",
    "\n",
    "<img src=\"_static/images/93_TORCS_simulator.png\" alt=\"Snow\" style=\"width:50%;\">\n",
    "<div class=\"slide title\"> Fig 3: TORCS simulator </div>\n",
    "\n",
    "**We have created a Gymnasium environment for the simulator** that collect info through the server for training.\n",
    "\n",
    "**Observation space**: 19-ray lidar.\n",
    "\n",
    "**Action space**: Steering.\n",
    "\n",
    "**Reward**: Maximized when the car stays aligned with the track center, decreasing otherwise until termination if the car veers off-road.\n",
    "\n",
    "For more details refer to the [manual](https://arxiv.org/pdf/1304.1672) and the Torcs environment in the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please configure TORCS with the following settings (go to RACE - PRACTICE - CONFIGURE RACE):\n",
    "\n",
    "#  Track: E-Track 4\n",
    "#  Driver: scr_server_1\n",
    "\n",
    "os.system('torcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise III\n",
    "\n",
    "In this exercise, we'll train a car using suboptimal data. Behavioral Cloning (BC) alone may not give an optimal policy, but DAGGER with an expert policy can refine it quickly to near-optimal levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 1: Create TORCS Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are creating the default TORCS environment we won't need to create it explicitly\n",
    "ENV_NAME = EnvFactory.torcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize TORCS behavior policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_selected_to_visualize = widget_list([BEHAVIOR_POLICY, BehaviorPolicyType.torcs_expert_policy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    behavior_policy_name=policy_selected_to_visualize.value,\n",
    "    num_frames=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 2: Create Minari dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the policy\n",
    "BEHAVIOR_POLICY = BehaviorPolicyType.torcs_drunk_driver_policy\n",
    "DATA_SET_IDENTIFIER = \"torcs_suboptimal\"\n",
    "NUM_STEPS = 4000\n",
    "\n",
    "config_torcs_data_set = create_minari_datasets(\n",
    "    env_name=ENV_NAME,\n",
    "    dataset_identifier=DATA_SET_IDENTIFIER,\n",
    "    num_colected_points=NUM_STEPS,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    ")\n",
    "\n",
    "_ = os.system(\"pkill torcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 3: feed dataset to ReplayBuffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = config_torcs_data_set.data_set_name\n",
    "buffer_data = load_buffer_minari(DATA_SET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4-5: DAGGER initialization phase\n",
    "\n",
    "**We'll gather data using our suboptimal driver policy and apply BC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config.\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 128\n",
    "NUMBER_TEST_ENVS = 1 # number of environments to test the policy during training\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None  # 1626\n",
    "PERCENTAGE_DATA_PER_EPOCH = 1.0 # a fraction to doesn't use full buffer.\n",
    "DEVICE = \"cuda\" # change to cpu if not gpu available\n",
    "\n",
    "OFFLINE_POLICY_NAME = PolicyName.imitation_learning_torcs # policy selected\n",
    "TRAINED_POLICY_NAME = \"policy_bc_for_dagger.pt\" # name of trained policy\n",
    "\n",
    "buffer_data = load_buffer_minari(DATA_SET_NAME) # feed dataset in Tianshou ReplayBuffer\n",
    "\n",
    "# Create meta-data for trained policy.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=PERCENTAGE_DATA_PER_EPOCH * len(buffer_data),\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_name=TRAINED_POLICY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restore trained policy and visualized it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bc_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bc_policy.load_state_dict(torch.load(str(os.path.join(log_path, TRAINED_POLICY_NAME)), map_location=\"cpu\"))\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    policy_model=trained_bc_policy,\n",
    "    num_frames=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4-5: DAGGER agregation phase\n",
    "\n",
    "## TODO: CONTINUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_expert = BehaviorPolicyRestorationConfigFactoryRegistry.torcs_expert_policy\n",
    "\n",
    "initial_output = compute_corrected_actions_from_policy_guided(\n",
    "    env_name=EnvFactory.torcs,\n",
    "    policy_guide=trained_bc_policy,\n",
    "    policy_a=policy_expert,\n",
    "    #policy_b=trained_bc_policy,\n",
    "    num_steps=DAGGER_NUM_STEPS,\n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DAGGER_ITERS = 4\n",
    "\n",
    "dagger_offline_policy = PolicyFactoryRegistry.dagger_torcs()\n",
    "\n",
    "corrected_actions = np.array(initial_output[\"actions_corrected_policy\"])\n",
    "collected_states = np.array(initial_output[\"collected_states\"])\n",
    "\n",
    "for dagger_iter in range(DAGGER_ITERS):\n",
    "\n",
    "    model_dagger_fit(\n",
    "        input_data=torch.Tensor(collected_states),\n",
    "        target_data=torch.Tensor(corrected_actions),\n",
    "        model=dagger_offline_policy\n",
    "    )\n",
    "\n",
    "    output = compute_corrected_actions_from_policy_guided(\n",
    "        env_name=EnvFactory.torcs,\n",
    "        policy_guide=dagger_offline_policy,\n",
    "        policy_a=policy_expert,\n",
    "        #policy_b=trained_bc_policy,\n",
    "        num_steps=NUM_STEPS,\n",
    "        visualize=True\n",
    "    )\n",
    "\n",
    "    corrected_actions = np.concatenate([np.array(output[\"actions_corrected_policy\"]), corrected_actions], axis=0)\n",
    "    collected_states = np.concatenate([np.array(output[\"collected_states\"]), collected_states], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    policy_model=dagger_offline_policy,\n",
    "    num_frames=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning with Decision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Transformer is an imitation learning approach through a conditional sequence modelling that can generate future actions that achieve the **desired return**, i.e. you condition on the rewards. Note that this is in contrast to RL where you always try to find the policy that maximizes the reward.\n",
    "\n",
    "<img src=\"_static/images/93_decision_transformer.png\" alt=\"Snow\" style=\"width:200%;\">\n",
    "<div class=\"slide title\"> Fig.5: Decision Transformer </div>\n",
    "\n",
    "\n",
    "Basically our policy will be given by:\n",
    "\n",
    "$$\n",
    "\\pi(a_t | s_0, R_0, a_0,  s_1, R_1, a_1,  ... , s_{t-1}, R_{t-1}, a_{t-1}, s_t, R_t)\n",
    "$$ \n",
    "\n",
    "where $R_t$ is the return to go or cumulative reward at time t, i.e. $R_t = \\sum_{t'=0}^{\\infty} r_{t + t' + 1}$ .\n",
    "Observe that this is a bit more intrincate than the algorithms before as here we need to have access to the rewards. This is not always simple as with expert data it is often the human that provides the reward feedback implicitly.\n",
    "\n",
    "\n",
    "The idea is simple:\n",
    "<img src=\"_static/images/93_decision_transformer_2.png\" alt=\"Snow\" style=\"width:200%;\">\n",
    "<div class=\"slide title\"> Fig.6: How Decision Transformers work.  </div>\n",
    "\n",
    "\n",
    "Note that a decision transformer is a powerful way to imitate the behavior you observe in expert data but, it \n",
    "lacks the temporal compositionality (i.e. the property to connect different trajectories to get a new one with a better reward) own by dynamic programming approaches like e.g. Q-learning. As we will see later, offline RL exploits this property quite a lot to find the optimal policy.\n",
    "\n",
    "However, there are other similar methods that use transformers and incorporate, to some extent, the temporal compositional property. For instance, the trajectory transformer (https://arxiv.org/pdf/2106.02039.pdf) is similar to a Decision Transformer in that they both train a transformer architecture, as seen in Fig. 7.,\n",
    "\n",
    "<img src=\"_static/images/93_levine_lecture_traj_transformer.png\" alt=\"stich_traj\" style=\"width:200%;\">\n",
    "<div class=\"slide title\"> Fig.7: Trajectory Transformers.  </div>\n",
    "\n",
    "to predict not only the next action but also the next state and reward. However, the main difference lies in the fact that, after training the transformer and considering it as a probability distribution of trajectories, it takes an additional step of planning using BEAM search:\n",
    "\n",
    "<img src=\"_static/images/93_beam_search.png\" alt=\"stich_traj\" style=\"width:50%;\"> \n",
    "\n",
    "This makes the algorithm more robust, as it allows it to concatenate independent trajectories, as we already discussed and depicted in the figure below.\n",
    "\n",
    "<img src=\"_static/images/stiching.png\" alt=\"stich_traj\" style=\"width:40%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is important to mention that you are not restricted to conditioning solely on the rewards; instead, you could condition on other quantities such as the target state (if applicable) or specific actions (such as the direction the car turns left), among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DT_MODEL = \\\n",
    "        \"../src/training_rl/offline_rl/data/decision_transformers/models/model_1000_ep_1000_steps_halfcheetah/model_d4rl_halfcheetah_medium_v0_April_24_v2.pt\"\n",
    "ENV_NAME = \"HalfCheetah-v3\"  # \"Walker2d-v3\"\n",
    "DATASET_DT_PATH = \"../src/training_rl/offline_rl/data/decision_transformers/d4rl_data/halfcheetah-medium-v0.pkl\"\n",
    "RENDER_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode='rgb_array' if RENDER_MODE else None)\n",
    "\n",
    "decision_transformer_config = get_decision_transformer_default_config()\n",
    "decision_transformer_config[\"device\"] = \"cpu\"\n",
    "device = decision_transformer_config[\"device\"]\n",
    "context_len = decision_transformer_config[\"context_len\"]\n",
    "\n",
    "model = create_decision_transformer_policy_from_dict(\n",
    "    config=decision_transformer_config,\n",
    "    action_space=env.action_space,\n",
    "    observation_space=env.observation_space\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH_TO_DT_MODEL, map_location=device))\n",
    "print(\"Policy loaded from: \", PATH_TO_DT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from itertools import accumulate\n",
    "\n",
    "with open(DATASET_DT_PATH, 'rb') as f:\n",
    "    trajectories_dataset = pickle.load(f)\n",
    "\n",
    "cumulative_rewards_per_episode = []\n",
    "trajectory_length_per_episode = []\n",
    "for trajectory in trajectories_dataset:\n",
    "    cumulative_rewards_per_episode.append(np.sum(trajectory['rewards']))\n",
    "    trajectory_length_per_episode.append(len(trajectory['observations']))\n",
    "\n",
    "    \n",
    "def plot_hist(values, bins, x_label=\"\", y_label=\"\", title=\"\"):    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(values, bins=bins, edgecolor='black')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(\n",
    "    values=cumulative_rewards_per_episode, \n",
    "    bins=10,\n",
    "    x_label='Rewards to go, $R_0$, per episode',\n",
    "    y_label='Frequency',\n",
    "    title='Collected data - Rewards to go',\n",
    ")\n",
    "\n",
    "plot_hist(\n",
    "    values=trajectory_length_per_episode, \n",
    "    bins=10,\n",
    "    x_label='trajectory lengths per episode',\n",
    "    y_label='Frequency',\n",
    "    title='Collected data - trajectory lengths',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison trainig vs inference reward to go**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RTG_TARGET = 3000\n",
    "EVAL_RTG_SCALE = 1000\n",
    "\n",
    "    \n",
    "trajectory_cumulative_rewards_plot(\n",
    "    env=env,\n",
    "    model=model,\n",
    "    initial_R_0=EVAL_RTG_TARGET, \n",
    "    trajectories_data=trajectories_dataset,\n",
    "    eval_rtg_scale=EVAL_RTG_SCALE,\n",
    "    num_episodes = 20,\n",
    "    context_len=context_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trained policy rendering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS_PER_EPISODE=4000\n",
    "RENDER_RTG_TARGET = 1000\n",
    "\n",
    "results = evaluate_on_env(\n",
    "    model= model,\n",
    "    device=\"cpu\",\n",
    "    context_len=context_len,\n",
    "    env=env,\n",
    "    rtg_target=RENDER_RTG_TARGET,\n",
    "    rtg_scale=EVAL_RTG_SCALE,\n",
    "    num_eval_ep=1,\n",
    "    max_test_ep_len=NUM_STEPS_PER_EPISODE,\n",
    "    render=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "329.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
