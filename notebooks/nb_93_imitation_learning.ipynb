{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:17.289865Z",
     "start_time": "2024-05-02T10:16:16.305508Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:17.299137Z",
     "start_time": "2024-05-02T10:16:17.290670Z"
    }
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:17.310502Z",
     "start_time": "2024-05-02T10:16:17.299603Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:24.104889Z",
     "start_time": "2024-05-02T10:16:24.079561Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "load_env_variables()\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import minari\n",
    "import torch\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, register_grid_envs, EnvFactory)\n",
    "from training_rl.offline_rl.custom_envs.utils import (\n",
    "    Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import (\n",
    "    MinariDatasetConfig, create_minari_datasets)\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import \\\n",
    "    offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import \\\n",
    "    offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig, get_trained_policy_path)\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import \\\n",
    "    restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import (compare_state_action_histograms,\n",
    "                                          load_buffer_minari,\n",
    "                                          state_action_histogram)\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env, trajectory_cumulative_rewards_plot)\n",
    "from training_rl.offline_rl.utils import widget_list\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, widget_list\n",
    "from tianshou.data import Batch\n",
    "from training_rl.offline_rl.visualizations.utils import compute_corrected_actions_from_policy_guided\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyRestorationConfigFactoryRegistry\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyFactoryRegistry\n",
    "from training_rl.offline_rl.offline_policies.dagger_torcs_policy import model_dagger_fit\n",
    "from training_rl.offline_rl.offline_trainings.training_decision_transformer import evaluate_on_env\n",
    "from training_rl.offline_rl.offline_policies.decision_transformer_policy import get_decision_transformer_default_config, \\\n",
    "    create_decision_transformer_policy_from_dict\n",
    "\n",
    "\n",
    "# Only for Docker\n",
    "#os.environ['PATH'] = '/opt/conda/bin:/opt/conda/condabin:/tmp/code/.venv/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/jovyan/tfl-training-rl/torcs/BUILD/bin'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Imitation Learning </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning\n",
    "\n",
    "**Imitation learning is a supervise learning approach that focuses on learning policies or behaviors by observing and imitating expert demonstrations**. Instead of learning from trial and error, imitation learning leverages existing expert knowledge to train agents.\n",
    "\n",
    "This makes these algorithms appealing as, **you don't need to create a reward function for your task**, like in situations where the manual approach becomes essential because creating a reward function directly is not feasible, such as when training a self-driving vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Clonning (BC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behavioral Cloning (BC) is one of the simplest imitation learning algorithms, which essentially involves applying supervised learning to expert data collected during demonstrations:\n",
    "\n",
    "\n",
    "$$ D = \\{(s_0, a_0), (s_1, a_1), \\ldots, (s_T, a_T)\\} \\quad \\tag{Dataset} $$\n",
    "\n",
    "$$\\text{where} \\quad \\pi_{\\theta^*}(s_t) \\text{ s.t. } \\theta^* = argmin_\\theta  L_{BC}(\\theta) \\tag{learned policy}$$\n",
    "\n",
    "$$\\text{with} \\quad L_{BC}(\\theta) = \\sum_{t=0}^T \\left(\\pi_\\theta(s_t) - a_t\\right)^2 \\tag{Cost function}$$\n",
    "\n",
    "\n",
    "and, $\\pi_\\theta(s_t)$, typically a DNN. In essence, BC aims to minimize the discrepancy between the actions produced by the learned policy and the actions demonstrated by the expert, making it a straightforward approach to imitation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this exercise, we will work with a simple example of Behavioral cloning (BC) . The goal is to explore some of the issues with imitation learning and become familiar with the Tianshou library for offline RL training!** . \n",
    "\n",
    "**For this exercise, there's no homework required. However, please take some time to familiarize yourself with the code provided. These concepts will be revisited often in subsequent exercises.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will create a Minari dataset, as we did previously, but this time we will use the function **create_minari_datasets(...)** which saves some useful metadata. This metadata is important for recreating the environment associated with the data when testing our trained policy.\n",
    "\n",
    "We will also utilize one of our registered behavioral policies.\n",
    "\n",
    "\n",
    "The pipeline will be the following:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "\n",
    "Before to go on let's take a look to the code structure:\n",
    "\n",
    "<img src=\"_static/images/93_code_structure.png\" alt=\"Snow\" style=\"width:40%;\">\n",
    "\n",
    "Regarding trainings we will use often the function:\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config = OFFLINE_POLICY_CONFIG,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "This is essentially a wrapper around the Tianshou OfflineTrainer class. It takes the OFFLINE_POLICY_CONFIG, which is of type TrainedPolicyConfig and contains useful information about the policy to be trained as well as the dataset used for training. The wrapper then trains the policy in batches of BATCH_SIZE from the dataset, ensuring that the number of steps collected from the data in every epoch does not exceed STEP_PER_EPOCH.\n",
    "\n",
    "Let's go to the exercise now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:29.365116Z",
     "start_time": "2024-05-02T10:16:29.265704Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete.value\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_big_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = EnvFactory[ENV_NAME].get_env(render_mode=RenderMode.RGB_ARRAY_LIST,grid_config=env_2D_grid_initial_config)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavior policies configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:31.260256Z",
     "start_time": "2024-05-02T10:16:31.246809Z"
    }
   },
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_suboptimal_initial_0_0_final_0_7\n",
    "DATA_SET_IDENTIFIER_I = \"_suboptimal_exercise_I_nb_93\"\n",
    "NUM_STEPS_I = 500\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.random\n",
    "DATA_SET_IDENTIFIER_II = \"_random_exercise_I_nb_93\"\n",
    "NUM_STEPS_II = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:31.646890Z",
     "start_time": "2024-05-02T10:16:31.632511Z"
    }
   },
   "outputs": [],
   "source": [
    "behavior_policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T10:16:35.977967Z",
     "start_time": "2024-05-02T10:16:33.206058Z"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=behavior_policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    fps=8.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect Minari dataset**\n",
    "\n",
    "Note that we have more noisy data than expert information, but we still have enough expert data in proportion. This situation is not so common in realistic problems, where expert (or close-to-expert) data is challenging to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")\n",
    "buffer_data = load_buffer_minari(config_combined_data.data_set_name)\n",
    "data_size = len(buffer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the state-action data distribution.** \n",
    "\n",
    "We will be using the Tianshou RL library, so we'll load the previously collected dataset into a Tianshou ReplayBuffer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the state-action distributions make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's train our first offline RL algorithm: Imitation Learning.**\n",
    "\n",
    "Before we proceed, let's take a moment to become a little more familiar with the code. Let's spend some minutes reviewing:\n",
    "\n",
    "    a - il_policy.py\n",
    "    b - policy_registry.py\n",
    "    c - offline_training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model policy to be trained.\n",
    "\n",
    "POLICY_NAME = PolicyName.imitation_learning\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = len_buffer\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the state-action BC policy distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "NUM_EPISODES = 40 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the learned policy has a very similar distribution to the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now visualize the policy**\n",
    "\n",
    "Below the imitation_policy_sampling=False arguments will give us the $\\arg \\max_a \\pi(s|a)$. By setting it to True you will be sampling actions from the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As our dataset includes a fair amount of expert data, by taking the $\\arg \\max_a \\pi(s|a)$, we are able to remove the noise from the data and obtain the expert policy. This is a nice property of imitation learning! But if the expert data is not enough the BC algorithm will imitate the noise and the policy will be far from optimal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try now the following:\n",
    "\n",
    "**a - Start the agent from a different position. What happens?.**\n",
    "\n",
    "**b - Remove the obstacle and examine the state-action distribution. What do you observe? Can you explain it?.\n",
    "      What do you think would happen if you start the agent from a previously forbidden position?**\n",
    "      \n",
    "**c -  Let's revisit Exercise I and see what happens if you increase the noise significantly, like to 40K-50K?**\n",
    "\n",
    "\n",
    "Hint: You can use a different ObstacleTypes.obst_free_8x8 in your configuration and change the initial state, like this:\n",
    "\n",
    "    NEW_INITIAL_STATE = (1,0)\n",
    "    env.set_new_obstacle_map(ObstacleTypes.obst_free_8x8.value)\n",
    "    env.set_starting_point(NEW_INITIAL_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle_availables = [ObstacleTypes.obst_free_8x8, ObstacleTypes.obst_big_8x8]\n",
    "selected_obstacle = widget_list(obstacle_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_INITIAL_STATE = (2,2)\n",
    "env.set_starting_point(NEW_INITIAL_STATE)\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value) #ObstacleTypes.obst_big_8x8\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 20\n",
    "env.set_starting_point((0,0))\n",
    "env.set_new_obstacle_map(ObstacleTypes.obst_free_8x8.value)\n",
    "\n",
    "\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that in real-life scenarios, the forbidden zone—represented by the black region in the original environment—might correspond to a playground or a garden. There could be valid reasons for avoiding this area, so it's crucial not to enter it. If an agent does venture into this forbidden zone, it should strive to return to in-distribution states. Offline reinforcement learning (RL) addresses this challenge, as we will explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION c**\n",
    "\n",
    "As you can see, increasing the noise level causes the trained policy to mimic the noise, resulting in a far from policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTRIBUTIONAL SHIFT EFFECT**: As we observed in the previous exercises, a distributional shift effect often occurs, primarily due to out-of-distribution state-action pairs as the agent explores unfamiliar areas. This undesired effect is caused by function approximation, meaning that the DNN policy cannot perfectly represent the state-action distribution in regions with limited or no data. Eliminating this effect is crucial. Near unexplored regions, the policy's behavior becomes unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error between the behavior and learned policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to be a bit more formal and compute an upper bound for the error between the behavioral and learned policies in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/93_imitation_learning.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig.1 Once out-of-distribution BC agent is unlikely to recover </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a cost function given by: $ c(s,a) = 0  \\text{ if } a = \\pi^\\beta(s)$, otherwise $c(s,a)=1$, where $\\pi^\\beta(s)$ represents the behavior policy.  Additionally, assume that $ \\pi_\\theta (a \\ne \\pi^\\beta(s) | s) \\le \\epsilon \\text{ for all } s \\in D_{\\text{train}} $ . The total error can then be estimated as:\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\sum_t c(s_t, a_t) \\right] \\le \\epsilon H + (1 - \\epsilon) (\\epsilon (H-1) +  ... ) \\sim O(\\epsilon H²)$$\n",
    "\n",
    "with $H$ being the time horizon.\n",
    "\n",
    "Exercise: What is the error if the episode terminate as soon as you are out-of-distribution?\n",
    "\n",
    "Solution: The error is of $O(\\epsilon H)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "a - BC becomes interesting when one has access to noisy expert data, as it provides a means to reduce noise from the expert data.**\n",
    "\n",
    "b - Another noteworthy aspect of BC is that it doesn't rely on rewards, making it a less complex solution to the problem of reward shaping in reinforcement learning.**\n",
    "\n",
    "**Cons**: \n",
    "\n",
    "a - in realistic applications, obtaining expert data is often a challenge. In many cases, we have access to only a limited number of trajectories, rendering this method less useful for extracting optimal policies. \n",
    "\n",
    "b - as we've observed, encountering o.o.d. scenarios (common during inference when visiting state-actions not included in the training dataset) causes BC to behave unpredictably. This unpredictability arises due to the lack of feedback typically present in online RL settings.\n",
    "\n",
    "c - Additionally, simple imitation learning approaches often struggle to address an important property mentioned in the Minari datasets introduction (nb_91), known as the stitching property. This property refers to the ability of the learned policy to combine suboptimal trajectories into better ones, which is crucial in real-world problems.\n",
    "\n",
    "We will explore later how offline RL provides a strategic approach to address these drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Clonning Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAGGER (Dataset Aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved versions of Behavioral Cloning (BC), such as DAgger (Dataset Aggregation), involve **rolling out the policy after initial BC training**. If **new states** emerge during rollout, **additional feedback is sought from human experts**. While this approach can lead to significant improvements, it can also incur substantial costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/93_dagger_pseudocode.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig.2: DAGGER pseudocde </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error in Dagger is: $$ \\mathbb{E} \\left[ \\sum_t c(s_t, a_t) \\right] \\sim O(\\epsilon H)$$\n",
    "\n",
    "**Dagger is powerful but impractical, as it gradually expands the dataset by incorporating new states and actions with expert knowledge in each iteration. This approach aligns more closely with online RL methodologies and is not typically feasible in offline RL, where states not included in the dataset are typically inaccessible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we'll work with a race simulator called TORCS. Our goal is to develop an imitation learning policy that autonomously learns driving techniques from an expert driver, leveraging lidar sensors to navigate a race circuit. We'll initially apply Behavioral Cloning (BC) and then assess the improvements achieved by employing DAGGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TORCS short introduction\n",
    "\n",
    "TORCS (The Open Racing Car Simulator) is a free 3D car racing game made in C++. TORCS allows easy addition of AI controllers for cars, making it popular for AI research in racing. It has a complex physics engine that considers things like aerodynamics, wheel rotation, car damage, and fuel, providing a detailed environment for learning by agents.\n",
    "\n",
    "<img src=\"_static/images/93_TORCS_simulator.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig 3: TORCS simulator </div>\n",
    "\n",
    "In TORCS, cars usually know everything about the game, like the surroundings and other cars, which isn't like real autonomous agents. To make it more realistic, the server acts as a middleman between the game and the player's control of a single car. The controllers are separate programs that talk to TORCS through the server using UDP connections. The server tells the controller what the car senses, and the controller decides how the car should act. This setup makes the controller act like an independent agent in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at TORCS and see how it works.**\n",
    "\n",
    "Please configure TORCS with the following settings (go to RACE - PRACTICE - CONFIGURE RACE):\n",
    "\n",
    "**Track: E-Track 4**\n",
    "**Driver: scr_server_1**\n",
    "\n",
    "These configurations are to be used in the upcoming exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('torcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will focus on training a single car to drive autonomously using collected data that may not be optimal. As a result, Behavioral Cloning (BC) may not produce an optimal policy based on this data alone. However, the DAGGER aggregation method, utilizing an expert policy, has the potential to refine the policy to a near-optimal level relatively quickly.\n",
    "\n",
    "We have integrated the simulator into a gymnasium environment. Our observations will be based on a lidar system with 19 rays, while our actions will primarily involve steering. It's worth noting that the environment also allows control over acceleration and gear, and there is an abundance of observations provided by the simulator.\n",
    "\n",
    "<img src=\"_static/images/93_torcs_circuit.png\" alt=\"Snow\" style=\"width:40%\">\n",
    "<div class=\"slide title\"> Fig 4: We will use the E-Track 4. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create TORCS Minari datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = EnvFactory.torcs\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.torcs_expert_policy_with_noise\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.torcs_expert_policy\n",
    "BEHAVIOR_POLICY_III = BehaviorPolicyType.torcs_drunk_driver_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize TORCS behavior policies**\n",
    "\n",
    "**Pay close attention to the behavior of the 'expert_noisy' policy, noting that the car drifts off the road after a few meters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_selected_to_visualize = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II, BEHAVIOR_POLICY_III])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    behavior_policy_name=policy_selected_to_visualize.value,\n",
    "    num_frames=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create minari dataset**\n",
    "\n",
    "We will train the policy using the 'expert_and_noise' dataset, but we will also collect expert data for later comparison and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME_I = \"torcs_expert_and_noise\"\n",
    "NUM_STEPS_I = 3000\n",
    "DATA_SET_NAME_II = \"torcs_expert\"\n",
    "NUM_STEPS_II = 7000\n",
    "DATA_SET_NAME_III = \"torcs_suboptimal\"\n",
    "NUM_STEPS_III = 10000\n",
    "\n",
    "\n",
    "config_minari_data_I_and_II = create_combined_minari_dataset(\n",
    "    env_name=ENV_NAME,\n",
    "    dataset_names=(DATA_SET_NAME_I, DATA_SET_NAME_II),\n",
    "    num_collected_points=(NUM_STEPS_I, NUM_STEPS_II),\n",
    "    behavior_policy_names=(BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "    combined_dataset_identifier=\"torcs_driver_expert_and_expert_plus_noise\",\n",
    ")\n",
    "\n",
    "\n",
    "config_minari_suboptimal = create_minari_datasets(\n",
    "    env_name = ENV_NAME,\n",
    "    dataset_name = DATA_SET_NAME_III,\n",
    "    num_colected_points = NUM_STEPS_III,\n",
    "    behavior_policy_name = BEHAVIOR_POLICY_III,\n",
    ")\n",
    "\n",
    "collected_datasets_names = [config_minari_data_I_and_II.data_set_name] + config_minari_data_I_and_II.children_dataset_names + [config_minari_suboptimal.data_set_name]\n",
    "\n",
    "\n",
    "_ = os.system(\"pkill torcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III-a\n",
    "\n",
    "a - Use Behavioral Cloning (BC) to train the imitation learning policy based on the \"torcs_expert_and_noise\" dataset. Just let it run for a few epochs.\n",
    "\n",
    "b - Visualized the policy and extract conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_datasets_names = [\"torcs-torcs_driver_expert_and_expert_plus_noise-v0\", \"torcs-torcs_expert_and_noise-v0\", \"torcs-torcs_expert-v0\", \"torcs-torcs_suboptimal-v0\"]\n",
    "collected_datasets = widget_list(collected_datasets_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 6\n",
    "BATCH_SIZE = 128\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None  # 1626\n",
    "PERCENTAGE_DATA_PER_EPOCH = 1.0\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "OFFLINE_POLICY_NAME = PolicyName.imitation_learning_torcs\n",
    "DATA_SET_NOISY_NAME = collected_datasets.value\n",
    "TRAINED_POLICY_NAME = \"policy_bc.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(DATA_SET_NOISY_NAME)\n",
    "data_config = MinariDatasetConfig.load_from_file(DATA_SET_NOISY_NAME)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_NOISY_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=PERCENTAGE_DATA_PER_EPOCH * len(buffer_data),\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_name=TRAINED_POLICY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's visualize the trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_policy_selected = widget_list([TRAINED_POLICY_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bc_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_NOISY_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bc_policy.load_state_dict(torch.load(str(os.path.join(log_path, trained_policy_selected.value)), map_location=\"cpu\"))\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    policy_model=trained_bc_policy,\n",
    "    num_frames=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see what would happen if we drive with the learned policy and compare the actions taken by the learned policy with those that would have been taken by the expert.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = compute_corrected_actions_from_policy_guided(\n",
    "    env_name=ENV_NAME,\n",
    "    policy_guide=trained_bc_policy,\n",
    "    policy_a=BehaviorPolicyType.torcs_expert_policy,\n",
    "    num_steps=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, BC was able to reduce noise and extract expert knowledge from the data! You can compare with the noisy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = compute_corrected_actions_from_policy_guided(\n",
    "    env_name=ENV_NAME,\n",
    "    policy_guide=BehaviorPolicyType.torcs_expert_policy_with_noise,\n",
    "    policy_a=BehaviorPolicyType.torcs_expert_policy,\n",
    "    num_steps=3000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we have taken much more expert data than noise. However, if you increase the amount of data, the policy won't be able to mimic the expert policy at all. Instead, it will imitate the noise, behaving in an unpredictable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III-b\n",
    "\n",
    "Let's now train a BC policy using the \"torcs_suboptimal\" dataset. You can revisit Exercise III-a and rerun the training for a few epochs. What observations do you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**: When the data is suboptimal, the BC algorithm struggles to make improvements. However, as we'll explore in the next exercise, DAGGER can effectively enhance the BC policy using expert feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise III-c\n",
    "\n",
    "**Let's determine the optimal policy using the DAGGER algorithm with the 'torcs_suboptimal' dataset. We'll start by using the BC policy trained in Exercise III-b. If the BC policy isn't well-trained, there's no need to worry, it doesn't need to be robust for DAGGER implementation. Your goal is to implement a basic DAGGER algorithm following the pseudocode outlined in Figure 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 - Restore initial BC policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_SUBOPTIMAL_NAME = \"torcs-torcs_suboptimal-v0\"\n",
    "POLICY_NAME_EXERCISE_III_B = \"policy_bc.pt\"\n",
    "OFFLINE_POLICY_NAME = PolicyName.imitation_learning_torcs\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "DAGGER_NUM_STEPS = 5000\n",
    "DAGGER_POLICY_NAME = \"dagger_trocs.pt\"\n",
    "DAGGER_ITERS = 25\n",
    "\n",
    "\n",
    "buffer_data = load_buffer_minari(DATA_SET_SUBOPTIMAL_NAME)\n",
    "data_config = MinariDatasetConfig.load_from_file(DATA_SET_SUBOPTIMAL_NAME)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_SUBOPTIMAL_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "\n",
    "trained_bc_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_SUBOPTIMAL_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bc_policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_NAME_EXERCISE_III_B), map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 - initial expert correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_expert = BehaviorPolicyRestorationConfigFactoryRegistry.torcs_expert_policy\n",
    "\n",
    "initial_output = compute_corrected_actions_from_policy_guided(\n",
    "    env_name=EnvFactory.torcs,\n",
    "    policy_guide=trained_bc_policy,\n",
    "    policy_a=policy_expert,\n",
    "    #policy_b=trained_bc_policy,\n",
    "    num_steps=DAGGER_NUM_STEPS,\n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3  - create our dagger policy (we could use the previous BC one but we will create a new one).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dagger_offline_policy = PolicyFactoryRegistry.dagger_torcs()\n",
    "\n",
    "corrected_actions = np.array(initial_output[\"actions_corrected_policy\"])\n",
    "collected_states = np.array(initial_output[\"collected_states\"])\n",
    "\n",
    "for dagger_iter in range(DAGGER_ITERS):\n",
    "\n",
    "    model_dagger_fit(\n",
    "        input_data=torch.Tensor(collected_states),\n",
    "        target_data=torch.Tensor(corrected_actions),\n",
    "        model=dagger_offline_policy\n",
    "    )\n",
    "\n",
    "    output = compute_corrected_actions_from_policy_guided(\n",
    "        env_name=EnvFactory.torcs,\n",
    "        policy_guide=dagger_offline_policy,\n",
    "        policy_a=policy_expert,\n",
    "        #policy_b=trained_bc_policy,\n",
    "        num_steps=DAGGER_NUM_STEPS,\n",
    "        visualize=True\n",
    "    )\n",
    "\n",
    "    corrected_actions = np.concatenate([np.array(output[\"actions_corrected_policy\"]), corrected_actions], axis=0)\n",
    "    collected_states = np.concatenate([np.array(output[\"collected_states\"]), collected_states], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    policy_model=dagger_offline_policy,\n",
    "    num_frames=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning with Decision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Transformer is an imitation learning approach through a conditional sequence modelling that can generate future actions that achieve the **desired return**, i.e. you condition on the rewards. Note that this is in contrast to RL where you always try to find the policy that maximizes the reward.\n",
    "\n",
    "<img src=\"_static/images/93_decision_transformer.png\" alt=\"Snow\" style=\"width:200%;\">\n",
    "<div class=\"slide title\"> Fig.5: Decision Transformer </div>\n",
    "\n",
    "\n",
    "Basically our policy will be given by:\n",
    "\n",
    "$$\n",
    "\\pi(a_t | s_0, R_0, a_0,  s_1, R_1, a_1,  ... , s_{t-1}, R_{t-1}, a_{t-1}, s_t, R_t)\n",
    "$$ \n",
    "\n",
    "where $R_t$ is the return to go or cumulative reward at time t, i.e. $R_t = \\sum_{t'=0}^{\\infty} r_{t + t' + 1}$ .\n",
    "Observe that this is a bit more intrincate than the algorithms before as here we need to have access to the rewards. This is not always simple as with expert data it is often the human that provides the reward feedback implicitly.\n",
    "\n",
    "\n",
    "The idea is simple:\n",
    "<img src=\"_static/images/93_decision_transformer_2.png\" alt=\"Snow\" style=\"width:200%;\">\n",
    "<div class=\"slide title\"> Fig.6: How Decision Transformers work.  </div>\n",
    "\n",
    "\n",
    "Note that a decision transformer is a powerful way to imitate the behavior you observe in expert data but, it \n",
    "lacks the temporal compositionality (i.e. the property to connect different trajectories to get a new one with a better reward) own by dynamic programming approaches like e.g. Q-learning. As we will see later, offline RL exploits this property quite a lot to find the optimal policy.\n",
    "\n",
    "However, there are other similar methods that use transformers and incorporate, to some extent, the temporal compositional property. For instance, the trajectory transformer (https://arxiv.org/pdf/2106.02039.pdf) is similar to a Decision Transformer in that they both train a transformer architecture, as seen in Fig. 7.,\n",
    "\n",
    "<img src=\"_static/images/93_levine_lecture_traj_transformer.png\" alt=\"stich_traj\" style=\"width:200%;\">\n",
    "<div class=\"slide title\"> Fig.7: Trajectory Transformers.  </div>\n",
    "\n",
    "to predict not only the next action but also the next state and reward. However, the main difference lies in the fact that, after training the transformer and considering it as a probability distribution of trajectories, it takes an additional step of planning using BEAM search:\n",
    "\n",
    "<img src=\"_static/images/93_beam_search.png\" alt=\"stich_traj\" style=\"width:50%;\"> \n",
    "\n",
    "This makes the algorithm more robust, as it allows it to concatenate independent trajectories, as we already discussed and depicted in the figure below.\n",
    "\n",
    "<img src=\"_static/images/stiching.png\" alt=\"stich_traj\" style=\"width:40%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is important to mention that you are not restricted to conditioning solely on the rewards; instead, you could condition on other quantities such as the target state (if applicable) or specific actions (such as the direction the car turns left), among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DT_MODEL = \\\n",
    "        \"../src/training_rl/offline_rl/data/decision_transformers/models/model_1000_ep_1000_steps_halfcheetah/model_d4rl_halfcheetah_medium_v0_April_24_v2.pt\"\n",
    "ENV_NAME = \"HalfCheetah-v3\"  # \"Walker2d-v3\"\n",
    "DATASET_DT_PATH = \"../src/training_rl/offline_rl/data/decision_transformers/d4rl_data/halfcheetah-medium-v0.pkl\"\n",
    "RENDER_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode='rgb_array' if RENDER_MODE else None)\n",
    "\n",
    "decision_transformer_config = get_decision_transformer_default_config()\n",
    "decision_transformer_config[\"device\"] = \"cpu\"\n",
    "device = decision_transformer_config[\"device\"]\n",
    "context_len = decision_transformer_config[\"context_len\"]\n",
    "\n",
    "model = create_decision_transformer_policy_from_dict(\n",
    "    config=decision_transformer_config,\n",
    "    action_space=env.action_space,\n",
    "    observation_space=env.observation_space\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH_TO_DT_MODEL, map_location=device))\n",
    "print(\"Policy loaded from: \", PATH_TO_DT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from itertools import accumulate\n",
    "\n",
    "with open(DATASET_DT_PATH, 'rb') as f:\n",
    "    trajectories_dataset = pickle.load(f)\n",
    "\n",
    "cumulative_rewards_per_episode = []\n",
    "trajectory_length_per_episode = []\n",
    "for trajectory in trajectories_dataset:\n",
    "    cumulative_rewards_per_episode.append(np.sum(trajectory['rewards']))\n",
    "    trajectory_length_per_episode.append(len(trajectory['observations']))\n",
    "\n",
    "    \n",
    "def plot_hist(values, bins, x_label=\"\", y_label=\"\", title=\"\"):    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(values, bins=bins, edgecolor='black')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(\n",
    "    values=cumulative_rewards_per_episode, \n",
    "    bins=10,\n",
    "    x_label='Rewards to go, $R_0$, per episode',\n",
    "    y_label='Frequency',\n",
    "    title='Collected data - Rewards to go',\n",
    ")\n",
    "\n",
    "plot_hist(\n",
    "    values=trajectory_length_per_episode, \n",
    "    bins=10,\n",
    "    x_label='trajectory lengths per episode',\n",
    "    y_label='Frequency',\n",
    "    title='Collected data - trajectory lengths',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison trainig vs inference reward to go**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_RTG_TARGET = 3000\n",
    "EVAL_RTG_SCALE = 1000\n",
    "\n",
    "    \n",
    "trajectory_cumulative_rewards_plot(\n",
    "    env=env,\n",
    "    model=model,\n",
    "    initial_R_0=EVAL_RTG_TARGET, \n",
    "    trajectories_data=trajectories_dataset,\n",
    "    eval_rtg_scale=EVAL_RTG_SCALE,\n",
    "    num_episodes = 20,\n",
    "    context_len=context_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**trained policy rendering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS_PER_EPISODE=4000\n",
    "RENDER_RTG_TARGET = 1000\n",
    "\n",
    "results = evaluate_on_env(\n",
    "    model= model,\n",
    "    device=\"cpu\",\n",
    "    context_len=context_len,\n",
    "    env=env,\n",
    "    rtg_target=RENDER_RTG_TARGET,\n",
    "    rtg_scale=EVAL_RTG_SCALE,\n",
    "    num_eval_ep=1,\n",
    "    max_test_ep_len=NUM_STEPS_PER_EPISODE,\n",
    "    render=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311.989px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
