{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:55.530895Z",
     "start_time": "2024-05-06T16:44:54.548693Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:55.543363Z",
     "start_time": "2024-05-06T16:44:55.532433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:55.551636Z",
     "start_time": "2024-05-06T16:44:55.543959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:56.191797Z",
     "start_time": "2024-05-06T16:44:55.552397Z"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "load_env_variables()\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, EnvFactory)\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import create_minari_datasets\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig, get_trained_policy_path)\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import compare_state_action_histograms, state_action_histogram\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env, trajectory_cumulative_rewards_plot)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, widget_list\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyRestorationConfigFactoryRegistry\n",
    "from training_rl.offline_rl.offline_policies.dagger_torcs_policy import model_dagger_fit\n",
    "from training_rl.offline_rl.offline_trainings.training_decision_transformer import evaluate_on_env\n",
    "from training_rl.offline_rl.offline_policies.decision_transformer_policy import get_decision_transformer_default_config, \\\n",
    "    create_decision_transformer_policy_from_dict\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from training_rl.offline_rl.visualizations.utils import policy_rollout_torcs_env, \\\n",
    "    compare_policy_decisions_vs_expert_suggestions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if not os.environ.get(\"DISPLAY\"):\n",
    "    os.environ['MUJOCO_GL'] = 'egl'\n",
    "    \n",
    "render_mode = RenderMode.RGB_ARRAY_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Imitation Learning </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imitation Learning Exercises\n",
    "\n",
    "**Imitation learning is a supervised learning approach that focuses on learning policies or behaviors by observing and imitating expert demonstrations**.\n",
    "\n",
    "This makes these algorithms appealing as, **you don't need to create a reward function for your task**.\n",
    "\n",
    "We will begin with Behavioral Cloning (BC), as introduced earlier. Later, we will cover the well-known improvement DAGGER, and if time permits, Decision Transformers (DT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember our pipeline for offline learning:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:50%;\">\n",
    "\n",
    "In this exercise, we will work with the full pipeline, selecting some imitation learning algorithms in step 4. \n",
    "\n",
    "Note: For the offline RL part, we will use the exact same pipeline but plug in our offline RL algorithms in step 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Behavioral Cloning (BC)\n",
    "\n",
    "It has been introduced previously, but let's remember the main ingredients:\n",
    "\n",
    "$$ D = \\{(s_0, a_0), (s_1, a_1), \\ldots, (s_T, a_T)\\} \\quad \\tag{Dataset} $$\n",
    "\n",
    "$$\\text{where} \\quad \\pi_{\\theta^*}(s_t) \\text{ s.t. } \\theta^* = argmin_\\theta  L_{BC}(\\theta) \\tag{learned policy}$$\n",
    "\n",
    "$$\\text{with} \\quad L_{BC}(\\theta) = \\sum_{t=0}^T \\left(\\pi_\\theta(s_t) - a_t\\right)^2 \\tag{Cost function}$$\n",
    "\n",
    "and, $\\pi_\\theta(s_t)$, typically a DNN. **Note that there is no trace of rewards here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise I\n",
    "\n",
    "In this exercise, we will:\n",
    "\n",
    "1 - Study the out-of-distribution (o.o.d.) effect, as mentioned before, in a simple 2D grid-world environment.\n",
    "\n",
    "<img src=\"_static/images/93_imitation_learning.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig.1 Once out-of-distribution BC agent is unlikely to recover </div>\n",
    "\n",
    "2 - Evaluate how well BC handles noisy data, which is a common issue when working with collected expert data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:57.550597Z",
     "start_time": "2024-05-06T16:44:57.449195Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete.value\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_big_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = EnvFactory[ENV_NAME].get_env(render_mode=RenderMode.RGB_ARRAY_LIST,grid_config=env_2D_grid_initial_config)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Behavior policies and dataset configurations**\n",
    "\n",
    "Note that we have more noisy data than expert information, as it is common in realistic problems, where expert (or close-to-expert) data is challenging to collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:59.060561Z",
     "start_time": "2024-05-06T16:44:59.046108Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_suboptimal_initial_0_0_final_0_7\n",
    "DATA_SET_IDENTIFIER_I = \"_suboptimal_exercise_I_nb_93\"\n",
    "NUM_STEPS_I = 500\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.random\n",
    "DATA_SET_IDENTIFIER_II = \"_random_exercise_I_nb_93\"\n",
    "NUM_STEPS_II = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize behavior policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:44:59.773602Z",
     "start_time": "2024-05-06T16:44:59.760719Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "behavior_policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T16:45:50.361578Z",
     "start_time": "2024-05-06T16:45:03.336824Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=behavior_policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    fps=8.0,\n",
    "    inline=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Collect Minari dataset**\n",
    "\n",
    "We will use the 'create_combined_minari_dataset(...)' function, which utilizes Minari's 'combine_datasets(...)' method in the backend, to merge different datasets.Please, give a look to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create the two collected datasets and merge them\n",
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 3: Feed dataset to Tianshou ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# feed selected Minari dataset into ReplayBuffer\n",
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check that the state-action distributions make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 4-5: Select offline policy and training\n",
    "\n",
    "Before we proceed, let's take a moment to become a little more familiar with the code. Let's spend some minutes reviewing:\n",
    "\n",
    "    a - il_policy.py\n",
    "    b - policy_registry.py\n",
    "    c - offline_training.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.imitation_learning\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = len_buffer\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**state-action BC policy distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "NUM_EPISODES = 40 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As expected, the learned policy has a very similar distribution to the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize BC trained policy**\n",
    "\n",
    "Below the imitation_policy_sampling=False arguments will give us the $\\arg \\max_a \\pi(a|s)$. By setting it to True you will be sampling actions from the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Summary and conclusions\n",
    "\n",
    "**As our dataset includes a fair amount of expert data, by taking the $\\arg \\max_a \\pi(s|a)$, we are able to remove the noise from the data and obtain the expert policy. This is a nice property of imitation learning! But if the expert data is not enough the BC algorithm will imitate the noise and the policy will be far from optimal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try now the following:\n",
    "\n",
    "**a - Start the agent from a different position. What happens?.**\n",
    "\n",
    "**b - Remove the obstacle and examine the state-action distribution. What do you observe? Can you explain it?.\n",
    "      What do you think would happen if you start the agent from a previously forbidden position?**\n",
    "      \n",
    "**c -  Let's revisit Exercise I and see what happens if you increase the noise significantly, like to 40K-50K?**\n",
    "\n",
    "\n",
    "Hint: You can use a different ObstacleTypes.obst_free_8x8 in your configuration and change the initial state, like this:\n",
    "\n",
    "    NEW_INITIAL_STATE = (1,0)\n",
    "    env.set_new_obstacle_map(ObstacleTypes.obst_free_8x8.value)\n",
    "    env.set_starting_point(NEW_INITIAL_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "obstacle_availables = [ObstacleTypes.obst_free_8x8, ObstacleTypes.obst_big_8x8]\n",
    "selected_obstacle = widget_list(obstacle_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NEW_INITIAL_STATE = (2,2)\n",
    "env.set_starting_point(NEW_INITIAL_STATE)\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value) #ObstacleTypes.obst_big_8x8\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPISODES = 20\n",
    "env.set_starting_point((0,0))\n",
    "env.set_new_obstacle_map(ObstacleTypes.obst_free_8x8.value)\n",
    "\n",
    "\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "new_keys = [(env.to_xy(state_action[0]), state_action[1]) for state_action in list(state_action_count_data.keys())]\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Keep in mind that in real-life scenarios, the forbidden zone - represented by the black region in the original environment - might correspond to a playground or a garden. There could be valid reasons for avoiding this area, so it's crucial not to enter it. If an agent does venture into this forbidden zone, it should strive to return to in-distribution states. Offline reinforcement learning (RL) addresses this challenge, as we will explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SOLUTION c**\n",
    "\n",
    "As you can see, increasing the noise level causes the trained policy to mimic the noise, resulting in a far from policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conclusions\n",
    "\n",
    "**DISTRIBUTIONAL SHIFT EFFECT**: As we observed in the previous exercises, a distributional shift effect often occurs, primarily due to out-of-distribution state-action pairs as the agent explores unfamiliar areas. This undesired effect is caused by function approximation, meaning that the DNN policy cannot perfectly represent the state-action distribution in regions with limited or no data. Eliminating this effect is crucial. Near unexplored regions, the policy's behavior becomes unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BC pros and cons\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "a - BC becomes interesting when one has access to noisy expert data, as it provides a means to reduce noise from the expert data.\n",
    "\n",
    "b - Another noteworthy aspect of BC is that it doesn't rely on rewards, making it a less complex solution to the problem of reward shaping in reinforcement learning.\n",
    "\n",
    "**Cons**: \n",
    "\n",
    "a - In realistic applications, obtaining expert data is often a challenge. In many cases, we have access to only a limited number of trajectories, rendering this method less useful for extracting optimal policies. \n",
    "\n",
    "b - As we've observed, encountering o.o.d. scenarios (common during inference when visiting state-actions not included in the training dataset) causes BC to behave unpredictably. This unpredictability arises due to the lack of feedback typically present in online RL settings.\n",
    "\n",
    "c - Additionally, simple imitation learning approaches often struggle to address an important property mentioned in the Minari datasets introduction (nb_91), known as the stitching property. This property refers to the ability of the learned policy to combine suboptimal trajectories into better ones, which is crucial in real-world problems.\n",
    "\n",
    "We will explore later how offline RL provides a strategic approach to address these drawbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DAgger (Dataset Aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Improved versions of Behavioral Cloning (BC), such as DAgger (Dataset Aggregation), involve **rolling out the policy after initial BC training**. If **new states** emerge during rollout, **additional feedback is sought from human experts**. While this approach can lead to significant improvements, it can also incur substantial costs.\n",
    "\n",
    "<img src=\"_static/images/93_DAGGER.jpg\" alt=\"Snow\" style=\"width:50%;\">\n",
    "<div class=\"slide title\"> Fig.2: DAGGER idea </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/93_dagger_pseudocode.png\" alt=\"Snow\" style=\"width:80%;\">\n",
    "<div class=\"slide title\"> Fig.2: DAGGER pseudocode </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the error in Dagger is: $$ \\mathbb{E} \\left[ \\sum_t c(s_t, a_t) \\right] \\sim O(\\epsilon H)$$\n",
    "\n",
    "\n",
    "In summary:\n",
    "\n",
    "**-DAgger is powerful but impractical: it gradually expands the dataset by incorporating new states and actions with expert knowledge in each iteration.**\n",
    "\n",
    "**-DAgger introduced a usually complex reward implicitly through the expert feedback**\n",
    "\n",
    "**-DAgger aligns more closely with online RL methodologies as in offline RL, states not included in the dataset are inaccessible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise III\n",
    "\n",
    "In this exercise we will train a self-driving car in the TORCS race simulator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### TORCS race simulator quick overview\n",
    "\n",
    "TORCS (The Open Racing Car Simulator) is a free 3D car racing game made in C++ (it includes aerodynamics, wheel rotation, car damage, fuel, etc.)\n",
    "\n",
    "<img src=\"_static/images/93_TORCS_simulator.png\" alt=\"Snow\" style=\"width:50%;\">\n",
    "<div class=\"slide title\"> Fig 3: TORCS simulator </div>\n",
    "\n",
    "**We have created a Gymnasium environment for the simulator** that collects information through the server for training.\n",
    "\n",
    "**Observation space**: 19-ray lidar.\n",
    "\n",
    "**Action space**: Steering.\n",
    "\n",
    "**Reward**: Maximized when the car stays aligned with the track center, decreasing otherwise until termination if the car veers off-road.\n",
    "\n",
    "For more details refer to the [manual](https://arxiv.org/pdf/1304.1672) and the Torcs environment in the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Please configure TORCS with the following settings (go to RACE - PRACTICE - CONFIGURE RACE):\n",
    "\n",
    "#  Track: E-Track 4\n",
    "#  Driver: scr_server_1\n",
    "\n",
    "os.system('torcs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this exercise, we'll train a self-driving car using suboptimal data. Behavioral Cloning (BC) alone may not give an optimal policy, but DAGGER with an expert policy can refine it quickly to near-optimal levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 1: Create TORCS Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# As we are creating the default TORCS environment we won't need to create it explicitly\n",
    "ENV_NAME = EnvFactory.torcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize TORCS behavior policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "policy_selected_to_visualize = widget_list([\n",
    "    BehaviorPolicyType.torcs_drunk_driver_policy, \n",
    "    BehaviorPolicyType.torcs_expert_policy,\n",
    "    BehaviorPolicyType.torcs_expert_policy_with_noise,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    behavior_policy_name=policy_selected_to_visualize.value,\n",
    "    num_frames=4000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 2: Create Minari dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Configure the policy\n",
    "#BEHAVIOR_POLICY = BehaviorPolicyType.torcs_drunk_driver_policy\n",
    "BEHAVIOR_POLICY = policy_selected_to_visualize.value\n",
    "DATA_SET_IDENTIFIER = \"torcs_suboptimal\"\n",
    "NUM_STEPS = 6000\n",
    "\n",
    "config_torcs_data_set = create_minari_datasets(\n",
    "    env_name=ENV_NAME,\n",
    "    dataset_identifier=DATA_SET_IDENTIFIER,\n",
    "    num_colected_points=NUM_STEPS,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    ")\n",
    "\n",
    "_ = os.system(\"pkill torcs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 3: feed dataset to replay buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "DATA_SET_NAME = config_torcs_data_set.data_set_name\n",
    "buffer_data = load_buffer_minari(DATA_SET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 4-5: DAgger initialization phase\n",
    "\n",
    "**Let's train a BC policy with the collected data** .\n",
    "\n",
    "**Suggestion: You would like to turn-on the graphics to speed up the training**. Open TORCS and go to:\n",
    "\n",
    "RACE/PRACTICE/CONFIGURE/RACE/ACCEPT/ACCEPT and on that screen change display from 'normal' to 'results only'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Training config.\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "NUMBER_TEST_ENVS = 1 # number of environments to test the policy during training\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None  # 1626\n",
    "PERCENTAGE_DATA_PER_EPOCH = 1.0 # a fraction to doesn't use full buffer.\n",
    "DEVICE = \"cuda\" # change to cpu if not gpu available\n",
    "\n",
    "OFFLINE_POLICY_NAME = PolicyName.imitation_learning_torcs # offline policy selected\n",
    "TRAINED_POLICY_NAME = \"policy_bc_for_dagger.pt\" # name of trained policy\n",
    "\n",
    "buffer_data = load_buffer_minari(DATA_SET_NAME) # feed dataset in Tianshou ReplayBuffer\n",
    "\n",
    "# Create meta-data for trained policy.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=PERCENTAGE_DATA_PER_EPOCH * len(buffer_data),\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_name=TRAINED_POLICY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Restored BC trained policy and visualized it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trained_bc_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bc_policy.load_state_dict(torch.load(str(os.path.join(log_path, TRAINED_POLICY_NAME)), map_location=\"cpu\"))\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    policy_model=trained_bc_policy,\n",
    "    num_frames=NUM_STEPS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### STEP 4-5: DAgger aggregation phase\n",
    "\n",
    "**We will ask for expert advise and aggregate that information into our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Expert policy\n",
    "policy_expert = BehaviorPolicyRestorationConfigFactoryRegistry.torcs_expert_policy\n",
    "\n",
    "output_initial_phase = policy_rollout_torcs_env(\n",
    "    driver_policy=trained_bc_policy,\n",
    "    advisor_policy=policy_expert,\n",
    "    env_collected_quantities=\"angle\",\n",
    "    num_steps=NUM_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's compare the decisions made by the BC policy against the ones that would have been taken by the expert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "compare_policy_decisions_vs_expert_suggestions(\n",
    "    policy_actions=output_initial_phase[\"actions_driver\"],\n",
    "    expert_suggestions=output_initial_phase[\"actions_advisor\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "DAGGER_ITERS = 10\n",
    "\n",
    "corrected_actions = np.array(output_initial_phase[\"actions_advisor\"])\n",
    "collected_observations = np.array(output_initial_phase[\"observations\"])\n",
    "\n",
    "# We will make a copy of the bc policy just to get bc in case we need it later\n",
    "dagger_policy = deepcopy(trained_bc_policy)\n",
    "\n",
    "\n",
    "try:\n",
    "        \n",
    "    for dagger_iter in range(DAGGER_ITERS):\n",
    "\n",
    "        model_dagger_fit(\n",
    "            input_data=torch.Tensor(collected_observations),\n",
    "            target_data=torch.Tensor(corrected_actions),\n",
    "            model=dagger_policy,\n",
    "            epochs=1,\n",
    "        )\n",
    "\n",
    "        output_aggregation_phase = policy_rollout_torcs_env(\n",
    "            driver_policy=dagger_policy,\n",
    "            advisor_policy=policy_expert,\n",
    "            env_collected_quantities=\"angle\",\n",
    "            num_steps=NUM_STEPS,\n",
    "        )\n",
    "\n",
    "        compare_policy_decisions_vs_expert_suggestions(\n",
    "            policy_actions=output_aggregation_phase[\"actions_driver\"],\n",
    "            expert_suggestions=output_aggregation_phase[\"actions_advisor\"]\n",
    "        )\n",
    "\n",
    "        corrected_actions = np.concatenate([np.array(output_aggregation_phase[\"actions_advisor\"]), corrected_actions], axis=0)\n",
    "        collected_observations = np.concatenate([np.array(output_aggregation_phase[\"observations\"]), collected_observations], axis=0)\n",
    "\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=None,\n",
    "    policy_model=dagger_offline_policy,\n",
    "    num_frames=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imitation Learning with Decision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Decision Transformer is an imitation learning approach through a conditional sequence modeling that can generate future actions that achieve the desired return**, i.e., you **condition on the rewards**. Note that this is in contrast to RL, where you always try to find the policy that maximizes the reward.\n",
    "\n",
    "<div style=\"margin-top: 10px;\">\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "        <div style=\"width: 90%;\">\n",
    "            <img src=\"_static/images/93_decision_transformer.png\" alt=\"KL divergence\" width=\"100%\">\n",
    "            <div class=\"slide title\"> Fig.5: Decision Transformer </div>\n",
    "        </div>\n",
    "        <div style=\"width: 10%;\"></div> <!-- Empty div for space in the middle -->\n",
    "        <div style=\"width: 90%;\">\n",
    "            <img src=\"_static/images/93_decision_transformer_2.png\" alt=\"Your Second Image\" width=\"100%\">\n",
    "        </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Basically our policy will be given by: \n",
    "\n",
    "$$\\pi(a_t | s_0, R_0, a_0,  s_1, R_1, a_1,  ... , s_{t-1}, R_{t-1}, a_{t-1}, s_t, R_t)$$,\n",
    "\n",
    "where $R_t$ is the return to go or cumulative reward at time t, i.e. $R_t = \\sum_{t'=0}^{\\infty} r_{t + t' + 1}$ .\n",
    "    \n",
    "Note that a **decision transformer** is a powerful way to imitate the behavior you observe in expert data but, **it lacks the temporal compositionality** (i.e. the stitching property to connect different trajectories to get a new one with a better reward) own by dynamic programming approaches like e.g. Q-learning. As we will see later, offline RL exploits this property quite a lot to find the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, there are other similar methods that use transformers and incorporate, to some extent, the temporal compositional property. For instance, the trajectory transformer (https://arxiv.org/pdf/2106.02039.pdf) is similar to a Decision Transformer in that they both train a transformer architecture, as seen in Fig. 6.,\n",
    "\n",
    "<img src=\"_static/images/93_levine_lecture_traj_transformer.png\" alt=\"stich_traj\" style=\"width:70%;\">\n",
    "<div class=\"slide title\"> Fig.6: Trajectory Transformers.  </div>\n",
    "\n",
    "to predict not only the next action but also the next state and reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, the main difference lies in the fact that, after training the transformer and considering it as a probability distribution of trajectories, it takes an additional step of planning using BEAM search:\n",
    "\n",
    "<div style=\"margin-top: 10px;\">\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "        <div style=\"width: 30%;\">\n",
    "            <img src=\"_static/images/93_beam_search.png\" alt=\"KL divergence\" width=\"100%\">\n",
    "            <div class=\"slide title\"> Fig.7: Left: BEAM search - Right: stitching property. </div>\n",
    "        </div>\n",
    "        <div style=\"width: 10%;\"></div> <!-- Empty div for space in the middle -->\n",
    "        <div style=\"width: 20%;\">\n",
    "            <img src=\"_static/images/stiching.png\" alt=\"Your Second Image\" width=\"100%\">\n",
    "        </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, it is important to mention that you are not restricted to conditioning solely on the rewards; instead, you could condition on other quantities such as the target state (if applicable) or specific actions (such as the direction the car turns left), among others. There are different algorithms that explode these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_DT_MODEL = \"../src/training_rl/offline_rl/data/decision_transformers/models/model_d4rl_walker2d-medium-v1_May_10_v0.pt\"\n",
    "#PATH_TO_DT_MODEL = \\\n",
    "#        \"../src/training_rl/offline_rl/data/decision_transformers/models/model_1000_ep_1000_steps_halfcheetah/model_d4rl_halfcheetah_medium_v0_April_24_v2.pt\"\n",
    "ENV_NAME =  \"Walker2d-v3\" #\"HalfCheetah-v3\" \n",
    "#DATASET_DT_PATH = \"../src/training_rl/offline_rl/data/decision_transformers/d4rl_data/halfcheetah-medium-v0.pkl\"\n",
    "DATASET_DT_PATH = \"../src/training_rl/offline_rl/data/decision_transformers/d4rl_data/walker2d-medium-v1.pkl\"\n",
    "RENDER_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Create the environment and restore trained model**\n",
    "\n",
    "The model has already been trained (see offline_rl/offline_trainings/training_decision_transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME, render_mode='rgb_array' if RENDER_MODE else None)\n",
    "\n",
    "decision_transformer_config = get_decision_transformer_default_config()\n",
    "decision_transformer_config[\"device\"] = \"cpu\"\n",
    "device = decision_transformer_config[\"device\"]\n",
    "context_len = decision_transformer_config[\"context_len\"]\n",
    "\n",
    "model = create_decision_transformer_policy_from_dict(\n",
    "    config=decision_transformer_config,\n",
    "    action_space=env.action_space,\n",
    "    observation_space=env.observation_space\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH_TO_DT_MODEL, map_location=device))\n",
    "print(\"Policy loaded from: \", PATH_TO_DT_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's take a look at the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with open(DATASET_DT_PATH, 'rb') as f:\n",
    "    trajectories_dataset = pickle.load(f)\n",
    "\n",
    "cumulative_rewards_per_episode = []\n",
    "trajectory_length_per_episode = []\n",
    "for trajectory in trajectories_dataset:\n",
    "    cumulative_rewards_per_episode.append(np.sum(trajectory['rewards']))\n",
    "    trajectory_length_per_episode.append(len(trajectory['observations']))\n",
    "\n",
    "    \n",
    "def plot_hist(values, bins, x_label=\"\", y_label=\"\", title=\"\"):    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(values, bins=bins, edgecolor='black')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_hist(\n",
    "    values=cumulative_rewards_per_episode, \n",
    "    bins=10,\n",
    "    x_label='Rewards to go, $R_0$, per episode',\n",
    "    y_label='Frequency',\n",
    "    title='Collected data - Rewards to go',\n",
    ")\n",
    "\n",
    "plot_hist(\n",
    "    values=trajectory_length_per_episode, \n",
    "    bins=10,\n",
    "    x_label='trajectory lengths per episode',\n",
    "    y_label='Frequency',\n",
    "    title='Collected data - trajectory lengths',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Comparison of rewards to go: training vs inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "EVAL_RTG_TARGET = 500\n",
    "EVAL_RTG_SCALE = 1000\n",
    "\n",
    "    \n",
    "trajectory_cumulative_rewards_plot(\n",
    "    env=env,\n",
    "    model=model,\n",
    "    initial_R_0=EVAL_RTG_TARGET, \n",
    "    trajectories_data=trajectories_dataset,\n",
    "    eval_rtg_scale=EVAL_RTG_SCALE,\n",
    "    num_episodes = 20,\n",
    "    context_len=context_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Visualize the trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "NUM_STEPS_PER_EPISODE=4000\n",
    "RENDER_RTG_TARGET = 600\n",
    "EVAL_RTG_SCALE = 1000\n",
    "\n",
    "results = evaluate_on_env(\n",
    "    model= model,\n",
    "    device=\"cpu\",\n",
    "    context_len=context_len,\n",
    "    env=env,\n",
    "    rtg_target=RENDER_RTG_TARGET,\n",
    "    rtg_scale=EVAL_RTG_SCALE,\n",
    "    num_eval_ep=1,\n",
    "    max_test_ep_len=NUM_STEPS_PER_EPISODE,\n",
    "    render=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "339.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
