{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Offline Reinforcement Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What kind of data is used in offline RL?\n",
    "\n",
    "- only actions: BC, inverse RL\n",
    "- perfect trajectories and rewards: modified BC, critic regularized regression, CQL\n",
    "- imperfect trajectories: want to improve on behavior policy. BQL, IQL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Typical problems in offline RL\n",
    "\n",
    "- Distribution shift -> Show example\n",
    "- Argmax in learned critic is problematic (like adversarial attack) -> Show example\n",
    "- Poor behavior policy\n",
    "- If only actions are collected - what is the reward? Inverse RL is hard and ambiguous\n",
    "- Poor examples are not collected at all by good policies (driving car off the road), major distribution shift. Solution - something like Dagger, if access to env is possible somehow (show a Dagger example?)\n",
    "- Standard off-policy algorithms don't work well since errors in critic are not corrected by collecting more samples (compare SAC and AWAC)\n",
    "- ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why offline RL then?\n",
    "- Might be no other choice (no access to env)\n",
    "- Way easier to implement (supervised learning, no sampling loop)\n",
    "- When no clear reward is given (navigating drones, self-driving cars, robot arms, etc.)\n",
    "- As a pre-training step before an actual RL algo (AWAC and follow-up papers) -> Show example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Software for offline RL\n",
    "- d4rl -> minari\n",
    "- supervised libraries, but also tianshou"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples:\n",
    "- how to collect data\n",
    "- mujoco, data collected with experts - CRR, BC.\n",
    "- mujoco, suboptimal policy - BQL, IQL?\n",
    "- using d4rl/minari datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning algorithms primarily rely on an online learning approach, which poses a significant challenge to their widespread use. RL typically involves a continuous process of gathering experience by engaging with the environment using the latest policy, and then using this experience to enhance the policy. **In many situations, this online interaction is neither practical nor safe due to costly or risky data collection, as seen in fields such as robotics, healthcare, etc.**.\n",
    "\n",
    "**Even in cases where online interaction is viable, there is often a preference to leverage existing data**. This is particularly true in complex domains where substantial datasets are essential for effective generalization.\n",
    "\n",
    "**Offline Reinforcement Learning(RL), also known as Batch Reinforcement Learning, is a variant of RL that effectively leverages large, previously collected datasets for large-scale real-world applications**. The use of static datasets means that during the training process of the agent, offline RL does not perform any form of online interaction and exploration, which is also the most significant difference from online reinforcement learning methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the problem to be solved?\n",
    "\n",
    "The offline RL problem can be defined as a data-driven approach to the previously seen online RL methods. As before, the goal is to minimize the discounted expected reward:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "J (\\pi) = \\mathbb{E}_{\\tau \\sim D}  \\left[ \\sum_{t = 0}^{\\infty} \\gamma^t r (s_t, a_t) \\right]\n",
    "\\tag{1}\n",
    "\\label{eq:discounted_rew_offline}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "But this is done directly on the dataset $D$:\n",
    "\n",
    "$$\n",
    "D = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\ldots, (s_T, a_T, r_T)\\} \\quad \\tag{Dataset}\n",
    "$$\n",
    "\n",
    "comprising state/action/reward values. This is in contrast to online RL, where the trajectories $\\tau$ are collected through interaction with the environment. **Note that $D$ doesn't necessarily need to be related to the specific task at hand, but it should be sufficiently representative (i.e., containing high-reward regions of the problem). If $D$ is derived from data collected from unrelated tasks, we will need to design our own reward function, just as we did in the online RL setting.**\n",
    "\n",
    "The dataset can be collected from a suboptimal policy, a random policy, provided by a human expert, or a mixture of them. Such a policy is called a behavior policy (or expert policy), denoted as $\\pi_b$.\n",
    "\n",
    "As an example of a behavior policy ($\\pi_b$), when training a robot to navigate a room, $\\pi_b$ might involve rules like \"avoid obstacles,\" \"move forward,\" or \"stop.\" It could be operated manually by a human who controls the robot based on sensor data, such as lidar for collision detection, or it could be a combination of suboptimal policies.\n",
    "\n",
    "**The goal of offline RL is to derive an optimal or near-optimal policy directly from $D$ without requiring further interactions with the environment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important points:\n",
    "\n",
    "- Offline RL differs from \"imitation learning\" (discussed later), which is its supervised learning counterpart. **In imitation learning, the policy closely mirrors the behavior policy, while offline RL aims for a superior policy, ideally near the optimal one.**\n",
    "\n",
    "- Offline RL brings complexity because the offline policy differs in state-action distributions from the behavior policy, leading to **distributional shift challenges**. Similar challenges may arise in behavioral cloning, discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples where offline RL could be highly beneficial:\n",
    "    \n",
    "**Decision's Making in Health Care**: In healthcare, we can use Markov decision processes to model the diagnosis and treatment of patients. Actions are interventions like tests and treatments, while observations are patient symptoms and test results. Offline RL is safer and more practical than active RL, as treating patients directly with partially trained policies is risky.\n",
    "\n",
    "**Learning Robotic Manipulation Skills**: In robotics, we can use active RL for skill learning, but generalizing skills across different environments is challenging. Offline RL allows us to reuse previously collected data from various skills to accelerate learning of new skills. For example, making soup with onions and carrots can build on experiences with onions and meat or carrots and cucumbers, reducing the need for new data collection.\n",
    "\n",
    "**Learning Goal-Directed Dialogue Policies**: Dialogue systems, like chatbots helping users make purchases, can be modeled as MDPs. Collecting data by interacting with real users can be costly. Offline data collection from humans is a more practical approach for training effective conversational agents.\n",
    "\n",
    "**Autonomous Driving**: Training autonomous vehicles in real-world environments can be dangerous and costly. Offline RL can use data from past driving experiences to improve vehicle control and decision-making, making it safer and more efficient.\n",
    "\n",
    "**Energy Management**: Optimizing energy consumption in buildings or industrial processes can be a critical task. Offline RL can analyze historical energy usage data to develop efficient control strategies, reducing energy costs and environmental impact.\n",
    "\n",
    "**Finance**: Portfolio management and trading strategies often require learning from historical financial data. Offline RL can help develop and refine investment policies by utilizing past market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the process is clear:\n",
    "\n",
    "**Phase A**: Collect data set, $D$, of state-action pairs through a behavior policy $\\pi_b$: e.g. a robot randomly moving (or human controlled) in a given space, data collected from an autonomous vehicle, etc. The data collected doesn't need to come from an expert (typically the case in real situations) and during this phase we are not worry in general about a specific task (i.e. rewards). In fact, it could be that the data is collected from a robot doing a different task that the one we are interested in. We want just a set of allowed state-action pairs that could be usable and representative for the task in mind.\n",
    "\n",
    "**Phase B**: In this phase we want to solve a given task (so we need to design rewards) only through the provided initial data without any interaction with the environment but still be able to find an optimal or near-optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Today, we will explore different points in offline Reinforcement Learning and in particular**:\n",
    "\n",
    "    1. The technical challenges in offline RL.\n",
    "    \n",
    "    2. Differences between approaches like imitation learning and online vs. offline RL, \n",
    "       and what we can adapt from online methods for the offline setting.\n",
    "\n",
    "       \n",
    "    3. Standard data collection approaches and libraries used in the offline RL \n",
    "       community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our exercises, we'll primarily use a 2D grid environment for two important reasons:\n",
    "\n",
    "\n",
    "    1. It allows for quick training and data collection, making trainings and \n",
    "       data-collection quite fast giving you the possibility to play around in the \n",
    "       workshop!\n",
    "       \n",
    "    \n",
    "    2. Simplifying and customizing the environment to introduce varying levels of complexity in a controlled \n",
    "       manner, along with the option to create your own straightforward behavior policies, can facilitate a\n",
    "       clearer exploration of the core concepts and advantages of offline Reinforcement Learning (RL), which \n",
    "       can be quite challenging or almost impossible with high-dimensional spaces.\n",
    "\n",
    "**Please note that both the provided library (/offline_rl) and the exercises (notebooks and /offline_rl/scripts) are adaptable to tackle more intricate environments and tasks. Don't hesitate to experiment with them on your own. Be prepared for some patience, as training in RL can be time-consuming when dealing with complex problems!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[ Levine et al. '2021 - Offline Reinforcement Learning: Tutorial, Review,\n",
    "and Perspectives on Open Problems ](https://arxiv.org/pdf/2005.01643.pdf).\n",
    "\n",
    "[Prudencio et al. ' 2023 - A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems ](https://arxiv.org/pdf/2203.01387.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
