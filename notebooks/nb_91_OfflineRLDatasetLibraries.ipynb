{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Offline-RL open source datasets </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Open Source Datasets libraries for offline RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**The goal of offline RL or imitation learning is to learn a policy from a fixed dataset. This approach has gained significant attention because it allows RL methods to utilize vast, pre-collected datasets, somewhat similar to how large datasets have propelled advances in supervised learning.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Data collection from sensors or cameras is now easy in many areas like robotics, automotive, and manufacturing. We need a standard way to organize and process this data, including custom data from real machines or simulations. Before, offline RL didn't have a standard method. New algorithms needed a lot of data preprocessing, especially with large datasets, which was costly.\n",
    "\n",
    "The MINARI library was made to handle offline RL data tasks efficiently. It also offers a diverse collection of datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MINARI Dataset\n",
    "(previously called D4RL from UC Berkeley/Google Brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Minari is becoming the standard choice, replacing D4RL in the community. Its main goal is to standardize data handling and offer open-source datasets tailored for offline RL.**\n",
    "\n",
    "These datasets match real-world needs and are crucial for testing and improving offline algorithms. Minari provides datasets with random, medium, and expert policies in various environments, helping us evaluate whether an algorithm can extract meaning from noise. \n",
    "\n",
    "These datasets can be used for benchmarking, as existing benchmarks for online RL are not suitable for offline RL, as we'll discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In particular the provided datasets focus mainly on the following properties that appear often in realistic situations:\n",
    "\n",
    "1 - **Narrow and biased data distributions**: e.g. from deterministic policies: Narrow datasets may arise in human demonstrations or in hand-crafted policies. **(not an issue in online RL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2 - **Undirected and multitask data**: refers to data collected without a specific task in mind, such as internet user interactions or videos for autonomous driving. Although gathered without a clear scope, this data is intended for solving specific tasks. Offline RL aims to identify and use the most rewarding paths within this data.\n",
    "\n",
    "The main purpose is to test how well the offline agent can be used for \"trajectory stitching,\" which involves combining trajectories from different tasks to achieve new objectives, rather than searching for out-of-distribution trajectories.\n",
    "\n",
    "<img src=\"_static/images/stiching.png\" alt=\"stich_traj\" style=\"width:200px;\">\n",
    "\n",
    "As seen in the figure, suppose we have collected data from our car only for paths 1-2 (green) and 2-3 (yellow); we should be able to use this data to teach our car to go from 1-3. **(not an issue in online RL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3 - **Sparse rewards**: pose challenges in online setups because they are closely tied to exploration. In offline RL, we explore only within the dataset, making it perfect for studying how algorithms handle sparse rewards. Crafting effective rewards can be difficult, and overly complex rewards may lead to suboptimal results. On the other hand, sparse rewards are often easier to design since they just define the task's success criteria, making them attractive to work with\n",
    "\n",
    "<img src=\"_static/images/91_2d_maze.png\" alt=\"Snow\" style=\"width:30%;\">\n",
    "<div class=\"slide title\"> Sparse reward scenario: the reward is 0 everywhere except at the end, where it is equal to 1. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4 - **Suboptimal data**: Give a clear task the data could not contain any optimal trajectory so this is a realistic scenario in general and still the offline agent should be able to find the best trajectory within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "5 - **Non-representable behavior policies**: non-Markovian behavior policies, and partial observability. For instance, if the data is collected with a classical control algorithm that have access to a window of previous states. **(not an issue in online RL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6 - **Realistic domains**: Different Mujoco tasks as robot manipulation or multi-tasking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's give a look to [Minari](https://minari.farama.org/main/content/basic_usage/) and in particular to the provided datasets. (~ 20 minutes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Useful minari methods we will use in the workshop\n",
    "\n",
    "**DataCollector (gymnasium.Wrapper) class**: Collects data ${state_i, action_i, reward_i, termination_i, truncation_i, info_i, i=1,..,H}$ through a Gymnasium environment by performing rollouts of $\\pi_b(a|s)$, similar to what we did in the online RL section.\n",
    "\n",
    "**DataCollector.create_dataset(...)**: Saves the dataset to storage.\n",
    "\n",
    "**minari.create_dataset_from_buffer(...)**: Converts historical data to Minari format. Requires to preprocess your data as explained in the Minari documentation.\n",
    "\n",
    "**minari.load_dataset(...)/minari.list_local_datasets()**: Loads/lists Minari GCP registered datasets.\n",
    "\n",
    "**minari.combine_datasets(...)**: Merges two Minari datasets into one.\n",
    "\n",
    "**Minari also includes other functionalities, such as splitting datasets or saving metadata, but our focus will primarily be on the previously mentioned methods.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minari dataset structure\n",
    "\n",
    "Here is the typical Minari dataset folder structure:\n",
    "\n",
    "<img src=\"_static/images/nb_91_minari_dataset.png\" alt=\"minari_folder_structure\" style=\"width:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And this is how the data is saved in the .hdf5 file:\n",
    "\n",
    "<img src=\"_static/images/nb_91_minari_dataset_2.png\" alt=\"minari_folder_structure\" style=\"width:300px;\">\n",
    "\n",
    "\n",
    "HDF5 is an open-source file format for large, complex data. It uses a file directory structure, compresses data for smaller file sizes, and allows data slicing to process subsets without loading everything into memory.\n",
    "\n",
    "Minari stores datasets in HDF5 format using h5py. HDF5 organizes data into groups and datasets, supports data slicing, and allows custom meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RL Unplugged dataset\n",
    "\n",
    "(Deep Mind - Google Brain) [github](https://github.com/google-deepmind/deepmind-research/tree/master/rl_unplugged) and [blog](https://www.deepmind.com/blog/rl-unplugged-benchmarks-for-offline-reinforcement-learning)\n",
    "\n",
    "The tasks are diverse, but the key point is that **most datasets are based on behavior policies trained online. This means the data might not accurately represent real-world scenarios where human experts and non-RL policies are common**. Also, the majority of data comes from medium to expert policies. While not perfect, these datasets are valuable for benchmarking algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Open X-Embodiment Repository\n",
    "October 2023 - Partners from 33 academic labs.\n",
    "\n",
    "This [library](https://robotics-transformer-x.github.io/) introduced the **Open X-Embodiment Repository** that **includes a dataset with 22 different robot types** for X-embodiment learning, i.e. to learn from diverse and large-scale datasets from multiple robots for better transfer learning and improved generalization.\n",
    "\n",
    "[Let's give a look](https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[ \\[Fu.Justin et. al. '2021 \\] D4RL: Datasets for Deep Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "\n",
    "[ MINARI: A dataset API for Offline Reinforcement Learning ](https://minari.farama.org/main/content/basic_usage/) \n",
    "\n",
    "[ C. Gulcehre et al. '2021, “RL unplugged: A suite of benchmarks for offline\n",
    "reinforcement learning](https://arxiv.org/abs/2006.13888)\n",
    "\n",
    "[ A. Padalkar et. al. '2023 Open X-Embodiment: Robotic Learning Datasets and RT-X Models ](https://robotics-transformer-x.github.io/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def create_dropdown(options):\n",
    "    # Create the dropdown widget\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=options,\n",
    "        value=options[0],  # Default value\n",
    "        description='Select an option:'\n",
    "    )\n",
    "\n",
    "    # Define a function to handle the dropdown value change\n",
    "    def handle_dropdown_change(change):\n",
    "        print(f'Selected option: {change.new}')\n",
    "\n",
    "    # Attach the event handler to the dropdown widget\n",
    "    dropdown.observe(handle_dropdown_change, names='value')\n",
    "\n",
    "    # Display the dropdown widget\n",
    "    display(dropdown)\n",
    "\n",
    "# Define the options for the dropdown menu\n",
    "options = ['Option 1', 'Option 2', 'Option 3']\n",
    "\n",
    "# Call the create_dropdown function with the options\n",
    "create_dropdown(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
