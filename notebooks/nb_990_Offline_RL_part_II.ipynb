{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches in Offline RL to Address Distributional Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various approaches, but the core idea is to strike a balance where the policy distribution remains reasonably close to the behavioral one while also improving its performance. This involves introducing some distributional shift to enhance the policy without going out of distribution, all while ensuring that the effective sample size remains large enough to be representative during inference. Achieving this balance is a challenging task and a highly active area of research in the RL community.\n",
    "\n",
    "To attain the aforementioned goal, offline RL algorithms can be classified into three primary categories:\n",
    "\n",
    "**I - Policy constraint**\n",
    "\n",
    "**II - Policy Regularization**\n",
    "\n",
    "**III - Importance sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I: **Policy constraint:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I-a) Non-implicit or Direct: We have access to the behavior policy**, $\\bf \\pi_\\beta$. For instance it could be a suboptimal classical policy (i.e. non RL) or computed from behavioral cloning on a given dataset.\n",
    "\n",
    "As we already have $\\pi_\\beta$ we can constrain the learned and behavioral policy through:\n",
    "\n",
    "\\begin{align*}\n",
    "D_{KL}(\\pi(a|s)||\\pi_{\\beta}(a|s)) \\leq \\epsilon\n",
    "\\end{align*}\n",
    "\n",
    "and as shown in (ref.1 )we can bound $D_{KL}(d_{\\pi}(s)||d_{\\pi_{\\beta}}(s))$ by $\\delta$, which is $O\\left(\\frac{\\epsilon}{{(1 - \\gamma)}^2}\\right)$ . Here $d_{\\pi}(s)$ is the state visitation frequency induced by the policy $\\pi$. In summary if $d_{\\pi}(s)$ and $d_{\\pi_{\\beta}}(s)$ are close enough this will guarantee that the state distributions will be similar and so the space of states that we visit during data collection will be similar to the one we will encounter in inference.\n",
    "\n",
    "Basically this kind of methods will use this constraint in actor-critic like algorithms, i.e.:\n",
    "\n",
    "$$\n",
    "{\\hat Q}^{\\pi}_{k+1} \\leftarrow \\arg \\min_Q \\mathbb{E}_{(s,a,s')\\sim D} \\left[ Q(s, a) - \\left( r(s, a) + \\gamma \\mathbb{E}_{a' \\sim\\pi_k(a'|s')}[{\\hat Q}^{\\pi}_k(s', a')] \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}_{k+1}}(s, a) \\right] \\\\\n",
    "\\text{s.t. } D(\\pi, \\pi_{\\beta}) \\leq \\epsilon.\n",
    "$$\n",
    "\n",
    "\n",
    "We could also add the constraint in the evaluation an improvement steps, i.e. ( What is the difference?):\n",
    "\n",
    "$$\n",
    "{\\hat Q}^{\\pi}_{k+1} \\leftarrow \\arg \\min_Q \\mathbb{E}_{(s,a,s')\\sim D} \\left[ Q(s, a) - \\left( r(s, a) + \\gamma \\mathbb{E}_{a' \\sim\\pi_k(a'|s')}[{\\hat Q}^{\\pi}_k(s', a')] -\\alpha\\gamma D(\\pi_k(\\cdot|s'), \\pi_\\beta(\\cdot|s')) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}_{k+1}}(s, a) -\\alpha\\gamma D(\\pi_k(\\cdot|s), \\pi_\\beta(\\cdot|s)) \\right] \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "However, in some situations we will need to deviate considerably from the behavior policy to find optimal actions and the $D_{KL}$ constraint could be too conservative.\n",
    "\n",
    "To overcome this issues another approach is to constraint the policies but in their support, i.e. in the space of action where they are defined, as see in the figure below.\n",
    "\n",
    "<img src=\"_static/imagespolicy_constraint_vs_support.png\" alt=\"offline_rl_4\" width=500cm>\n",
    "\n",
    "ToDo: Give an example of support matching!! --> see 2023 review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I-b) Implicit: We don't need $\\pi_\\beta$, and we can work directly with our data $D$**. This is the situation many times as the lack of data or in complex high dimensional spaces cloning a policy that match the real data distribution could be extremely hard.\n",
    "\n",
    "In this approach you assume that you have a behavioral policy $\\mu$ (this will be integrated out later) and so you want to find a better one $\\pi$. What you could do is to maximize the difference reward:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\eta(\\pi) = J(\\pi) - J(\\mu) \\quad \\hbox{with} \\quad J (\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}  \\left[ \\sum_{t = 0}^{\\infty} \\gamma^t r (s_t, a_t) \\right] \n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "It can be shown that (1) can be written as this (similar to Trust Region Policy Optimization (TRPO) derivation):\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\eta(\\pi) = \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\mathbb{E}_{a \\sim \\pi(a|s)} [A^{\\mu}(s, a)] \\\\ \\text{s.t.} \\quad D(\\pi(\\cdot|s) || \\mu(\\cdot|s) ) \\leq \\epsilon\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "what it makes sense intuitively as by maximizing (2) we are trying to find tha state-action pairs, i.e the $(s,a)$'s, generated from $\\pi$ that will produce the trajectories on the dataset with maximum cumulative reward (i.e. maximum $A^\\mu(s,a)$), in other words the best trajectories in our dataset! However, we need to restrict the (s,a) pairs to be close to the dataset and that's the reason of the $D_{KL}$ divergence.\n",
    "\n",
    "At this point (2) can be formulated as a constrained optimization problem in a Lagrangian language:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "L(\\pi, \\beta) =  \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\mathbb{E}_{a \\sim \\pi(a|s)} [A^{\\mu}(s, a)] + \\lambda \\left( \\epsilon -  D(\\pi(\\cdot|s) || \\mu(\\cdot|s)) \\right)\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "and this can be maximized easily so after some algebra we find that:\n",
    "\n",
    "$\n",
    "\\pi^*(a|s) = \\frac{1}{Z(s)} \\mu(a|s) \\exp\\left(\\frac{1}{\\lambda} A^{\\mu}(s, a)\\right) \\tag{4}.\n",
    "$\n",
    "\n",
    "So $\\pi^*$ will be the optimal policy. But what we can do now is to approximate it by a DNN\n",
    ", $\\pi_\\theta$ and again we can impose that $\\pi_\\theta$ and $\\pi^*$ be close distributions on the dataset, i.e.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi_\\theta (a|s) = argmin_{\\pi_\\theta} \\mathbb{E}_{s \\sim d\\mu(s)} \\left[ D(\\pi^*(\\cdot|s) \\, \\Vert \\, \\pi_\\theta(\\cdot|s)) \\right] = \\\\\n",
    "\\arg\\max_{\\pi_\\theta} \\mathbb{E}_{s\\sim d\\mu(s)}\\mathbb{E}_{a\\sim\\mu(a|s)} \\left[ \\log \\pi_\\theta(a|s) \\exp\\left(\\frac{1}{\\beta} A^{\\mu}(s, a)\\right) \\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "So again simple intuition: pairs (s,a) that could produce potentially high trajectory rewards on the dataset will be preferred by $\\pi_\\theta(a | s)$.\n",
    "\n",
    "\n",
    "\n",
    "are computing the expectation value of the policy $\\mu$ advantage, $A^\\mu(s,a)$, so basically the mean cumulative reward followed by $mu$ but starting from a state-action pair sampled from $\\pi$.\n",
    "\n",
    "\n",
    "\n",
    "It can be shown that given two policies $\\pi$ and $\\mu$ the following general result holds:\n",
    "\n",
    "$\\eta(\\pi) = \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\mathbb{E}_{a \\sim \\pi(a|s)} [A^{\\mu}(s, a)] = \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\mathbb{E}_{a \\sim \\pi(a|s)} \\left[ R^{\\mu}_{s,a} - V^{\\mu}(s) \\right]\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\underset{\\pi}{\\text{arg max}} \\int \\int \\frac{d\\mu(s)}{da} \\frac{d\\pi(a|s)}{ds} (R^{\\mu}_{s,a} - V^{\\mu}(s)) \\, da \\, ds \\tag{5}\\\\\n",
    "\\text{s.t.} \\int \\frac{d\\mu(s)}{ds} \\text{DKL}(\\pi(\\cdot|s) || \\mu(\\cdot|s)) \\, ds \\leq \\epsilon\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "This derivation comes from the AWR offpolicy algorithm but there are slightly different implementations like the AWAC that uses an offpolicy Q-function $Q_π$ to estimate the advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy constraint methods are powerful, but they can be often be too pessimistic, which is always undesirable. For instance, if we know that a certain state has all actions with zero reward, we should not care about constraining the policy in this state once it can inadvertently affect our neural network approximator while forcing the learned policy to be close to the behavior policy in this irrelevant state. We effectively limit how good of a policy we can learn from our dataset by being too pessimistic.\n",
    "\n",
    "Also, as we use function approximation on these methods this could produce some issues for instance when we fit an unimodal policy into multimodal data. In that case, policy constraint methods can fail dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Regularization is an alternative approach to ensuring the robustness of learned value functions, specifically Q-functions. **This approach involves regularizing the value function directly, aiming to prevent overestimation, especially for actions that fall outside the distribution seen during training**.\n",
    "\n",
    "It's versatile, applicable to different RL methods, including actor-critic and Q-learning methods, and doesn't necessitate explicit behavior policy modeling (similar to the implicit constraint methods).\n",
    "\n",
    "\n",
    "Perhaps one of the most famous examples is the CQL (Conservative Q-Learning) algorithm that introduces the following constraint as Q-value regularization:\n",
    "\n",
    "\\begin{equation}\n",
    "CCQL_0(D, \\phi) = E_{s\\sim D, a\\sim \\mu(a|s)}[Q_{\\phi}(s, a)]\\ \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Note that if we choose  $Q_\\phi$ that minimizes (1) then we will be minimizing the Q-values in all states of the dataset. Suppose now that we choose the policy $\\mu$ in an adversarial way such that it maximizes the constraint (1) then the net effect will be that the penalty will push down on high Q-values. This is what is rigorously shown in the CQL paper where they found that the solution of:\n",
    "\n",
    "$\\hat{Q}^{k+1}_{\\text{CQL}} \\gets \\hbox{argmin}_Q \\left[ \\color{red} {\\alpha\\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\mu}[Q(s,a)] } + \\frac{1}{2} \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q(s,a) - \\mathcal{B}^{\\pi}Q(s,a)\\big)^2\\Big] \\right]. \\tag{2}$\n",
    "\n",
    "produces a lower bound for Q(s,a). There are different choices of $\\mu$. If you could choose $\\mu$ as $\\pi$ but also there are other choices, but these are technical details (see ... for more details).\n",
    "\n",
    "\n",
    "In summary, CQL employs a conservative penalty mechanism, which pushes down on high Q-values by choosing an adversarial behavior policy µ(a|s). This promotes cautious Q-value estimation, particularly for out-of-distribution actions. The chosen µ(a|s) and penalty weight α are critical factors in this process, leading to a provably conservative Q-learning or actor-critic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[Schulman et al. 2017 - Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477.pdf)\n",
    "\n",
    "[Kumar et al. 2020 - Conservative Q-Learning for Offline Reinforcement Learning](https://arxiv.org/pdf/2006.04779.pdf)\n",
    "\n",
    "[ Levine et al. 2021 - Offline Reinforcement Learning: Tutorial, Review,\n",
    "and Perspectives on Open Problems ](https://arxiv.org/pdf/2005.01643.pdf)\n",
    "\n",
    "[Peng et al. 2019 - Simple and Scalable Off-Policy Reinforcement Learning](https://arxiv.org/abs/1910.00177)\n",
    "\n",
    "[Nair et al. '2020 - AWAC: Accelerating Online Reinforcement Learning with Offline Datasets](https://arxiv.org/abs/2006.09359)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
