{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Training RL Agents </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training RL Agents\n",
    "\n",
    "In this notebook we will dive a bit deeper into RL by training agents on the pendulum environment\n",
    "that we already encountered before and analyze the results.\n",
    "We will be concerned with questions like reward shaping, stability of results,\n",
    "generalization to non-training situations and other issues related to real-world applications of RL.\n",
    "\n",
    "Here we will only look at model-free RL since model-based RL will often require some domain specific\n",
    "algorithms and engineering. Also, the openly available tools for model-based RL are far less mature\n",
    "than for model-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.classic_control import PendulumEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "from tianshou.env import ShmemVectorEnv\n",
    "from tianshou.highlevel.config import SamplingConfig\n",
    "from tianshou.highlevel.env import ContinuousEnvironments, EnvFactory\n",
    "from tianshou.highlevel.experiment import (\n",
    "    ExperimentConfig,\n",
    ")\n",
    "from training_rl.env_utils import demo_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir log --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanilla pendulum\n",
    "\n",
    "Let us start by simply using gym's pendulum as is and training a soft actor critic (an off-policy algorithm) on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.highlevel.params.lr_scheduler import LRSchedulerFactoryLinear\n",
    "from typing import Literal\n",
    "import torch\n",
    "from tianshou.highlevel.params.dist_fn import DistributionFunctionFactoryIndependentGaussians\n",
    "from tianshou.highlevel.params.policy_params import PPOParams\n",
    "from tianshou.highlevel.experiment import PPOExperimentBuilder\n",
    "\n",
    "\n",
    "def train_ppo_agent(\n",
    "    env_factory: EnvFactory,\n",
    "    experiment_config: ExperimentConfig = None,\n",
    "    buffer_size: int = 4096,\n",
    "    hidden_sizes: Sequence[int] = (64, 64),\n",
    "    lr: float = 3e-4,\n",
    "    gamma: float = 0.99,\n",
    "    epoch: int = 100,\n",
    "    step_per_epoch: int = 30000,\n",
    "    step_per_collect: int = 2048,\n",
    "    repeat_per_collect: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    training_num: int = 64,\n",
    "    test_num: int = 10,\n",
    "    rew_norm: bool = True,\n",
    "    vf_coef: float = 0.25,\n",
    "    ent_coef: float = 0.0,\n",
    "    gae_lambda: float = 0.95,\n",
    "    bound_action_method: Literal[\"clip\", \"tanh\"] | None = \"clip\",\n",
    "    lr_decay: bool = True,\n",
    "    max_grad_norm: float = 0.5,\n",
    "    eps_clip: float = 0.2,\n",
    "    dual_clip: float | None = None,\n",
    "    value_clip: bool = False,\n",
    "    norm_adv: bool = False,\n",
    "    recompute_adv: bool = True,\n",
    "):\n",
    "    experiment_config = experiment_config or ExperimentConfig()\n",
    "    log_name = os.path.join(\"ppo\", str(experiment_config.seed))\n",
    "\n",
    "    sampling_config = SamplingConfig(\n",
    "        num_epochs=epoch,\n",
    "        step_per_epoch=step_per_epoch,\n",
    "        batch_size=batch_size,\n",
    "        num_train_envs=training_num,\n",
    "        num_test_envs=test_num,\n",
    "        buffer_size=buffer_size,\n",
    "        step_per_collect=step_per_collect,\n",
    "        repeat_per_collect=repeat_per_collect,\n",
    "    )\n",
    "\n",
    "    experiment = (\n",
    "        PPOExperimentBuilder(env_factory, experiment_config, sampling_config)\n",
    "        .with_ppo_params(\n",
    "            PPOParams(\n",
    "                discount_factor=gamma,\n",
    "                gae_lambda=gae_lambda,\n",
    "                action_bound_method=bound_action_method,\n",
    "                reward_normalization=rew_norm,\n",
    "                ent_coef=ent_coef,\n",
    "                vf_coef=vf_coef,\n",
    "                max_grad_norm=max_grad_norm,\n",
    "                value_clip=value_clip,\n",
    "                advantage_normalization=norm_adv,\n",
    "                eps_clip=eps_clip,\n",
    "                dual_clip=dual_clip,\n",
    "                recompute_advantage=recompute_adv,\n",
    "                lr=lr,\n",
    "                lr_scheduler_factory=LRSchedulerFactoryLinear(sampling_config)\n",
    "                if lr_decay\n",
    "                else None,\n",
    "                dist_fn=DistributionFunctionFactoryIndependentGaussians(),\n",
    "            ),\n",
    "        )\n",
    "        .with_actor_factory_default(hidden_sizes, torch.nn.Tanh, continuous_unbounded=True)\n",
    "        .with_critic_factory_default(hidden_sizes, torch.nn.Tanh)\n",
    "        .build()\n",
    "    )\n",
    "    experiment_result = experiment.run(log_name)\n",
    "    return experiment_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pendulum_env(render_mode: Literal[\"rgb_array\"] | None = None):\n",
    "    return TimeLimit(PendulumEnv(render_mode=render_mode), max_episode_steps=200)\n",
    "\n",
    "\n",
    "class PendulumEnvFactory(EnvFactory):\n",
    "    def create_envs(\n",
    "        self, num_training_envs: int, num_test_envs: int\n",
    "    ) -> ContinuousEnvironments:\n",
    "        env = get_pendulum_env()\n",
    "        train_envs = ShmemVectorEnv([get_pendulum_env] * num_training_envs)\n",
    "        test_envs = ShmemVectorEnv([get_pendulum_env] * num_test_envs)\n",
    "        return ContinuousEnvironments(\n",
    "            env=env,\n",
    "            train_envs=train_envs,\n",
    "            test_envs=test_envs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_result = train_ppo_agent(\n",
    "    PendulumEnvFactory(), epoch=1, step_per_epoch=20000, training_num=10,test_num=1\n",
    ")\n",
    "policy = exp_result.world.policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pend_env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = pend_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.compute_action(obs, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_model(pend_env, policy.compute_action, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "As you see, the reward is not easily interpretable. In fact, it is composed of different quantities (do have a look at the source\n",
    "code of the environment, it is also provided below). How would you go about evaluating an agent's performance? Think about an\n",
    "evaluation strategy and put it into code. It might contain the average time needed until the pendulum is stabilized, the average\n",
    "torque per unit time, the average angular distance travelled by the pendulum or other metrics you find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Rewards\n",
    "\n",
    "With the vanilla pendulum, the agent receives rewards continuously based on the angle. The reward also\n",
    "contains information about the torque. What if we tried training with sparse rewards, where we motivate\n",
    "the agent to move a pendulum to a certain angle-range as fast as possible and to leave it there?\n",
    "\n",
    "Since gym environments are not modular, we cannot easily modify the reward. We could follow the strategies\n",
    "outlined before to change the reward (and in a real project this would be the way to go). However, for\n",
    "educational purposes we will instead copy-paste gym's source code and simply modify it according to our needs\n",
    "prior to each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.classic_control import utils\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "DEFAULT_X = np.pi\n",
    "DEFAULT_Y = 1.0\n",
    "\n",
    "\n",
    "class CustomPendulumEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ## Description\n",
    "\n",
    "    The inverted pendulum swingup problem is based on the classic problem in control theory.\n",
    "    The system consists of a pendulum attached at one end to a fixed point, and the other end being free.\n",
    "    The pendulum starts in a random position and the goal is to apply torque on the free end to swing it\n",
    "    into an upright position, with its center of gravity right above the fixed point.\n",
    "\n",
    "    The diagram below specifies the coordinate system used for the implementation of the pendulum's\n",
    "    dynamic equations.\n",
    "\n",
    "    ![Pendulum Coordinate System](/_static/diagrams/pendulum.png)\n",
    "\n",
    "    -  `x-y`: cartesian coordinates of the pendulum's end in meters.\n",
    "    - `theta` : angle in radians.\n",
    "    - `tau`: torque in `N m`. Defined as positive _counter-clockwise_.\n",
    "\n",
    "    ## Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` representing the torque applied to free end of the pendulum.\n",
    "\n",
    "    | Num | Action | Min  | Max |\n",
    "    |-----|--------|------|-----|\n",
    "    | 0   | Torque | -2.0 | 2.0 |\n",
    "\n",
    "\n",
    "    ## Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(3,)` representing the x-y coordinates of the pendulum's free\n",
    "    end and its angular velocity.\n",
    "\n",
    "    | Num | Observation      | Min  | Max |\n",
    "    |-----|------------------|------|-----|\n",
    "    | 0   | x = cos(theta)   | -1.0 | 1.0 |\n",
    "    | 1   | y = sin(theta)   | -1.0 | 1.0 |\n",
    "    | 2   | Angular Velocity | -8.0 | 8.0 |\n",
    "\n",
    "    ## Rewards\n",
    "\n",
    "    The reward function is defined as:\n",
    "\n",
    "    *r = -(theta<sup>2</sup> + 0.1 * theta_dt<sup>2</sup> + 0.001 * torque<sup>2</sup>)*\n",
    "\n",
    "    where `$\\theta$` is the pendulum's angle normalized between *[-pi, pi]* (with 0 being in the upright position).\n",
    "    Based on the above equation, the minimum reward that can be obtained is\n",
    "    *-(pi<sup>2</sup> + 0.1 * 8<sup>2</sup> + 0.001 * 2<sup>2</sup>) = -16.2736044*,\n",
    "    while the maximum reward is zero (pendulum is upright with zero velocity and no torque applied).\n",
    "\n",
    "    ## Starting State\n",
    "\n",
    "    The starting state is a random angle in *[-pi, pi]* and a random angular velocity in *[-1,1]*.\n",
    "\n",
    "    ## Episode Truncation\n",
    "\n",
    "    The episode truncates at 200 time steps.\n",
    "\n",
    "    ## Arguments\n",
    "\n",
    "    - `g`: acceleration of gravity measured in *(m s<sup>-2</sup>)* used to calculate the pendulum dynamics.\n",
    "      The default value is g = 10.0 .\n",
    "\n",
    "    ```python\n",
    "    import gymnasium as gym\n",
    "    gym.make('Pendulum-v1', g=9.81)\n",
    "    ```\n",
    "\n",
    "    On reset, the `options` parameter allows the user to change the bounds used to determine\n",
    "    the new random state.\n",
    "\n",
    "    ## Version History\n",
    "\n",
    "    * v1: Simplify the math equations, no difference in behavior.\n",
    "    * v0: Initial versions release (1.0.0)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, g=10.0, target_angle_range=(-np.pi / 5, np.pi / 5)):\n",
    "        self.target_angle_range = target_angle_range\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = g\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_dim = 500\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
    "        # This will throw a warning in tests/envs/test_envs in utils/env_checker.py as the space is not symmetric\n",
    "        #   or normalised as max_torque == 2 by default. Ignoring the issue here as the default settings are too old\n",
    "        #   to update to follow the gymnasium api\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        costs = angle_normalize(th) ** 2 + 0.1 * thdot**2 + 0.001 * (u**2)\n",
    "\n",
    "        newthdot = thdot + (3 * g / (2 * l) * np.sin(th) + 3.0 / (m * l**2) * u) * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "        newth = th + newthdot * dt\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        min_th, max_th = self.target_angle_range\n",
    "        angle_cost = 0 if min_th < angle_normalize(th) < max_th else 1\n",
    "        return self._get_obs(), -costs, False, False, {}\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        if options is None:\n",
    "            high = np.array([DEFAULT_X, DEFAULT_Y])\n",
    "        else:\n",
    "            # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "            # state/observations.\n",
    "            x = options.get(\"x_init\") if \"x_init\" in options else DEFAULT_X\n",
    "            y = options.get(\"y_init\") if \"y_init\" in options else DEFAULT_Y\n",
    "            x = utils.verify_number_and_cast(x)\n",
    "            y = utils.verify_number_and_cast(y)\n",
    "            high = np.array([x, y])\n",
    "        low = -high  # We enforce symmetric limits.\n",
    "        self.state = self.np_random.uniform(low=low, high=high)\n",
    "        self.last_u = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot], dtype=np.float32)\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            assert self.spec is not None\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym.make(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[classic-control]`\"\n",
    "            ) from e\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_dim, self.screen_dim)\n",
    "                )\n",
    "            else:  # mode in \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_dim, self.screen_dim))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_dim, self.screen_dim))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        bound = 2.2\n",
    "        scale = self.screen_dim / (bound * 2)\n",
    "        offset = self.screen_dim // 2\n",
    "\n",
    "        rod_length = 1 * scale\n",
    "        rod_width = 0.2 * scale\n",
    "        l, r, t, b = 0, rod_length, rod_width / 2, -rod_width / 2\n",
    "        coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        transformed_coords = []\n",
    "        for c in coords:\n",
    "            c = pygame.math.Vector2(c).rotate_rad(self.state[0] + np.pi / 2)\n",
    "            c = (c[0] + offset, c[1] + offset)\n",
    "            transformed_coords.append(c)\n",
    "        gfxdraw.aapolygon(self.surf, transformed_coords, (204, 77, 77))\n",
    "        gfxdraw.filled_polygon(self.surf, transformed_coords, (204, 77, 77))\n",
    "\n",
    "        gfxdraw.aacircle(self.surf, offset, offset, int(rod_width / 2), (204, 77, 77))\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf, offset, offset, int(rod_width / 2), (204, 77, 77)\n",
    "        )\n",
    "\n",
    "        rod_end = (rod_length, 0)\n",
    "        rod_end = pygame.math.Vector2(rod_end).rotate_rad(self.state[0] + np.pi / 2)\n",
    "        rod_end = (int(rod_end[0] + offset), int(rod_end[1] + offset))\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf, rod_end[0], rod_end[1], int(rod_width / 2), (204, 77, 77)\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf, rod_end[0], rod_end[1], int(rod_width / 2), (204, 77, 77)\n",
    "        )\n",
    "\n",
    "        fname = path.join(path.dirname(__file__), \"assets/clockwise.png\")\n",
    "        img = pygame.image.load(fname)\n",
    "        if self.last_u is not None:\n",
    "            scale_img = pygame.transform.smoothscale(\n",
    "                img,\n",
    "                (scale * np.abs(self.last_u) / 2, scale * np.abs(self.last_u) / 2),\n",
    "            )\n",
    "            is_flip = bool(self.last_u > 0)\n",
    "            scale_img = pygame.transform.flip(scale_img, is_flip, True)\n",
    "            self.surf.blit(\n",
    "                scale_img,\n",
    "                (\n",
    "                    offset - scale_img.get_rect().centerx,\n",
    "                    offset - scale_img.get_rect().centery,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # drawing axle\n",
    "        gfxdraw.aacircle(self.surf, offset, offset, int(0.05 * scale), (0, 0, 0))\n",
    "        gfxdraw.filled_circle(self.surf, offset, offset, int(0.05 * scale), (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        else:  # mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_env = TimeLimit(CustomPendulumEnv(), max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_result = train_ppo_agent(\n",
    "    PendulumEnvFactory(), epoch=1, step_per_epoch=20000, training_num=10,test_num=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferring to perturbed environments\n",
    "\n",
    "The environment assumes a fixed mass. What if were to apply the same agent on an env with a different mass?\n",
    "Note that planning algorithms a la MPC would have no problem with this at all, their performance would not\n",
    "go down as long as mass is included in the dynamics-model.\n",
    "\n",
    "Not so for the \"real RL\" agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_pendulum_env(render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "demo_model(env, policy.compute_action, num_steps=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pendulum is balanced upright but an excessive amount of torque is being applied constantly.\n",
    "How could we improve this situation?\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Try the following: we randomize the pendulum's mass at reset of episodes and also add mass to the observations.\n",
    "For that wou will again follow the *bad-practice* and modify the environment by overriding methods directly.\n",
    "Don't do this in a real project! Part of the reason for doing it here is to highlight how cumbersome and fragile such\n",
    "a software design becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "There are many possibilities to extend the experiments done above. You could try:\n",
    "\n",
    "    1. Also changing l and g and adding them to the observation.\n",
    "    2. Normalizing all observations to lie within 0 and 1 (or at least between -1 and 1)\n",
    "    3. What if we could not observe the angular velocity? Remove the velocity from the observation.\n",
    "       This renders the decision process non-Markovian and partially observed.\n",
    "       However, adding a single past observation is sufficient to restore the Markov property.\n",
    "       Add a history of previous observations and actions to the environment. You can use\n",
    "       gymnasium's `FrameStack` and `FlattenObservation` wrappers for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
