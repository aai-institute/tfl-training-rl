{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T20:30:25.731444Z",
     "start_time": "2024-04-29T20:30:25.095932Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import \\\n",
    "    BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import \\\n",
    "    ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import (\n",
    "    CustomEnv, RenderMode, register_grid_envs)\n",
    "from training_rl.offline_rl.custom_envs.utils import (\n",
    "    Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset \n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.utils import generate_compatible_minari_dataset_name\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import \\\n",
    "    offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import \\\n",
    "    offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig, get_trained_policy_path)\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import \\\n",
    "    restore_trained_offline_policy\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, widget_list\n",
    "from training_rl.offline_rl.visualizations.utils import snapshot_env\n",
    "from training_rl.offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions, snapshot_env)\n",
    "from training_rl.offline_rl.utils import (compare_state_action_histograms,\n",
    "                                          load_buffer_minari,\n",
    "                                          state_action_histogram)\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import MinariDatasetConfig\n",
    "\n",
    "\n",
    "load_env_variables()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "register_grid_envs()\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Offline RL algorithms exercises </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline RL algorithms exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we saw that off-policy methods cannot learn from data efficiently unless a fair amount of data covering a large part of your environmnet states is available. Only in that case the agent is able to explore the environment and get feedback similarly as you will do in on-line approach. However, this is a rare situation and, as we discussed before, hard to fullfil in realistic application and this is one of the reason we would like to use offline RL were just a small amount of data is available.\n",
    "\n",
    "We also saw that one of the major issues when you apply off-policy methods to collected data is the tendency of the agent to go o.o.d. and more importantly its imposibility to get back to the in-distribution region, as once it goes o.o.d. the policy behaves unpredictable and this error propagates in the policy evaluation process (i.e. the dynamic programming equations) and the algorithm is not able to learn anything. \n",
    "\n",
    "\n",
    "## Exercise I\n",
    "\n",
    "**Similarly as we did in the offpolicy notebook (nb_95) we will collect a few amount of expert data and a larger amount of suboptimal data .... and we will see how the two offline rl algorithms introduce before, BCQ and CQL, are able to recover the expert policy without going o.o.d. We will compare our results with the imitation learning approach, in particular with the BC algorithm, that as we saw in the imitation learning section (nb_93) is another valid option, as far as you have expert data.**\n",
    "\n",
    "\n",
    "In this exercise we will collect two datasets with expert and suboptimal data that tries to bring the agent from (3,0) to (0,7) .\n",
    "\n",
    "I  - **Suboptimal expert policy**:  collect ~ 2000 steps\n",
    "\n",
    "II - **expert policy**: collect ~ 500 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Policies and Minari DataSet configurations\n",
    "\n",
    "**Create the environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.obstacle_8x8_top_right\n",
    "INITIAL_STATE = (3, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configure behavior policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_suboptimal_rnd_initial_3_0_final_3_7\n",
    "DATA_SET_IDENTIFIER_I = \"_suboptimal_for_offline_rl\"\n",
    "NUM_STEPS_I = 500\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_suboptimal_determ_initial_3_0_final_3_7\n",
    "DATA_SET_IDENTIFIER_II = \"_expert_for_offline_rl\"\n",
    "NUM_STEPS_II = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_data_sets_offline_rl\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")\n",
    "\n",
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.01))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and analysis of results\n",
    "\n",
    "In this part of the exercise you need to: \n",
    "\n",
    "a) Restore the policy configurations (through TrainedPolicyConfig) for three offline RL policies, namely **BCQ, CQL and BC**, i.e.:\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig( ... )\n",
    "\n",
    "Give a look to the policy parameteres in offline_rl/offline_policies. \n",
    "\n",
    "\n",
    "b) Train the policies on the **expert data**:\n",
    "\n",
    "NUM_EPOCHS =10\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "STEP_PER_EPOCH = len_buffer\n",
    "\n",
    "\n",
    "offline_training( ... )\n",
    "\n",
    "\n",
    "c) Visualize the policies:\n",
    "\n",
    "offpolicy_rendering( ... )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_rl_policies = [PolicyName.bcq_discrete, PolicyName.cql_discrete, PolicyName.imitation_learning]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS =10\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0*len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_obstacles = [ObstacleTypes.obstacle_8x8_top_right, ObstacleTypes.obst_free_8x8]\n",
    "selected_obstacle = widget_list(available_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (3, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(selected_data_set.value, selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, SAVED_POLICY_NAME), map_location=\"cpu\"))\n",
    "\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value)\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "#snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that your policies are trained let's do some analysis:**\n",
    "\n",
    "1 - Train a BCQ and a CQL policies and see if they are able to learn the expert data.\n",
    "\n",
    "2 - As we saw imitation learning is quite good when you have expert data. Train a BC policy and compare with the \n",
    "offline methods. (Answer: All of the reproduce expert data)\n",
    "\n",
    "3 - Now rollout the three policies from o.o.d. data. What do you observe?\n",
    "(Answer: Offpolicy ones are better to come-back to in-distribution ---> Why??)\n",
    "\n",
    "4 - Remove the obstacle and do a rollout of the three policies. What do you observe?\n",
    "(Answer: BC goes totally o.o.d.)\n",
    "\n",
    "5 - Use now the combined dataset that includes a fair amount of noise. What do you notice what happens with the policiey? ---> Mybe BS --> See Levine discussion about noisy data but critical states ......\n",
    "\n",
    "(Answer: BC doesn't do a good job to imitate the expert data as it copy the noise. BCQ and CQL are able to separate noise from expert data --> Not what is happening so far!!!\n",
    "\n",
    "It should happens that BC get confused and BCQ/CQL get an even better trajectory as they collect feedback from the reward...\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise II \n",
    "\n",
    "TODO --> BUG: It seems there was a bug as the env used to collect it uses always same initial_state --> corrected now but check that everything works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we'll explore how BCQ and CQL, address the issue of connecting suboptimal trajectories in order to get new ones with higer rewards (stitching property). We will see how they compare with imitation learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start again with the previous setup. So as we did before we will create again two datasets one from a policy moving suboptimal from (0,0) to (7,0) and the other from another policy moving from (4,0) to (7,7). The scope is to find an agent able to connect trajectories coming from both datasets in order to find the optimal path between (0,0) and (7,7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavioral Policies and Minari DataSet configurations\n",
    "\n",
    "\n",
    "**Create the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_free_8x8\n",
    "INITIAL_STATE_POLICY_I = (0,0)\n",
    "INITIAL_STATE_POLICY_II = (4,0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config_I = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_I,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env_2D_grid_initial_config_II = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_II,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode),\n",
    "                                          env_config=env_2D_grid_initial_config_I)\n",
    "snapshot_env(env)\n",
    "\n",
    "env.set_starting_point(INITIAL_STATE_POLICY_II)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**configure behavioral policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIFIER_COMBINED_DATASETS = \"_stiching_property_I\"\n",
    "\n",
    "# Dataset I\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip\n",
    "DATA_SET_IDENTIFIER_I = \"_move_downwards\"\n",
    "NUM_STEPS_I = 20000\n",
    "\n",
    "# Dataset II\n",
    "#BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "DATA_SET_IDENTIFIER_II = \"_move_deterministic\"\n",
    "NUM_STEPS_II = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_policy_to_render = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy_name=select_policy_to_render.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_collected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = (env_2D_grid_initial_config_I, env_2D_grid_initial_config_II),\n",
    ")\n",
    "buffer_data = load_buffer_minari(config_combined_data.data_set_name)\n",
    "data_size = len(buffer_data)\n",
    "\n",
    "dataset_availables = [config_combined_data.data_set_name] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.03))\n",
    "\n",
    "if \"start_0_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((0,0))\n",
    "    snapshot_env(env)\n",
    "elif \"start_4_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((4,0))\n",
    "    snapshot_env(env)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_rl_policies = [PolicyName.bcq_discrete, PolicyName.cql_discrete, PolicyName.imitation_learning]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offiline - Training\n",
    "\n",
    "NUM_EPOCHS =10\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0*len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "#offline_policy_config.policy_config[\"unlikely_action_threshold\"]=0.6\n",
    "#offline_policy_config.policy_config[\"min_q_weight\"]=15.0\n",
    "#offline_policy_config.policy_config[\"num_quantiles\"]=5\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_obstacles = [ObstacleTypes.obstacle_8x8_top_right, ObstacleTypes.obst_free_8x8]\n",
    "selected_obstacle = widget_list(available_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (3, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    policy_name=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(selected_data_set.value, selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, SAVED_POLICY_NAME), map_location=\"cpu\"))\n",
    "\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value)\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "#snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you observe? Try to increase the number of expert samples and run it again? What happens now?**\n",
    "\n",
    "**As we can see the BCQ algorithm is able to stitch two trajectories in order to create an optimal one.**\n",
    "\n",
    "**Try to do the same with CQL and compare results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a BCQ policy using the TORCS data from our previous DAGGER exercise.\n",
    "\n",
    "Imitation learning failed to achieve autonomous driving, as it imitate the behavior policy that caused the car to crash after a few meters. Let's test if the BCQ algorithm can successfully navigate the entire race track.\n",
    "\n",
    "Note that we won't be able to get results as good as DAGGER, as it not only introduced expert knowledge but also new states, which makes the problem much easier to solve. However access to human experts is quite rare in real problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_datasets_names = [\"torcs-torcs_suboptimal-v0\"]\n",
    "collected_datasets = widget_list(collected_datasets_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 128\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None  # 1626\n",
    "PERCENTAGE_DATA_PER_EPOCH = 1.0\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "OFFLINE_POLICY_NAME = PolicyName.bcq_continuous\n",
    "DATA_SET_NOISY_NAME = collected_datasets.value\n",
    "TRAINED_POLICY_NAME = \"policy_bcq.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(DATA_SET_NOISY_NAME)\n",
    "data_config = MinariDatasetConfig.load_from_file(DATA_SET_NOISY_NAME)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=DATA_SET_NOISY_NAME,\n",
    "    policy_name= OFFLINE_POLICY_NAME,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=PERCENTAGE_DATA_PER_EPOCH * len(buffer_data),\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    policy_name=TRAINED_POLICY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trained_policy_selected = widget_list([TRAINED_POLICY_NAME, \"policy_best_reward.pth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_bcq_policy = restore_trained_offline_policy(offline_policy_config)\n",
    "log_name = os.path.join(DATA_SET_NOISY_NAME, OFFLINE_POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "trained_bcq_policy.load_state_dict(torch.load(str(os.path.join(log_path, trained_policy_selected.value)), map_location=\"cpu\"))\n",
    "trained_bcq_policy\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=\"torcs\",\n",
    "    render_mode=None,\n",
    "    policy_model=trained_bcq_policy,\n",
    "    num_frames=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise IV \n",
    "\n",
    "Robotic hand --> ToDo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline RL proves valuable in various scenarios, especially when:\n",
    "\n",
    "a. Robots require intelligent behavior in complex open-world environments demanding extensive training data due to robust visual perception requirements. (complex environment modeling and extensive data collection)\n",
    "\n",
    "b. Robot grasping tasks, which involve expert data that cannot be accurately simulated, providing an opportunity to assess our BCQ algorithm.\n",
    "\n",
    "c. Robotic navigation tasks, where offline RL aids in crafting effective navigation policies using real-world data.\n",
    "\n",
    "d. Autonomous driving, where ample expert data and an offline approach enhance safety.\n",
    "\n",
    "e. Healthcare applications, where safety is paramount due to the potential serious consequences of inaccurate forecasts.\n",
    "\n",
    "... and many more.\n",
    "\n",
    "However, if you have access to an environment with abundant data, online Reinforcement Learning (RL) can be a powerful choice due to its potential for exploration and real-time feedback. Nevertheless, the landscape of RL is evolving, and a data-centric approach is gaining prominence, exemplified by vast datasets like X-Embodiment. It's becoming evident that robots trained with diverse data across various scenarios tend to outperform those solely focused on specific tasks. Furthermore, leveraging multitask trained agents for transfer learning can be a valuable strategy for addressing your specific task at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
