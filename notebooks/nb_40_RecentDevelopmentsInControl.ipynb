{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Protocol\n",
    "\n",
    "import casadi\n",
    "import do_mpc\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from gymnasium import Env\n",
    "from IPython.display import HTML\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from training_rl.control import (\n",
    "    create_inverted_pendulum_environment,\n",
    "    InvertedPendulumParameters,\n",
    "    animate_full_inverted_pendulum_simulation,\n",
    "    simulate_environment,\n",
    "    show_video\n",
    ")\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "sns.set_theme()\n",
    "plt.rcParams[\"figure.figsize\"] = [9, 5]\n",
    "# This is needed because inside docker the rendering of mujoco environments may not work.\n",
    "render_mode = \"rgb_array\" if os.environ.get(\"DISPLAY\") else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Recent Developments in Control Theory</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recent Developments in Control Theory\n",
    "\n",
    "- So far we have focused on deterministic systems with no noise or disturbances.\n",
    "- In this part of the training, we will focus on stochastic systems and how MPC can be used to handle such systems.\n",
    "- We will also see how MPC can leverage data through learning-based approaches.\n",
    "- Additionally, we will explore how MPC can be combined with Reinforcement Learning to improve systems' safety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stochastic Optimal Control Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The methods discussed in this part deal with the problem of controlling dynamical systems\n",
    "that are subject to system constraints under uncertainty, which can affect numerous parts of the\n",
    "problem formulation. The system dynamics in discrete-time is given by:\n",
    "\n",
    "$$\n",
    "x_{k+1} = f_t(x_k, u_k, k, w_k, \\theta_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_k \\in \\mathbf{R}^n$ is the system state at time $k$.\n",
    "- $u_k \\in \\mathbf{R}^n$ is the applied input at time $k$.\n",
    "- $w_k$ describes a sequence of random variables corresponding to disturbances or process noise in the system, which are often assumed to be independent and identically distributed (i.i.d.).\n",
    "- $\\theta_t \\sim \\mathcal{Q}^{\\theta_t}$ is a random variable describing the parametric uncertainty of the system, which is therefore constant over time.\n",
    "- The subscript $t$ is used to emphasize that these quantities represent the true system dynamics or true optimal control problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The true problem therefore relates to the development of an optimal controller for a distribution of systems given by $\\mathcal{Q}^{\\theta_t}$ under random disturbances $w_k$.\n",
    "\n",
    "The optimality of the controller is defined with respect to a cost or objective function. In the\n",
    "presence of random model uncertainties, the cost is often defined as the expectation of a sum of\n",
    "potentially time-varying stage costs of the states and inputs over a possibly infinite horizon $T$:\n",
    "\n",
    "$$\n",
    "J_t = E\\left(\\sum \\limits_{k=0}^{T} g_t(x_k, u_k, k)\\right),\n",
    "$$\n",
    "\n",
    "where the expected value is taken with respect to all random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stochastic Predictive Control\n",
    "\n",
    "The constrained stochastic optimal control problem can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "J_t^* = \\displaystyle\\min_{\\pi_{k}} & \n",
    "E\\left[\\sum\\limits_{k=0}^{T} g_t(x_k, u_k, k)\\right]\n",
    "\\\\\n",
    "\\text{subject to} & x_{k + 1}= f_t(x_k, u_k, k, w_k, \\theta_t)\n",
    "\\\\\n",
    "& u_k = \\pi_k(x_0, \\dots, x_k)\\\\\n",
    "& \\bar{W} = [w_0, \\dots, w_{N - 1}] \\sim \\mathcal{Q}^{\\bar{W}}, \\theta_t \\sim \\mathcal{Q}^{\\theta_t}\\\\\n",
    "& P[\\bar{X}] = [x_0, \\dots, x_{N}] \\in \\bar{X}_j ) \\ge p_j, \\forall j = 1, \\dots, n_{cx}\\\\\n",
    "& P[\\bar{U}] = [u_0, \\dots, u_{N - 1}] \\in \\bar{U}_j ) \\ge p_j, \\forall j = 1, \\dots , n_{cu}\\\\\n",
    "\\end{array},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Optimizing over a sequence of control laws $\\{\\pi_k\\}$, which can make use of all information in the\n",
    "form of state measurements $x_k$ up to time step $k$. Problems of this form are in\n",
    "general very hard to solve, and direct efforts typically rely on some form of discretization in space\n",
    "and approximate dynamic programming or reinforcement learning.\n",
    "\n",
    "A notable exception, similar to what we have seen previously, is linear systems under additive noise and quadratic stage costs in the unconstrained setting, for which an exact solution, such as the standard linear quadratic regulator (LQR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MPC approximates the previous problem by repeatedly solving a simplified version of the\n",
    "problem initialized at the currently measured state $x_k$ over a shorter horizon $N$\n",
    "in a receding- horizon fashion.\n",
    "\n",
    "We introduce the prediction model:\n",
    "\n",
    "$$\n",
    "x_{i+1|k} = f(x_{i|k}, u_{i|k}, i + k, w_{i|k}, \\theta),\n",
    "$$\n",
    "\n",
    "where $f$ is the prediction dynamics. It typically aims at approximating the true dynamics but often differs, e.g.,\n",
    "for computational reasons or because a succinct description of the true dynamics is unavailable.\n",
    "\n",
    "We use the subscript $i|k$ to emphasize predictive quantities, where, e.g., $x_{i|k}$ is the i-step-ahead prediction of the state, initialized at $x_{0|k} = x_k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most widespread MPC formulations of are nominal MPC schemes, which do not consider any uncertainties in the prediction model but instead rely exclusively on the compensation of uncertainties via feedback and by re-solving the problem at the next sampling instance.\n",
    "\n",
    "In nominal MPC, the optimization can be performed over control sequences $U = [u_{0|k}, \\dots , u_{N - 1|k}]$ rather than policies, resulting in the constrained optimal control problem\n",
    "\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "J^∗ &= \\displaystyle\\min_{U} g_f(x_{N|k}, u_{N|k}, k + N) + \\sum\\limits_{i=0}^{N-1} g(x_{i|k}, u_{i|k}, i + k)\\\\\n",
    "\\text{subject to} & x_{i+1|k} = f(x_{i|k}, u_{i|k}, i + k)\\\\\n",
    "& U = [u_{0|k}, \\dots, u_{N|k}] \\in U_j, \\forall j = 1, \\dots, n_{cu} \\\\\n",
    "& X = [x_{0|k}, \\dots, x_{N|k}] \\in X_j \\forall j = 1, \\dots, n_{cx} \\\\\n",
    "& x_{N|k} \\in X_f\\\\\n",
    "& x_{0|k} = x_k\\\\\n",
    "\\end{array}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The control law is then implicitly defined through the optimization problem as:\n",
    "\n",
    "$$\n",
    "π^{\\text{MPC}}(x_k, k) = u_{0|k}^*,\n",
    "$$\n",
    "\n",
    "where $u_{0|k}^*$ is the first element of the computed optimal control sequence $U^∗$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Control Design Challenges\n",
    "\n",
    "- Ensuring recursive feasibility and achieving optimality despite a short prediction horizon.\n",
    "- Satisfying input and state constraints in the presence of uncertainty.\n",
    "- Ensuring computational tractability by properly reformulating constraints and costs and parameterizing control. policies\n",
    "\n",
    "There is no systematic and universal solution to the third challenge, and often the chosen approach is application dependent. Fortunately, the first and second challenges can be addressed by using data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Robust MPC\n",
    "\n",
    "- Robust MPC guarantees constraint satisfaction for all uncertain element realizations.\n",
    "- The model is split into a nominal part and additive uncertainty in a compact set. \n",
    "- The controller is designed to be robust against the uncertainty.\n",
    "- The MPC cost is typically optimized for the nominal system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Multi-Stage MPC\n",
    "\n",
    "The basic idea for the multi-stage approach is to consider various scenarios, where a scenario is defined by one possible realization of all uncertain parameters at every control instant within the horizon. The family of all considered discrete scenarios can be represented as a tree structure, called the scenario tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "    <img src=\"_static/images/40_multi_state_mpc.png\" width=\"60%\"/>\n",
    "    <figcaption>\n",
    "        Scenario tree representation of the uncertainty\n",
    "evolution for multi-stage MPC.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each node in the tree denotes the possible state of the system at every prediction step.\n",
    "- The branches represent the different possible realizations of the uncertainty.\n",
    "- The initial state of the system forms the root node of the tree.\n",
    "- The root node branches into several nodes in the first stage depending on the number of vertex matrix pairs of the parametric uncertainty.\n",
    "- All the nodes in the first stage branch again in the second stage.\n",
    "- The sequence continues until the end of prediction horizon N to form the complete scenario tree.\n",
    "- A path from the root node to the leaf node represents a scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example - Inverted Pendulum\n",
    "\n",
    "In a real system, usually the model parameters cannot be determined exactly, what represents an important source of uncertainty. In this example, we consider that the mass of the pendulum and that of the cart are not known precisely \n",
    "and vary with respect to their nominal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Model, States and Control inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = do_mpc.model.Model(\"continuous\")\n",
    "\n",
    "pos = model.set_variable(var_type=\"_x\", var_name=\"position\")\n",
    "theta = model.set_variable(var_type=\"_x\", var_name=\"theta\")\n",
    "dpos = model.set_variable(var_type=\"_x\", var_name=\"velocity\")\n",
    "dtheta = model.set_variable(var_type=\"_x\", var_name=\"dtheta\")\n",
    "u = model.set_variable(var_type=\"_u\", var_name=\"force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Certain parameters\n",
    "ip_parameters = InvertedPendulumParameters()\n",
    "k = 1 / 3\n",
    "l = ip_parameters.l\n",
    "gamma = ip_parameters.gamma\n",
    "g = ip_parameters.g\n",
    "mu_p = ip_parameters.mu_p\n",
    "mu_c = ip_parameters.mu_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Uncertain parameters\n",
    "m = model.set_variable(\"_p\", \"m\")\n",
    "M = model.set_variable(\"_p\", \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "numerator = (\n",
    "    (M + m) * g * casadi.sin(theta)\n",
    "    - casadi.cos(theta) * (gamma * u + m * l * dtheta**2 * casadi.sin(theta) - mu_c * dpos)\n",
    "    - (M + m) / (m * l) * mu_p * dtheta\n",
    ")\n",
    "denominator = (1 + k) * (M + m) * l - m * l * casadi.cos(theta) ** 2\n",
    "ddtheta = numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "numerator = (\n",
    "    m * g * casadi.cos(theta) * casadi.sin(theta)\n",
    "    - (1 + k) * (gamma * u + m * l * dtheta**2 * casadi.sin(theta) - mu_c * dpos)\n",
    "    - mu_p * dtheta * casadi.cos(theta) / l\n",
    ")\n",
    "denominator = m * casadi.cos(theta) ** 2 - (1 + k) * (M + m)\n",
    "ddpos = numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.set_rhs(\"position\", dpos)\n",
    "model.set_rhs(\"theta\", dtheta)\n",
    "model.set_rhs(\"velocity\", ddpos)\n",
    "model.set_rhs(\"dtheta\", ddtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mpc = do_mpc.controller.MPC(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = create_inverted_pendulum_environment()\n",
    "mpc_params = {\n",
    "    \"n_horizon\": 50,\n",
    "    \"n_robust\": 1,\n",
    "    \"t_step\": env.dt,\n",
    "    \"state_discretization\": \"collocation\",\n",
    "    \"collocation_type\": \"radau\",\n",
    "    \"collocation_deg\": 3,\n",
    "    \"collocation_ni\": 1,\n",
    "    \"store_full_solution\": True,\n",
    "    # Use MA27 linear solver in ipopt for faster calculations:\n",
    "    \"nlpsol_opts\": {\"ipopt.linear_solver\": \"mumps\"},\n",
    "}\n",
    "mpc.set_param(**mpc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "env = create_inverted_pendulum_environment()\n",
    "xss = np.array([0.5, 0, 0, 0])\n",
    "distance_cost = casadi.bilin(np.diag([1, 100, 0, 0]), model.x.cat - xss)\n",
    "terminal_cost = distance_cost\n",
    "stage_cost = distance_cost\n",
    "print(f\"{stage_cost=}\")\n",
    "print(f\"{terminal_cost=}\")\n",
    "mpc.set_objective(mterm=terminal_cost, lterm=stage_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "force_penalty = 0.1\n",
    "mpc.set_rterm(force=force_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# lower and upper bounds of the position\n",
    "x_max = 1\n",
    "mpc.bounds[\"lower\", \"_x\", \"position\"] = -x_max\n",
    "mpc.bounds[\"upper\", \"_x\", \"position\"] = x_max\n",
    "# lower and upper bounds of the input\n",
    "u_max = 3\n",
    "mpc.bounds[\"lower\", \"_u\", \"force\"] = -u_max\n",
    "mpc.bounds[\"upper\", \"_u\", \"force\"] = u_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Parameter Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m_values = ip_parameters.m * np.array([1.0, 1.30, 0.70])\n",
    "M_values = ip_parameters.M * np.array([1.0, 1.30, 0.70])\n",
    "mpc.set_uncertainty_values(m=m_values, M=M_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mpc.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class MPCController:\n",
    "    def __init__(self, mpc: do_mpc.controller.MPC) -> None:\n",
    "        self.mpc = mpc\n",
    "        self.mpc.reset_history()\n",
    "        self.mpc.x0 = np.zeros(4)\n",
    "        self.mpc.set_initial_guess()\n",
    "\n",
    "    def act(self, observation: NDArray) -> NDArray:\n",
    "        return mpc.make_step(observation.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "max_steps = 100\n",
    "env = create_inverted_pendulum_environment(\n",
    "    render_mode=render_mode, max_steps=max_steps, cutoff_angle=np.inf, initial_angle=0.99*np.pi\n",
    ")\n",
    "controller = MPCController(mpc)\n",
    "results = simulate_environment(env, max_steps=max_steps, controller=controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "show_video(results.frames, fps=1 / env.dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "animate_full_inverted_pendulum_simulation(mpc.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning-Based Model Predictive Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Learning-based MPC addresses the automated and data-driven generation or adaptation of elements of the MPC formulation to improve control performance.\n",
    "- The learning setup can be diverse:\n",
    "  - Offline learning involves adapting the controller between trials or episodes while collecting data.\n",
    "  - Online learning adjusts the controller during closed-loop operation (e.g. repetitive tasks) or using data from one task execution.\n",
    "- Much research has focused on automatically improving model quality, as this clearly affects MPC performance.\n",
    "- Some efforts address the MPC problem formulation directly.\n",
    "- Others use MPC concepts to satisfy constraints during learning-based control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning the system dynamics\n",
    "\n",
    "- MPC relies on accurate system models, so one approach is learning to adjust the model either during operation or between different operational instances.\n",
    "- Traditionally models are derived offline before control using first principles and identification.\n",
    "- Robust approaches often consider model uncertainty as well as process noise to lie in compact sets $\\theta_t \\in T, w_k \\in W$.\n",
    "- Stochastic MPC make use of distributional information on the uncertainties.\n",
    "- Learning-based MPC constructs and updates models and uncertainties from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Many learning-based MPC techniques make use of an explicit distinction between a nominal \n",
    "  system model $f_n$ and an additive learned term $f_l$ accommodating uncertainty:\n",
    "\n",
    "  $$\n",
    "  f(x, u, k, w, \\theta) = f_n(x, u, k) + f_l(x, u, k, w, \\theta)\n",
    "  $$\n",
    "\n",
    "- Successful learning methods are often based on probabilistic formulations, leading to a\n",
    "  nonlinear stochastic prediction model.\n",
    "\n",
    "- Leveraging the full potential of such models within MPC, however, is very challenging and remains an active  \n",
    "  research field. Many model-learning MPC schemes have therefore evolved from the extensively studied field of robust MPC, offering a large body of available theoretical results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Robust MPC allows some adaptation but only considers fixed, known model uncertainty.\n",
    "- Learning-based MPC aims to estimate model uncertainty directly from data.\n",
    "- This allows adjusting the uncertainty over time to reduce conservatism. \n",
    "- Many techniques use set-membership identification with bounded noise.\n",
    "- Parametric approaches find the set of parameter values $\\theta$ consistent with observations.\n",
    "- Non-parametric approaches form estimates of $f$ directly from data points.\n",
    "- The goal is to learn the model uncertainty from measurements, not assume it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Stochastic non-parametric approaches\n",
    "\n",
    "- Gaussian process (GP) regression is commonly used in learning-based control due to its flexible non-parametric stochastic approach.\n",
    "- GP regression assumes dynamics with additive Gaussian noise and models function values as jointly Gaussian based on a kernel function.\n",
    "- Using recorded state/input data, GP regression provides posterior mean and variance functions as the estimator.\n",
    "- The variance indicates residual model uncertainty from insufficient data.\n",
    "- Prediction involves propagating the stochastic state distributions, often approximated as Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The learned part of the model can address discrepancies with a nominal model.\n",
    "- Uncertainty estimates enable heuristic constraint tightening for safety.\n",
    "- Computation time is a challenge, addressed via data selection, approximations, and simplifying variance.\n",
    "- GP-based MPC has been applied successfully to various robotic and process control tasks.\n",
    "- It is a highly data-efficient model-based reinforcement learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"_static/images/40_gaussian_process.svg\" width=\"80%\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<figure style=\"float: left; width: 70%; margin-right: 10%;\">\n",
    "    <img src=\"_static/images/40_learning_based_mpc_gp.png\" width=\"100%\"/>\n",
    "</figure>\n",
    "\n",
    "Gaussian process–based MPC for autonomous racing. (b,c) The resulting trajectories of a similar approach applied to miniature radio-controlled cars, with the initial nominal controller shown in panel b and the improved trajectories after learning shown in panel c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe width=\"800\" height=\"480\" src=\"https://www.youtube.com/embed/-cdXw1MyTUA?si=S3DXY90f8QEPFddI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning the controller design\n",
    "\n",
    "- Beyond the model, the MPC cost function and constraints strongly influence closed-loop performance.\n",
    "- Learning approaches can design the MPC problem to achieve desired controller behavior.\n",
    "- A parameterized MPC formulation is considered with cost $g(x,u,\\theta_l)$ and constraints $\\mathcal{X}(\\theta_\\mathcal{X}), \\mathcal{U}(\\theta_\\mathcal{U})$:\n",
    "\n",
    "  $$\n",
    "  \\begin{array}\\\\\n",
    "  U^∗ &= \\displaystyle\\arg\\min_{U} \\sum\\limits_{i=0}^{T} g(x_i,u_i,\\theta_l)\\\\\n",
    "  \\text{subject to} & x_{i+1} = f(x_i, u_i, \\theta_f)\\\\\n",
    "  & U = [u_0 , \\dots, u_N ] \\in \\mathcal{U}(\\theta_\\mathcal{U})\\\\\n",
    "  & X = [x_0 , \\dots, x_N ] \\in \\mathcal{X}(\\theta_\\mathcal{X})\\\\\n",
    "  & x_0 = x_k\\\\\n",
    "  \\end{array}\n",
    "  $$\n",
    "\n",
    "- Both the cost and constraints are parameterized and learned based on observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance-Driven Controller Learning\n",
    "\n",
    "- MPC can be a rough approximation of the true stochastic optimal control problem. \n",
    "- Performance-driven learning finds MPC parameters to optimize closed-loop performance.\n",
    "- It uses Bayesian optimization to optimize the true closed-loop cost $J(\\theta)$ as a function of parameters $\\theta$.\n",
    "- $J(\\theta)$ is modeled as a Gaussian process to enable optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The finite prediction horizon is a limitation, mitigated via terminal cost and constraint.\n",
    "- Learning constructs/improves these from data to approximate infinite-horizon cost. \n",
    "- For nominal systems, trajectories ending in $X_f$ yield an enlarged invariant set.\n",
    "- Data is used to iteratively grow $X_f$ and improve the terminal cost and constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning from Demonstration with Inverse Optimal Control\n",
    "\n",
    "- Designing a cost function or defining an objective mathematically in order to achieve a desired\n",
    "  complex behavior can be tedious and require extensive parameter tuning and development time\n",
    "- Inverse optimal control addresses this problem by inferring the cost and constraints from demonstrations.\n",
    "- The hypothesis underlying inverse optimal control is that the observed demonstrations\n",
    "  are the solution of a corresponding optimal control problem. \n",
    "- Most of the research in inverse optimal control has assumed knowledge of potential constraints\n",
    "  and focused on the cost function instead, following three main steps:\n",
    "\n",
    "  1. Define the optimal control problem with a parametric cost function $l(x, u, \\theta_l)$,\n",
    "     e.g., quadratic costs with unknown weights $l(x, u, \\theta_l) = x^T Q(\\theta_l)x + u^T R(\\theta_l)u$.\n",
    "  2. Derive optimality conditions for the parametric optimal control problem.\n",
    "  3. Solve optimality conditions for the parameters $\\theta_l$ given the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A related field of research is inverse reinforcement learning.\n",
    "- This technique similarly addresses the problem of identifying a cost or reward function,\n",
    "  typically in the context of probabilistic decision-making, which is often expressed\n",
    "  in terms of Markov decision processes. \n",
    "- The unknown parameters are obtained by maximizing likelihood.\n",
    "- These frameworks are typically for discrete state and action spaces.\n",
    "- They do not explicitly consider system constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"_static/images/40_inverse_optimal_control.png\" width=\"80%\"/>\n",
    "    Concept of learning with inverse optimal control, where the cost function plays the\n",
    "central role of encoding the demonstrated behavior.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Safe Learning in Robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Robot learning aims to enable autonomous operation in complex, uncertain environments.\n",
    "- Challenges include partial knowledge of dynamics, sensors, and other agents. \n",
    "- Safety guarantees are crucial but difficult with partial knowledge.\n",
    "- Control theory uses models to provide guarantees. \n",
    "- Reinforcement learning is data-driven for adaptability but lacks guarantees.\n",
    "- Combining model- and data-driven approaches leverages their complementary strengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Key directions are:\n",
    "  - Robustness against worst-case scenarios.\n",
    "  - Adaptation by learning from observations.\n",
    "  - Leveraging models from domain knowledge and data.\n",
    "\n",
    "- Control provides the basis for safety-critical applications.\n",
    "- Safe RL research has grown rapidly.\n",
    "- Simulation enables RL progress but transferring to real robots remains challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "    <img src=\"_static/images/40_comparison_model_driven_data_driven.svg\" width=\"90%\"/>\n",
    "    <figcaption>\n",
    "        A comparison of model-driven, data-driven, and combined approaches.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The safe learning control problem is formulated as an optimization with 3 main components:\n",
    "\n",
    "  1. System model describing robot dynamics.\n",
    "  2. Cost function defining the control objective. \n",
    "  3. Constraints specifying safety requirements.\n",
    "\n",
    "- The goal is to find a policy fulfilling the task under the safety constraints.\n",
    "- Any of the 3 components could be initially unknown or partially known. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "    <img src=\"_static/images/40_safe_control_block_diagram.svg\" width=\"80%\"/>\n",
    "    <figcaption>\n",
    "        Block diagram representing safe learning control approaches.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Safety Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<figure>\n",
    "    <img src=\"_static/images/40_safety_levels.svg\" width=\"100%\"/>\n",
    "    <figcaption>\n",
    "        Illustration of Safety Levels.\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Safety level III: constraint satisfaction guaranteed.\n",
    "\n",
    "The system satisfies hard constraints:\n",
    "\n",
    "$$\n",
    "c_k^j(x_k, u_k, w_k) \\le 0\n",
    "$$\n",
    "\n",
    "for all times $k \\in \\{0, \\dots , N\\}$ and constraint indexes $j \\in \\{1, \\dots, n_c\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Safety level II: constraint satisfaction with high probability.\n",
    "\n",
    "The system satisfies probabilistic constraints:\n",
    "\n",
    "$$\n",
    "P\\left[c_k^j(x_k, u_k, w_k ) \\le 0 \\right] \\ge p^j,\n",
    "$$\n",
    "\n",
    "where $P[\\cdot]$ denotes the probability and $p^j \\in (0, 1)$ defines the likelihood of the jth constraint\n",
    "being satisfied, for all times $k \\in \\{0, \\dots , N\\}$ and constraint indexes $j \\in \\{1, \\dots, n_c\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Safety level I: constraint satisfaction encouraged\n",
    "\n",
    "The system encourages constraint satisfaction. This can be achieved in different ways:\n",
    "\n",
    "- One way is to add a penalty term to the objective function that discourages\n",
    "  the violation of constraints with a high cost. A non-negative $\\epsilon_j$ is added\n",
    "  to the right-hand side of the inequality in Safety level III, for all times $k \\in \\{0, \\dots , N\\}$\n",
    "  and constraint indexes $j \\in \\{1, \\dots, n_c\\}$:\n",
    "  \n",
    "  $$\n",
    "  c_k^j(x_k, u_k, w_k) \\le \\epsilon_j,\n",
    "  $$\n",
    "\n",
    "  and an appropriate penalty term l\u0003 (\u0002) ≥ 0, with l\u0003 (\u0002) = 0 ⇐⇒ \u0002 = 0, is added to the objective\n",
    "  function. The vector \u0002 includes all elements ϵj and is an additional variable of the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Another way is to provide guarantees on the expected value of the constraint but only at a trajectory level:\n",
    "\n",
    "  $$\n",
    "  J_{c^j} = E\\left[ \\sum\\limits_{k=0}^{N-1} c_k^j(x_k, u_k, w_k) \\right] \\le d_j,\n",
    "  $$\n",
    "\n",
    "  where $J_{c^j}$ represents the expected total constraint cost, and $d_j$ defines the constraint threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  Safe Learning Control Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning uncertain dynamics to safely improve performance\n",
    "\n",
    "These works rely on an apriori model of the robot dynamics. The robot's performance is improved by learning the uncertain dynamics from data. Safety is typically guaranteed based on standard control-theoretic\n",
    "frameworks, achieving safety level II or III."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encouraging safety and robustness in RL\n",
    "\n",
    "These works encompass approaches that usually do not have knowledge of an apriori robot model or the safety constraints. Rather than providing hard safety guarantees, these approaches encourage safe robot operation (safety\n",
    "level I), for example, by penalizing dangerous actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Certifying learning-based control under dynamics uncertainty\n",
    "\n",
    "These works aim to provide safety certificates for learning-based controllers that do not inherently consider safety\n",
    "constraints. These approaches modify the learning controller output by constraining the control policy, leveraging a known safe backup controller, or modifying the controller output directly to achieve stability and/or constraint satisfaction. They typically achieve safety level II or III."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Predictive Safety Filter\n",
    "\n",
    "- General learning-based control, particularly Reinforcement Learning,\n",
    "  has shown great success in solving complex and high-dimensional control tasks.\n",
    "- However most techniques cannot ensure that safety constraints\n",
    "  under physical limitations are met, particularly during learning iterations.\n",
    "- To address this limitation, safety frameworks emerged from control theory.\n",
    "- MPC techniques can be used for such safety filters to turn a safety-critical dynamical system\n",
    "  into an inherently safe system to which any learning-based controller\n",
    "  without safety certificates can be applied out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<figure style=\"float: left; width: 70%;\">\n",
    "    <img src=\"_static/images/40_safety_filter.svg\" width=\"100%\"/>\n",
    "</figure>\n",
    "<div style=\"float: right; width: 25%;\">\n",
    "<br><br><br>Based on the current state $x$, a learning-based controller provides an input\n",
    "$u_L = \\pi_L(x) \\in \\mathbb{R}^m$, which is processed by the safety filter $u = \\pi_S(x, u_S)$ and applied to the real system.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The idea is to address the solution to the stochastic optimal control problem\n",
    "  through learning-based control methods.\n",
    "- The proposed learning-based control input $u_L(k)$ at time $k$ is\n",
    "  then verified in terms of safety by computing a safe backup trajectory from the one-step predicted\n",
    "  state $x_{1|k}$ to a safe terminal set $X_f$ or by modifying $u_L(k)$ as little as possible\n",
    "  while still providing a safe backup trajectory.\n",
    "- The optimization problem necessary for validating safety of the input\n",
    "  is computationally cheaper than a direct optimization of the task\n",
    "  and can often be carried out over a reasonably short horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model predictive safety filter $\\pi_S$ is realized through an MPC-like optimization problem of the form:\n",
    "\n",
    "$$\n",
    "\\begin{array}\\\\\n",
    "\\displaystyle\\min_{U} & || u_{0|k} - u_L(k)||\\\\\n",
    "\\text{subject to} & x_{i+1|k} = f(x_{i|k}, u_{i|k}, i + k)\\\\\n",
    "& U = [u_{0|k}, \\dots, u_{N|k}] \\in U_j, \\forall j = 1, \\dots, n_{cu}\\\\\n",
    "& X = [x_{0|k}, \\dots, x_{N|k}] \\in X_j, \\forall j = 1, \\dots, n_{cx}\\\\\n",
    "& x_{N|k} \\in X_f \\\\\n",
    "& x_{0|k} = x_k\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "<figure style=\"float: left; width: 70%;\">\n",
    "    <img src=\"_static/images/40_safe_learning_approaches.svg\" width=\"100%\"/>\n",
    "</figure>\n",
    "<div style=\"float: left; width: 20%;\">\n",
    "<br><br><br>Summary of safe learning control approaches.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- [<b id=\"rosolia_datadriven_2018\">[Rosolia, U., Zhang, X. and Borrelli, F., 2018]</b>](#rosolia_datadriven_2018-back) Rosolia, Ugo, Xiaojing Zhang, and Francesco Borrelli. [Data-driven predictive control for autonomous systems.](https://www.annualreviews.org/doi/full/10.1146/annurev-control-060117-105215) Annual Review of Control, Robotics, and Autonomous Systems 1 (2018): 259-286.\n",
    "\n",
    "- [<b id=\"hewing_learningbased_2020\">[Hewing, Lukas, et al. 2020]</b>](#hewing_learningbased_2020-back) Hewing, Lukas, Kim P. Wabersich, Marcel Menner, and Melanie N. Zeilinger. [Learning-based model predictive control: Toward safe learning in control.](https://www.annualreviews.org/doi/full/10.1146/annurev-control-090419-075625) Annual Review of Control, Robotics, and Autonomous Systems 3 (2020): 269-296.\n",
    "\n",
    "- [<b id=\"brunke_safe_2022\">[Brunke, Lukas, et al. 2022]</b>](#brunke_safe_2022-back) Brunke, Lukas, Melissa Greeff, Adam W. Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P. Schoellig. [Safe learning in robotics: From learning-based control to safe reinforcement learning.](https://www.annualreviews.org/doi/abs/10.1146/annurev-control-042920-020211) Annual Review of Control, Robotics, and Autonomous Systems 5 (2022): 411-444."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "315.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
