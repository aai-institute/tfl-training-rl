{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "source": [
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "from celluloid import Camera\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from functools import partial"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-input-nbconv"
    ]
   },
   "source": [
    "%presentation_style"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input",
     "remove-output",
     "remove-input-nbconv",
     "remove-output-nbconv"
    ]
   },
   "source": [
    "%%capture\n",
    "\n",
    "%set_random_seed 12"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "hide_input": true,
    "init_cell": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove-input-nbconv",
     "remove-cell"
    ]
   },
   "source": [
    "%load_latex_macros"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Intro to Reinforcement Learning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Concepts of Reinforcement Learning\n",
    "\n",
    "## Part 1: Introduction\n",
    "\n",
    "In the last part, we saw interaction with environments from a control perspective. \n",
    "For the next two days, we will view the task of controlling an environment from \n",
    "a _learning perspective_.\n",
    "\n",
    "Before diving into the notation and details of RL, let us shortly contrast the overall approach\n",
    "to supervised (or self-supervised) learning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bird's Eye's view of Supervised Learning\n",
    "\n",
    "A supervised learning project can roughly be viewed as the following points:\n",
    "\n",
    "1. Train a model on a dataset to fulfill some objective\n",
    "2. Evaluate the model on a separate test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But is this really the full picture in a realistic project? While for more academic situations it might be taken as given,\n",
    "in a real scenario somebody has to compile the datasets. In fact, this is often the hardest and most important part\n",
    "of the whole project! Let us extend the list a bit:\n",
    "\n",
    "0. Obtain train and test datasets\n",
    "1. Train a model on a dataset\n",
    "2. Evaluate the model on a separate test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we are getting closer to a realistic project. But there is still something missing - is it enough\n",
    "to just test the model on a test set? What if the model is not good enough in some cases?\n",
    "\n",
    "While sometimes the model details can be changed to improve the performance, often \n",
    "changes in the dataset are a much more impactful measure.\n",
    "\n",
    "For example, a product owner might notice that a face recognition system is not working well for \n",
    "people with glasses. After a quick glance, one discovers that the training dataset contains very\n",
    "few pictures of people with glasses. So the appropriate reaction is to collect more such pictures and retrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The extended list now looks like this:\n",
    "\n",
    "0. Obtain train and test datasets\n",
    "1. Train a model on a dataset to fulfill some objective\n",
    "2. Evaluate the model on a separate test set\n",
    "3. Check on which samples the model underperforms\n",
    "4. Analyze the results of step 3, and go back to step 0 if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How could you go about extending the dataset with samples on which the current model version\n",
    "underperforms? Let's slightly extend the previous example and imagine that the face-recognition\n",
    "system can also output the labels \"not a face\" and \"has glasses\".\n",
    "\n",
    "One could now crawl some large database of images, and use the current version of the model\n",
    "for an initial filtering of it, to mainly obtain faces. Now a human would go over the filtered\n",
    "images, and if necessary, reassign labels. Especially the images where the model has made a mistake\n",
    "are of particular value for the next training round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This means, that the current version of model itself is used in the acqusition of new data, \n",
    "in step 4. In addition, there is an external system (a human) which is responsible for\n",
    "switching labels if necessary.\n",
    "\n",
    "Thus, in the next training run the model is receiving something akin to __negative rewards__\n",
    "from the external system for getting the predictions wrong - new samples where it failed to have\n",
    "a small value of the training loss are added to the training data.\n",
    "\n",
    "It is also in a way __positively rewarded__ for samples where it performed well, since comparatively few\n",
    "new samples of this type will be added to the dataset, and the overall loss due to them will not \n",
    "increase by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are many ways to compare reinforcement learning to other learning paradigms, but\n",
    "in my opinion, the main difference is the following:\n",
    "\n",
    "__In RL, data acquisition is automated and part of the algorithm!__\n",
    "\n",
    "This means that the manually performed steps 4 and 0 in supervised learning (SL)\n",
    "are being handled by the RL algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"_static/images/rl-abstract.png\" alt=\"Decision Process\" style=\"width:100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other important differences are:\n",
    "\n",
    "- RL is generally concerned with sequential decision-making\n",
    "- Contrary to SL, RL can make use of **suboptimal information**\n",
    "- The training signal (typically sum of returns) in RL may be significantly delayed after an action\n",
    "- Evaluating an RL policy is generally harder than evaluating an SL model\n",
    "- Transfer learning in RL is often difficult or impossible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relativizing the Differences\n",
    "\n",
    "Of course, also SL is widely applied for modeling sequences. Suboptimal information\n",
    "in SL can be taken into account in some situations - e.g., when the goal is not to predict\n",
    "a perfect label/token from the input, but to correctly model a probability distributions\n",
    "of outputs (which then contains \"suboptimal\" ones). We will go back to that point later\n",
    "when we talk about the *Decision Transformer* algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The lines get even more blurry when comparing SL to offline RL. In offline RL, there\n",
    "is no interaction with the environment, thus the crucial part data acquisition \n",
    "(called exploration in RL contexts) is missing.\n",
    "\n",
    "While the other differences remain, offline RL *is* a certain kind of supervised \n",
    "learning. Thus, it is no wonder that many standard SL techniques prove useful there.\n",
    "\n",
    "In recent developments, large foundational models are increasingly being used for tasks\n",
    "that were previously considered to be only solvable through RL, even tackling the\n",
    "difficulties of transfer learning across different environments.\n",
    "\n",
    "These include the [Palm-E](https://palm-e.github.io/) and [Gato](https://www.deepmind.com/publications/a-generalist-agent) \n",
    "models, as well as the new and open dataset and models from \n",
    "the [Open-X](https://robotics-transformer-x.github.io/) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning Notation\n",
    "\n",
    "We will introduce standard RL Notation that is used in many publications and books, with a slight\n",
    "tilt towards software implementations. We will also focus on RL with function approximation,\n",
    "meaning that value functions and policies contain learnable parameters, and are typically\n",
    "represented by neural networks. This is contrary to tabular RL, where one has finite sets of states and actions,\n",
    "and the value functions and policies can be represented by tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Processes\n",
    "\n",
    "A Markov Decision Process (MDP) is a common starting point of any RL-related work. \n",
    "It is defined by a tuple $(S, A, P, R)$, where:\n",
    "\n",
    "- $S$ is the set of states in the environment.\n",
    "- $A$ is the set of actions available to the agent.\n",
    "- $P(s' | s, a)$ is the transition probability, representing the probability of transitioning from state $s$ to state $s'$ after taking action $a$.\n",
    "- $r(s, a)$ is the reward function, which gives the immediate reward after transitioning from state $s$ to state $s'$ by taking action $a$.\n",
    "\n",
    "The \"Markov\" property means that transition probabilities and rewards only depend on the current state and action, \n",
    "and not on the history of previous states and actions.\n",
    "\n",
    "Note that \"almost anything\" can be made Markovian by redefining what \"state\" means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy\n",
    "\n",
    "A policy $\\pi$ is a mapping from states to probabilities of selecting each possible action. \n",
    "It defines the agent's behavior in the environment. \n",
    "Formally, for each state $s$, $\\pi(a|s)$ represents the probability of taking action $a$ in state $s$.\n",
    "The final goal of RL is to find an optimal policy $\\pi^*$ that maximizes the expected return:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi^* = \\arg \\max_{\\pi} \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{H} \\gamma^t r(s_t, a_t) \\right] =: \\arg \\max_{\\pi} J(\\pi)\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- $H$ is the horizon, i.e., the (maximal) number of steps in a trajectory.\n",
    "- $\\gamma$ is the discount factor that balances immediate rewards against future rewards.\n",
    "- $\\mathbb{E}_{\\pi}$ is the expectation over trajectories sampled from the policy $\\pi$.\n",
    "- $J(\\pi)$ is the optimization objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parameterized policies (e.g. in deep RL), the policy is typically represented by a trainable function with parameters $\\theta$.\n",
    "The goal is then to find the optimal parameters $\\theta^*$ that maximize the expected return:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^* = \\arg \\max_{\\theta} \\mathbb{E}_{\\pi_{\\theta}} \\left[ \\sum_{t=0}^{H} \\gamma^t r(s_t, a_t) \\right] =: \\arg \\max_{\\pi} J(\\theta)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many RL algorithms are based on the idea of optimizing the objective $J(\\theta)$ by using gradient-based methods.\n",
    "These algorithms are often called policy-based, policy gradient methods, or policy-search. For them, it is of central importance\n",
    "to be able to approximate the gradient of $J(\\theta)$ with respect to $\\theta$. We will go into more detail about this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that taking the gradient of the above objective is not trivial! One of the central results of RL is the _Policy Gradient Theorem_ that expresses the gradient in a tractable form. We will see it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do you think would be a suitable distribution $\\pi_{\\theta}$ to use in continuous action spaces? What about discrete action spaces?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Observability\n",
    "\n",
    "In many real-world problems, the agent does not have access to the full state of the environment.\n",
    "Think of observing the screen of a video game, or navigating a car by observing camera feeds.\n",
    "Instead, it receives observations $o$ from the environment, which are typically noisy and incomplete. \n",
    "Mathematically, observations are generated by a function $O(s)$, which maps states to observations. \n",
    "\n",
    "Such situations are called Partially Observable Markov Decision Processes (POMDPs). They are the\n",
    "standard for most real-world applications of RL (although, somewhat unfortunately, not for \n",
    "the majority of academic research)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments in Software\n",
    "\n",
    "The starting point for most online RL Projects (with the possible exception of RL for large language models (LLMs))\n",
    "is an `environment`. The standard interface for environments is the `gymnasium` API. You already saw examples\n",
    "yesterday at the control workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What is the state of a gymnasium environment? What does it mean for this environment to be fully observable? Think for example about an environment that provides camera-feeds of a car driving in the city. What would be an effectively fully observed environment for driving a car?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%capture\n",
    "# playing around with environments\n",
    "cartpole_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "breakout_env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "halfcheetah_env = gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cartpole_env.observation_space"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "breakout_env.observation_space"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** which environment is fully observable, which one partially observable? How could you\n",
    "make the partially observable one fully observable?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important quantities are:\n",
    "\n",
    "- $\\tau$ is commonly used to denote trajectories, i.e., the sequence of states, actions, and rewards that the agent experiences.\n",
    "- $H$ is the horizon, i.e., the (maximal) number of steps in a trajectory.\n",
    "- $R(t)$ is the discounted return from step-t on, i.e. $R(t) = \\sum_{i=t}^{H} \\gamma^{i-t} r(s_i, a_i)$.\n",
    "  Here, $\\gamma$ is the discount factor, which balances immediate rewards against future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the horizons of the environments above? Hint - use the environment's `__repr__` (i.e. just print it).\n",
    "    Feel free to look at the source code of `gymnasium`.\n",
    "    \n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cartpole_env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - Basic Environment Interaction\n",
    "\n",
    "Write an agent that samples random actions from the environment. Then\n",
    "visualize the trajectories by plotting the frames, rewards, and returns."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "@dataclass\n",
    "class TrajEntry:\n",
    "    step: int\n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    reward: float\n",
    "    next_obs: np.ndarray\n",
    "    \n",
    "    frame: np.ndarray | None = None\n",
    "\n",
    "\n",
    "TTrajectory = list[TrajEntry]\n",
    "\n",
    "def get_trajectory_animation(\n",
    "    traj: TTrajectory,\n",
    "    entry_plotter: Callable[[TrajEntry], None],\n",
    "    title_extractor: Callable[[TrajEntry], str] | str = \"Trajectory\",\n",
    "    dpi=150,\n",
    "    figsize=(3, 3),\n",
    "    display_frame_count=True,\n",
    "):\n",
    "    fig = plt.figure(dpi=dpi, figsize=figsize)\n",
    "    camera = Camera(fig)\n",
    "    for id_frame, entry in enumerate(traj):\n",
    "        title = title_extractor if isinstance(title_extractor, str) else title_extractor(entry)\n",
    "        entry_plotter(entry)\n",
    "        ax = plt.gca()\n",
    "        ax.text(-0.05, 1.05, title, transform=ax.transAxes)\n",
    "        if display_frame_count:\n",
    "            ax.text(\n",
    "                .75,\n",
    "                1.05,\n",
    "                f\"frame: {id_frame}\",\n",
    "                fontsize=8,\n",
    "                bbox=dict(facecolor=\"gray\", fill=True, linewidth=1, boxstyle=\"round\"),\n",
    "                transform=ax.transAxes\n",
    "            )\n",
    "        camera.snap()\n",
    "    animation = camera.animate().to_html5_video()\n",
    "    plt.close()\n",
    "    display(HTML(animation))\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_image_with_text_boxes(image: np.ndarray, texts: list[str], fontsize=4):\n",
    "    plt.imshow(image)\n",
    "    d_between_boxes = 0.08\n",
    "    for i, text in enumerate(texts):\n",
    "        y_pos = -0.01- i * d_between_boxes\n",
    "        plt.text(\n",
    "            .75,\n",
    "            y_pos,\n",
    "            text,\n",
    "            fontsize=fontsize,\n",
    "            bbox=dict(facecolor=\"white\", fill=True, linewidth=1, boxstyle=\"round\"),\n",
    "            transform=plt.gca().transAxes\n",
    "        )\n",
    "\n",
    "def plot_traj_entry_with_reward_and_return(entry: TrajEntry, returns: list[float]):\n",
    "    cur_return = returns[entry.step]\n",
    "    plot_image_with_text_boxes(entry.frame, [f\"reward: {entry.reward:.2f}\", f\"return: {cur_return:.2f}\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cartpole_env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_returns(rewards: list[float], gamma: float = 1):\n",
    "    \"\"\"\n",
    "    Computes returns to go from a list of rewards that are assumed to come from a single episode\n",
    "    \"\"\"\n",
    "    returns_reverted = [rewards[-1]]\n",
    "    for rew in rewards[-2::-1]:\n",
    "        returns_reverted.append(rew + gamma * returns_reverted[-1])\n",
    "    return list(reversed(returns_reverted))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "traj = []\n",
    "rewards = []\n",
    "cartpole_env.reset()\n",
    "for step_num in range(50):\n",
    "    action = cartpole_env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = cartpole_env.step(action)\n",
    "    rewards.append(reward)\n",
    "    entry = TrajEntry(step=step_num, obs=obs, action=action, reward=reward, next_obs=obs)\n",
    "    entry.frame = cartpole_env.render()\n",
    "    traj.append(entry)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "traj_returns = compute_returns(rewards)\n",
    "entry_plotter = partial(plot_traj_entry_with_reward_and_return, returns=traj_returns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "get_trajectory_animation(traj, entry_plotter)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sceleton of an Online RL algorithm\n",
    "\n",
    "- fill in the blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Before we dive into how a policy that solves a decision process may be found, we shall talk about how\n",
    "policies can be evaluated. The evaluation plays a central building block for many policy improvement algorithms,\n",
    "so hang in there even if for the moment it might seem too dry!\n",
    "\n",
    "\n",
    "\n",
    "Already in the random policy we saw one aspect that makes RL harder than SL - it is difficult, or even impossible\n",
    "to attribute the return to a specific action, since the rewards may be delayed. This is called the **credit assignment problem**.\n",
    "\n",
    "As we will see below, evaluating a policy is also far from trivial. In fact, it is often the hardest part of RL.\n",
    "First, let us introduce some notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function\n",
    "\n",
    "The value function $V_\\pi(s_t)$ (with some abuse of notation) represents the expected return (cumulative future rewards) \n",
    "an agent can obtain starting from a state $s_t$ at time $t$ and then following the policy $\\pi$.\n",
    "\n",
    "\\begin{equation}\n",
    "V_\\pi(s_t) = \\mathbb{E}_{\\pi} \\left[ \\sum_{i=t}^{H} \\gamma^{i-t} r(s_i, a_i) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$ under the policy.\n",
    "- $\\gamma$ is the discount factor that balances immediate rewards against future rewards.\n",
    "\n",
    "We will be interested in the infinite horizon case, i.e., $H = \\infty$. \n",
    "In this case, one needs only one value function $V_{\\pi}(s)$, since the remaining horizon (and return) \n",
    "do not depend on the current time step $t$.\n",
    "\n",
    "Thus, we can drop the index $t$ and rewrite the value function as:\n",
    "\n",
    "\\begin{equation}\n",
    "V_{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ \\sum_{i=0}^{\\infty} \\gamma^i r(s_i, a_i) \\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Do the environments above have an infinite or a finite horizon? What about environments with a terminal state,\n",
    "will the formalism for infinite horizons still work?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL objective can be rewritten in terms of the value function as:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\pi) = \\mathbb{E}_{\\pi} \\ \\mathbb{E}_{s_0 \\sim \\rho_0} \\left[ V_{\\pi}(s_0) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Where $s_0$ is the initial state and $\\rho_0$ is the initial state distribution.\n",
    "\n",
    "**Question:** Which method defines the initial state distribution for `gymnasium` environments?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-function\n",
    "\n",
    "Another useful quantity is the Q-function, \n",
    "denoted as $Q_\\pi(s, a)$. It represents the expected return an agent can obtain from taking action $a$ in state $s$ and then following the policy $\\pi$. \n",
    "It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_\\pi(s, a) = r(s, a) + \\gamma \\ \\mathbb{E}_{s' \\sim P(\\cdot | s, a)} \\mathbb{E}_\\pi \\left[ r(s', a_0) + \\sum_{i=1}^{\\infty} \\gamma^{i} r(s_i, a_i) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Where $a_0 \\sim \\pi(\\cdot | s')$ is the action taken in the next state $s'$ by following the policy $\\pi$.\n",
    "\n",
    "The Q-function is closely related to the value function:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_\\pi(s, a) = r(s, a) + \\gamma \\ \\mathbb{E}_{s' \\sim P(\\cdot | s, a)} \\left[ V_\\pi(s') \\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Function\n",
    "\n",
    "Finally, one more ubiquitous quantity in  RL is the advantage function, denoted as $A_\\pi(s, a)$. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "A_\\pi(s, a) = Q_\\pi(s, a) - V_\\pi(s)\n",
    "\\end{equation}\n",
    "\n",
    "The advantage function represents the advantage of taking action $a$ in state $s$ over just following the policy $\\pi$\n",
    "from that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: property of A for perfect policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Value Functions\n",
    "\n",
    "The equations for $Q$ and $V$ are of telescopic nature, i.e., they can be rewritten recursively:\n",
    "\n",
    "\\begin{equation}\n",
    "V_\\pi(s) = \\mathbb{E}_{a \\sim \\pi(s)} \\left[ r(s, a) \\right]  + \\gamma \\ \\mathbb{E}_{a \\sim \\pi(s), s' \\sim P(\\cdot | s, a)} \\left[ V_\\pi(s') \\right]\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "Q_\\pi(s, a) = r(s, a) + \\gamma \\ \\mathbb{E}_{a' \\sim \\pi(s'), s' \\sim P(\\cdot | s, a)} \\left[ Q_\\pi(s', a') \\right]\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a value function is learned, the above equalities no longer hold exactly. Instead, one can view the right-hand side as an \n",
    "\"improved estimation\" of the value function. It is \"improved\", because it has used a sample from the environment,\n",
    "thereby incorporating \"ground truth\" information.\n",
    "\n",
    "For example, given a learned value function $V_\\theta$ (we are going to worry about how to learn it later), \n",
    "\n",
    "\n",
    "and a sample $(s, a, r(s,a), s')$ from the environment, one can get a less biased estimate of $V_\\theta(s)$, let's call it\n",
    "$V_{\\theta, 1}(s)$ through a single sample estimate of the above expectation value:\n",
    "\n",
    "\\begin{equation}\n",
    "V_{\\theta, 1}(s) = r(s, a) + \\gamma \\ V_\\theta(s')\n",
    "\\end{equation}\n",
    "\n",
    "This is called a one-step estimate, since it only uses one sample from the environment. The \n",
    "resulting $V_{\\theta, 1}(s)$ is less biased than $V_\\theta(s)$ by itself, but has higher variance,\n",
    " because the single-sample-expectation is unbiased, but has high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What would be a fully unbiased estimate of $V(s)$ given a full trajectory? What about a biased estimate with lowest possible variance?\n",
    "How can one balance bias and variance in the estimate?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-Lambda and Generalized Advantage Estimation\n",
    "\n",
    "Above we have seen the one-step estimate of the value function. It is also possible to use a multi-step estimate:\n",
    "\n",
    "\\begin{equation}\n",
    "V_{\\theta, n}(s) = r(s, a) + \\gamma \\ r(s_1, a_1) + \\gamma^2 \\ r(s_2, a_2) + \\dots + \\gamma^n \\ V_\\theta(s_n).\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For $n = \\infty$, this is the Monte Carlo estimate, that does not use $V_\\theta$ at all (although for truncated trajectories, \n",
    "$V_\\theta$ would still be used for the last state).\n",
    "\n",
    "![TD-Lambda](_static/images/50_td_lambda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for any $n$ one has a valid estimate of $V(s)$, one can combine them into a weighted average:\n",
    "\n",
    "\\begin{equation}\n",
    "V_{\\theta, \\lambda}(s) = (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} V_{\\theta, n}(s)\n",
    "\\end{equation}\n",
    "\n",
    "Without too much effort, one can derive a recursive formula for conveniently computing $V_{\\theta, \\lambda}(s)$\n",
    "for a trajectory $(s_0, a_0, r_0, s_1, a_1, r_1, \\dots, s_T, a_T, r_T, s_{T+1})$ by traversing it backwards:\n",
    "\n",
    "\\begin{equation}\n",
    "V_{\\theta, \\lambda}(s_t) = (1 - \\lambda) \\left[ r_t + \\gamma \\ V_{\\theta, \\lambda}(s_{t+1}) \\right] + \\lambda \\ V_{\\theta, \\lambda}(s_{t+1})\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "V_{\\theta, \\lambda}(s_T) = r_T + \\gamma \\ V_\\theta(s_{T+1}).\n",
    "\\end{equation}\n",
    "\n",
    "The parameter $\\lambda$ controls the trade-off between bias and variance, and this approach for estimating values is called TD-Lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact same approach can be used for estimating the Advantage function (or the Q-function), which is called Generalized Advantage Estimation (GAE):\n",
    "\n",
    "\\begin{equation}\n",
    "A_{\\theta, \\lambda}(s, a) = (1 - \\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} A_{\\theta, n}(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "which for a trajectory $(s_0, a_0, r_0, s_1, a_1, r_1, \\dots, s_T, a_T, r_T, s_{T+1})$ can be computed recursively as:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{\\theta, \\lambda}(s_t, a_t) = (1 - \\lambda) \\left[ r_t + \\gamma \\ A_{\\theta, \\lambda}(s_{t+1}, a_{t+1}) \\right] + \\lambda \\ A_{\\theta, \\lambda}(s_{t+1}, a_{t+1})\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "A_{\\theta, \\lambda}(s_T, a_T) = r_T + \\gamma V_\\theta(s_{T+1}) - V_\\theta(s_T).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above considerations, a somewhat rarely mentioned identity can be directly derived:\n",
    "\n",
    "For $(s, a, r, s')$ being parts of a trajectory, and with the advantage and value functions being estimated from the trajectory as above, the following holds:\n",
    "\n",
    "\\begin{equation}\n",
    "A_{\\theta, \\lambda}(s, a) = V_{\\theta, \\lambda}(s') - V_\\theta(s).\n",
    "\\end{equation}\n",
    "\n",
    "Thus, TD-Lambda and GAE are equivalent methods for estimating the value or the advantage function on a trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Value Functions\n",
    "\n",
    "Now that we have seen how to estimate value functions, we can turn to the question of how to learn them.\n",
    "In the following, we will focus on the value function $V_\\theta(s)$, but the same considerations apply to the Q-function $Q_\\theta(s, a)$.\n",
    "\n",
    "The sample-improved values that we have seen above can be viewed as **targets** for the value function. Thus, a straightforward learning approach would be to perform **supervised regression** by minimizing\n",
    " some error (often the mean-squared-error (MSE)) between the current value function and the target.\n",
    " \n",
    "The difference between the current value function and the target is called the Bellman error, or TD-error:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_1 = V_\\theta(s) - \\left[ r(s, a) + \\gamma \\ V_\\theta(s') \\right]\n",
    "\\end{equation}\n",
    "\n",
    "or, for TD-Lambda / GAE:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_\\lambda = V_\\theta(s) - V_{\\theta, \\lambda}(s)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Given some policy, an environment, and a value function represented by a neural network, how would you train the latter? Are there any possible problems you can think of?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** For learning a Q-function represented by a neural network, in practice one usually uses the 1-step target. Why do you think this is the case?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Value Functions to Actions - Value Iteration\n",
    "\n",
    "\n",
    "So far, we have only considered how to compute value functions. Now we will turn to the question of how to use them to select good actions.\n",
    "\n",
    "A simple way is to improve on the actions performed by some policy $\\pi$ is to estimate the Q-function, and then greedily select the action with the highest Q-value:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi'(s) := \\arg \\max_{a \\in A} Q_{\\pi,\\theta}(s, a)\n",
    "\\end{equation}\n",
    "\n",
    "Note that $\\pi'$ is a deterministic policy. \n",
    "Unsurprisingly, if $Q_{\\pi,\\theta}$ were the true $Q_\\pi$, this would lead to $\\pi'$ being better than $\\pi$ \n",
    "(due to the *policy improvement theorem*). In the function-approximation case, we can just hope that the estimate of $Q$ is good enough.\n",
    "\n",
    "Now the Q-function of this improved policy $\\pi'$ can be estimated by:\n",
    "\n",
    "\\begin{equation}\n",
    "Q_{\\pi',\\theta}(s, a) = r(s, a) + \\gamma \\max_{a'} Q_{\\pi,\\theta}(s', a')\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Q-function was already optimal (i.e., it would represent the best possible policy) - let us call it $Q^*$ - then it would satisfy the Bellman optimality equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Q^*(s, a) = r(s, a) + \\gamma\\mathbb{E}_{s'} [\\max_{a'} Q^*(s', a')]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The core idea of **value iteration** in the style of deep Q-learning is to use the improved Q-function as a target for the current Q-function.\n",
    "Thus, one wants to minimize the (mean of squares of the) 1-step Bellman error for the Q-function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_1 = Q_{\\pi,\\theta}(s, a) - \\left[ r(s, a) + \\gamma \\max_{a'} Q_{\\pi,\\theta}(s', a') \\right]\n",
    "\\end{equation}\n",
    "\n",
    "When this error is zero, the Q-function is optimal.\n",
    "The policy at any point is then given by simply selecting the action with the highest Q-value: $\\pi(s) := \\arg \\max_{a \\in A} Q_{\\pi,\\theta}(s, a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How would you design a Q-based deep RL algorithm based on the above idea? For which kind of environments would it work, and which problems could arise?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration and the Policy Gradient Theorem\n",
    "\n",
    "While purely value-based methods can give rise to powerful deep RL algorithms, they are difficult to use \n",
    "in environments with continuous action spaces. Therefore, we now go back to the original RL objective, \n",
    "and turn to policy-based methods. At the center of many policy-based methods is a\n",
    "fundamental result called the **policy gradient theorem**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) \\propto \\mathbb{E}_{\\pi_\\theta} \\left[ \\left( Q_\\pi(s, a) - b(s) \\right) \\nabla_{\\theta} \\log \\pi_\\theta(a|s) \\right]\n",
    "\\end{equation}\n",
    "  \n",
    "Where $b(s)$ is an arbitrary state-dependent **baseline** function.\n",
    "The most common choice for $b(s)$ is the value function $V(s)$, which leads to the policy gradient of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) \\propto \\mathbb{E}_{\\pi} \\left[ A_\\pi(s, a) \\nabla_{\\theta} \\log \\pi_\\theta(a|s) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Note that $\\mathbb{E}_{\\pi} \\left[ A_\\pi(s, a) \\right] = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different RL algorithms are obtained by choosing different methods for estimating advantages $A(s, a)$.\n",
    "For example, not using any learned function and estimating $A(s, a)$ by the Monte Carlo return $R(s, a)$\n",
    "leads to the REINFORCE algorithm.\n",
    "\n",
    "Using a learned value function $V_\\theta(s)$\n",
    " for estimating $A(s, a)$ leads to many successful\n",
    "**Actor-Critic algorithms**, such as A2C, A3C, and PPO. There, the *actor* is the policy $\\pi_\\theta(a|s)$,\n",
    "and the *critic* is typically a parameterized value function $V_\\theta(s)$. So, the previous section on\n",
    "policy evaluation comes in handy for policy-gradient methods as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the most straightforward implementation of this policy search scheme, the advantages $A_\\pi$ are\n",
    "estimated on the fly, i.e. in an on-policy fashion.\n",
    "\n",
    "Note that the critic is only ever used on values sampled from the environment with the, contrary to\n",
    "the value-iteration approach (where $\\max$ and $\\arg \\max$ have to be computed). Moreover, the critic\n",
    "is only used during training and is typically discarded during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How would you design a deep Actor-Critic RL algorithm? How would you perform updates of the actor and critic networks?\n",
    " What are important hyperparameters?\n",
    " \n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Methods for Continuous Control - DDPG and its Variants\n",
    "\n",
    "The Q-based method outlined above don't lend themselves directly for continuous action spaces - the $\\max$ and $\\arg \\max$\n",
    "operations are too expensive and unreliable there.\n",
    "\n",
    "Fortunately, the deterministic policy gradients (DPG) approach and its deep-learning successors give an elegant way\n",
    "out of this. This core idea is the following: instead of maximizing the Q-function over the action space, one can\n",
    "learn a deterministic policy $\\mu_\\theta(s)$ that takes the role of the $\\arg \\max$ operation. The Q-function is then\n",
    "updated by minimizing the (mean square of the) 1-step Bellman error:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta_1 = Q_{\\pi,\\theta}(s, a) - \\left[ r(s, a) + \\gamma \\ Q_{\\pi,\\theta}(s', \\mu_\\theta(s')) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "The policy is updated by using the **deterministic policy gradient theorem**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\theta} J(\\theta) \\propto \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_{\\theta} \\mu_\\theta(s) \\nabla_{a} Q_\\pi(s, a) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "For sampling actions from the policy during training, usually some exploration noise is added to the output of the policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since now the Q-values are estimated directly from a single sample (and not from a trajectory, like described above), \n",
    "the DPG-family of algorithms is amenable to off-policy learning.\n",
    "\n",
    "**Question:** This is a good moment to pause and look back at the algorithm schemes that we saw above. How would\n",
    "you summarize the differences between these approaches? What are the advantages and disadvantages of each? What\n",
    "could be turning knobs for improving their stability and efficiency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Important Topics in Algorithm Design\n",
    "\n",
    "Two major recurring (and highly related) schemes in derivations of RL algorithms are\n",
    "\n",
    "1. Regularization and conservative updates\n",
    "2. Entropy maximization, aka soft learning\n",
    "\n",
    "Regularization techniques build on the idea that changes to policies (and other quantities) should happen slowly,\n",
    "and entropy maximization roughly means that policies should be as stochastic as possible while still\n",
    "leading to high rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common technique for incorporating regularization is to include a penalty for deviating\n",
    "too far from the previous directly into the RL objective. Algorithms like PPO, TRPO as well as REPS, AWR and follow-ups\n",
    "are derived this way. This approach is also often used for RL from human feedback (RLHF) in language modelling.\n",
    " \n",
    "A nice aspect of this form is that an analytic, non-parametric solution of the modified RL objective can often be derived explicitly.\n",
    "\n",
    "With the modified objective being\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\pi') = \\mathbb{E}_{\\pi'} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) - \\alpha \\mathcal{D}_{KL}(\\pi'(\\cdot|s_t) || \\pi(\\cdot|s_t)) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "the solution is given by (using the policy gradient theorem and the value function as state-dependent baseline):\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi'(a|s) \\sim \\pi(a|s) \\exp \\left( \\frac{1}{\\alpha} A_\\pi(s, a) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "which is a weighted version of the original policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy maximizing algorithms like Soft Actor Critic (SAC) are often naturally derived from the \"RL as inference\" perspective,\n",
    "despite the original derivation of SAC following a different route. The objective considered in SAC is\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\pi) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t)) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{H}(\\pi(\\cdot|s_t))$ is the entropy of the policy distribution $\\pi(\\cdot|s_t)$.\n",
    "\n",
    "\n",
    "Maximising the entropy of the policy adds noise to the exploration, which is often beneficial for learning. The soft family of\n",
    "actor critic algorithms often tends to be more sample efficient and stable than their non-soft counterparts. An initial difficulty\n",
    "of these algorithms was that $\\alpha$ was a hyperparameter that had to be tuned carefully. However, follow-up work\n",
    "has introduced methods for automatically tuning $\\alpha$, significally simplifying the use of these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "So far, we have mainly talked about policy evaluation and policy improvement. Going back to our original comparison to\n",
    "supervised learning, this merely addresses the parts\n",
    "\n",
    "1. Train a model on a dataset to fulfill some objective\n",
    "2. Evaluate the model on a separate test set\n",
    "\n",
    "However, the most important part about how to obtain a suitable dataset and to steer the data acquisition process\n",
    "is still missing. This is the part of **exploration**, and is arguably the hardest and most important aspect of an RL\n",
    "algorithm.\n",
    "\n",
    "**Question:** What are some possible ways of exploration that you can think of? How does exploration differ for on-policy and off-policy algorithms?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration in RL is a very active area of research, and many modifications of existing RL algorithms have been proposed\n",
    "to handle exploration in a variety of settings. We will not go into a deep dive here, but instead have a small live \n",
    "discussion about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "remove-cell",
     "remove-cell-nbconv"
    ]
   },
   "source": [
    "<img src=\"_static/images/aai-institute-cover.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\">Thank you for the attention!</div>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "rise": {
   "footer": "<img src='_static/images/aai-logo.png' alt='logo' height='50em'>",
   "header": "<img src='_static/images/transferlab-logo.svg' alt='logo' height='20em' />",
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "148px",
    "width": "256px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "563.2px",
    "left": "125px",
    "top": "116.469px",
    "width": "307px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
