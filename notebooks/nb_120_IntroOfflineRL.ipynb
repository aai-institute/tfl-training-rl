{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Offline Reinforcement Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning algorithms primarily rely on an online learning approach, which poses a significant challenge to their widespread use. RL typically involves a continuous process of gathering experience by engaging with the environment using the latest policy, and then using this experience to enhance the policy. **In many situations, this online interaction is neither practical nor safe due to costly or risky data collection, as seen in fields such as robotics, healthcare, etc.**.\n",
    "\n",
    "**Even in cases where online interaction is viable, there is often a preference to leverage existing data**. This is particularly true in complex domains where substantial datasets are essential for effective generalization.\n",
    "\n",
    "**Offline Reinforcement Learning(RL), also known as Batch Reinforcement Learning, is a variant of RL that effectively leverages large, previously collected datasets for large-scale real-world applications**. The use of static datasets means that during the training process of the agent, offline RL does not perform any form of online interaction and exploration, which is also the most significant difference from online reinforcement learning methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the problem to be solved?\n",
    "\n",
    "The offline RL problem can be defined as a data-driven approach to the previously seen online RL methods. As before, the goal is to minimize the discounted expected reward:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "J (\\pi) = \\mathbb{E}_{\\tau \\sim D}  \\left[ \\sum_{t = 0}^{\\infty} \\gamma^t r (s_t, a_t) \\right]\n",
    "\\tag{1}\n",
    "\\label{eq:discounted_rew_offline}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "But this is done directly on the dataset $D$:\n",
    "\n",
    "$$\n",
    "D = \\{(s_0, a_0, r_0), (s_1, a_1, r_1), \\ldots, (s_T, a_T, r_T)\\} \\quad \\tag{Dataset}\n",
    "$$\n",
    "\n",
    "comprising state/action/reward values. This is in contrast to online RL, where the trajectories $\\tau$ are collected through interaction with the environment. **Note that $D$ doesn't necessarily need to be related to the specific task at hand, but it should be sufficiently representative (i.e., containing high-reward regions of the problem). If $D$ is derived from data collected from unrelated tasks, we will need to design our own reward function, just as we did in the online RL setting.**\n",
    "\n",
    "The dataset can be collected from a suboptimal policy, a random policy, provided by a human expert, or a mixture of them. Such a policy is called a behavior policy (or expert policy), denoted as $\\pi_b$.\n",
    "\n",
    "As an example of a behavior policy ($\\pi_b$), when training a robot to navigate a room, $\\pi_b$ might involve rules like \"avoid obstacles,\" \"move forward,\" or \"stop.\" It could be operated manually by a human who controls the robot based on sensor data, such as lidar for collision detection, or it could be a combination of suboptimal policies.\n",
    "\n",
    "**The goal of offline RL is to derive an optimal or near-optimal policy directly from $D,\" without requiring further interactions with the environment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other important points:\n",
    "\n",
    "- Offline RL differs from \"imitation learning\" (discussed later), which is its supervised learning counterpart. **In imitation learning, the policy closely mirrors the behavior policy, while offline RL aims for a superior policy, ideally near the optimal one.**\n",
    "\n",
    "- Offline RL brings complexity because the offline policy differs in state-action distributions from the behavior policy, leading to **distributional shift challenges**. Similar challenges may arise in behavioral cloning, discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples where offline RL could be highly benefitial:\n",
    "    \n",
    "**Decision Making in Health Care**: In healthcare, we can use Markov decision processes to model the diagnosis and treatment of patients. Actions are interventions like tests and treatments, while observations are patient symptoms and test results. Offline RL is safer and more practical than active RL, as treating patients directly with partially trained policies is risky.\n",
    "\n",
    "**Learning Robotic Manipulation Skills**: In robotics, we can use active RL for skill learning, but generalizing skills across different environments is challenging. Offline RL allows us to reuse previously collected data from various skills to accelerate learning of new skills. For example, making soup with onions and carrots can build on past experiences with onions and meat or carrots and cucumbers, reducing the need for new data collection.\n",
    "\n",
    "**Learning Goal-Directed Dialogue Policies**: Dialogue systems, like chatbots helping users make purchases, can be modeled as MDPs. Collecting data by interacting with real users can be costly. Offline data collection from humans is a more practical approach for training effective conversational agents.\n",
    "\n",
    "**Autonomous Driving**: Training autonomous vehicles in real-world environments can be dangerous and costly. Offline RL can use data from past driving experiences to improve vehicle control and decision-making, making it safer and more efficient.\n",
    "\n",
    "**Energy Management**: Optimizing energy consumption in buildings or industrial processes can be a critical task. Offline RL can analyze historical energy usage data to develop efficient control strategies, reducing energy costs and environmental impact.\n",
    "\n",
    "**Finance**: Portfolio management and trading strategies often require learning from historical financial data. Offline RL can help develop and refine investment policies by utilizing past market data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the process is clear:\n",
    "\n",
    "**Phase A**: Collect data set, $D$, of state-action paird through a behavior policy $\\pi_b$: e.g. a robot randomly moving (or human controlled) in a given space, data collected from an autonomous vehicle, etc. The data collected doesn't need to come from an expert (typically the case in real situations) and during this phase we are not worry in general about a specific task (i.e. rewards). In fact it could be that the data is collected from a robot doing a different task that the one we are interested in. We want just a set of allowed state-action pairs that could be usable and representative for the task in mind.\n",
    "\n",
    "**Phase B**: In this phase we want to solve a given task (so we need to design rewards) only through the provided initial data without any interaction with the environment but still be able to find an optimal or near-optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will explore different points in offline Reinforcement Learning and in particular:\n",
    "\n",
    "    1. The technical challenges in offline RL.\n",
    "    \n",
    "    2. Differences between approaches like imitation learning and online vs. offline RL, \n",
    "       and what we can adapt from online methods for the offline setting.\n",
    "\n",
    "       \n",
    "    3. Standard data collection approaches and libraries used in the offline RL \n",
    "       community\n",
    "\n",
    "For our exercises, we'll primarily use a 2D grid environment for two important reasons:\n",
    "\n",
    "    1. It allows for quick training and data collection, making trainings and \n",
    "       data-collection quite fast giving you the possibility to play around in the \n",
    "       workshop!\n",
    " \n",
    "    2. Simplifying the environment helps us focus on the fundamental concepts and \n",
    "       benefits of offline RL, which can be challenging in high-dimensional spaces.\n",
    "\n",
    "However, keep in mind that both the provided library and the exercises can be adapted to handle more complex scenarios so feel free to give them a try!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[ Levine et. al. '2021 - Offline Reinforcement Learning: Tutorial, Review,\n",
    "and Perspectives on Open Problems ](https://arxiv.org/pdf/2005.01643.pdf)\n",
    "\n",
    "[Prudencio et. al. ' 2023 - A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems ](https://arxiv.org/pdf/2203.01387.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "tianshou_dev",
   "language": "python",
   "name": "tianshou_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
