{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T08:19:31.038213Z",
     "end_time": "2023-11-23T08:19:32.729502Z"
    }
   },
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import warnings\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import RenderMode\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "import minari\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    MinariDatasetConfig, create_minari_datasets\n",
    "from training_rl.offline_rl.scripts.visualizations.utils import get_state_action_data_and_policy_grid_distributions, \\\n",
    "    snapshot_env\n",
    "from training_rl.offline_rl.custom_envs.utils import InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, state_action_histogram\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "from training_rl.offline_rl.utils import compare_state_action_histograms\n",
    "import torch\n",
    "import os\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from copy import copy\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "load_env_variables()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "register_grid_envs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this exercise, we will work with a simple example of Behavioral cloning (BC) . The goal is to explore some of the issues with imitation learning and become familiar with the Tianshou library for offline RL training!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we will create a Minari dataset, as we did previously, but this time we will use the function **create_minari_datasets(...)** which saves some useful metadata. This metadata is important for recreating the environment associated with the data when testing our trained policy.\n",
    "\n",
    "We will also utilize one of our registered behavioral policies, but let's first take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T08:19:56.844041Z",
     "end_time": "2023-11-23T08:19:56.851212Z"
    }
   },
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "BEHAVIOR_POLICY = BehaviorPolicyType.behavior_8x8_random_towards_left_within_strip\n",
    "OFFLINE_POLICY = PolicyName.imitation_learning\n",
    "\n",
    "# Two dimensional grid world data\n",
    "OBSTACLE = ObstacleTypes.obst_big_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the policy is not random; it includes expert information, as it has a bias to reach the target**. This is, of course, the preferred situation in realistic problems as well and the perfect situation to apply Behavioral Cloning (BC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's configure our Minari dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T08:20:06.535993Z",
     "end_time": "2023-11-23T08:20:07.272402Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER = \"_exercise_5\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "NUM_STEPS = 10000\n",
    "\n",
    "\n",
    "\n",
    "minari_dataset = create_minari_datasets(\n",
    "    env_name=ENV_NAME,\n",
    "    dataset_name=DATA_SET_NAME,\n",
    "    dataset_identifier=DATA_SET_IDENTIFIER,\n",
    "    version_dataset=VERSION_DATA_SET,\n",
    "    num_colected_points=NUM_STEPS,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    ")\n",
    "\n",
    "data = minari.load_dataset(minari_dataset.data_set_name)\n",
    "print(\"number of episodes collected: \", len(data))\n",
    "#for elem in data:\n",
    "#    print(elem.actions, elem.truncations, elem.terminations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the state-action data distribution. Since we are going to use the Tianshou RL library, we will load the previous dataset into a Tianshou ReplayBuffer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T08:20:09.958443Z",
     "end_time": "2023-11-23T08:20:10.972801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data saved in /offline_data\n",
    "NAME_EXPERT_DATA = minari_dataset.data_set_name\n",
    "\n",
    "# fill the ReplyBuffer\n",
    "buffer_data = load_buffer_minari(NAME_EXPERT_DATA)\n",
    "\n",
    "# We will need the env for the plots. \n",
    "data_config = MinariDatasetConfig.load_from_file(NAME_EXPERT_DATA)\n",
    "env_config = data_config.initial_config_2d_grid_env\n",
    "render_mode = RenderMode.RGB_ARRAY_LIST\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=env_config)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.007))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-action distribution makes sense! The agent starts at (0,0), and since the episode length is finite (200 steps), the ball will more likely be around the upper-left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train our first offline RL algorithm: Imitation Learning.\n",
    "\n",
    "Before we proceed, let's take a moment to become a little more familiar with the code. Let's spend some minutes reviewing:\n",
    "\n",
    "    a - il_policy.py\n",
    "    b - policy_registry.py \n",
    "    c - training.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-11-23T08:20:16.558905Z",
     "end_time": "2023-11-23T08:20:23.233513Z"
    }
   },
   "outputs": [],
   "source": [
    "# The model policy to be trained.\n",
    "\n",
    "POLICY_NAME = PolicyName.imitation_learning\n",
    "\n",
    "NUM_EPOCHS = 300\n",
    "BATCH_SIZE = 256\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at the state-action BC policy distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T08:20:27.821511Z",
     "end_time": "2023-11-23T08:20:58.988447Z"
    }
   },
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "NUM_EPISODES = 40 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.007))\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the learned policy has a very similar distribution to the dataset, so they will perform similarly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now visualize the policy \n",
    "\n",
    "Below the imitation_policy_sampling=False arguments will give us the $\\arg \\max_a \\pi(s|a)$. By setting it to True you will be sampling actions from the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As our dataset includes a fair amount of expert data, by taking the $\\arg \\max_a \\pi(s|a)$, we are able to remove the noise from the data and obtain the expert policy! This is a nice property of imitation learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that in real life, the forbidden zone (black region) in the environment could be a playground or a garden, and there could be a good reason not to go there. So let's use our trained agent and see what happens when we remove the restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the obstacle and examine the state-action distribution. What do you observe? Can you explain it?**\n",
    "\n",
    "Hint: You can create a copy of the previous environment and use ObstacleTypes.obst_free_8x8 in your configuration, like this:\n",
    "\n",
    "    new_env_config = copy(env_config)\n",
    "    new_env_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "    env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), \n",
    "    env_config=new_env_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T08:21:18.880512Z",
     "end_time": "2023-11-23T08:21:54.692452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's now remove the forbidden region and recreate the environment\n",
    "new_env_config = copy(env_config)\n",
    "new_env_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=new_env_config)\n",
    "\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "\n",
    "state_action_histogram(state_action_count_policy, title=\"State-Action policy distribution\")\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISTRIBUTIONAL SHIFT EFFECT**: As we can see, there is a distributional shift effect in this case, mainly due to out-of-distribution state-action pairs as the agent attempts to move into unexplored regions. This is an undesired effect caused by function approximation, i.e., the DNN policy cannot perfectly capture the state-action distribution in regions where there is little or no data. This effect is highly undesirable and should be eliminated. When close to unexplored regions, the policy will behave unpredictably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a forbidden position to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_env_config = copy(env_config)\n",
    "new_env_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "new_env_config.initial_state = (4,0)\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=render_mode), env_config=new_env_config)\n",
    "\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the behavior of the agent when it starts from out-of-distribution data is entirely unexpected. This is normal because the provided expert policy is only designed to guide the agent from any point within the allowed zone to the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, imitation learning becomes interesting when one has access to noisy expert data, as it provides a means to reduce noise from the expert data.**\n",
    "\n",
    "**Another noteworthy aspect of imitation learning is that it doesn't rely on rewards, making it a less complex solution to the problem of reward shaping in reinforcement learning.**\n",
    "\n",
    "However, in realistic applications, obtaining expert data is often a challenge. In many cases, we have access to only a limited number of trajectories, rendering this method less useful for extracting optimal policies.\n",
    "\n",
    "This is where offline RL algorithms come into play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl_ivan",
   "language": "python",
   "name": "training_rl_ivan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
