{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import warnings\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "from training_rl.offline_rl.scripts.visualizations.utils import snapshot_env\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv, RenderMode\n",
    "import gymnasium as gym\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "import torch\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "import os\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "\n",
    "\n",
    "load_env_variables()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ToDo: this should be load automatically\n",
    "register_grid_envs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this exercise we will test how conservative are the CQL and the BCQ**. \n",
    "\n",
    "As mentioned before regularization methods (like CQL) will be a good chooice when you want a more safe behavior for your agent but if you are intreseted in a optimal solution without too much care about safety contrainted methods like BCQ could be the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Env. Config.\n",
    "OBSTACLE = ObstacleTypes.horizontal_line_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), \n",
    "                                          env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIFIER_COMBINED_DATASETS = \"_conservative_test\"\n",
    "\n",
    "# Dataset I\n",
    "#BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip_and_left\n",
    "DATA_SET_IDENTIFIER_I = \"_move_downwards\"\n",
    "NUM_STEPS_I = 3000\n",
    "\n",
    "# Dataset II\n",
    "#BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "DATA_SET_IDENTIFIER_II = \"_move_deterministic\"\n",
    "NUM_STEPS_II = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Minari combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_colected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"combined_dataset\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering behavioral policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Train the CQL and BCQ algorithms on the collected dataset and extract conclusions about what do you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose yuor policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.cql_discrete\n",
    "\n",
    "\n",
    "NAME_EXPERT_DATA = config_combined_data.data_set_name\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ( If you want to use an already trained policy run the cell below and skip this one !! )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Let use the same policies as before but let's change the target to (4,7). \n",
    "\n",
    "2-a) Use a free obstacle enviroment. What do you think are going to be the results?\n",
    "\n",
    "2-b) Use the BCQ now with the obstacle call wall_object8x8 (Please, run the next cell to give a look.)\n",
    "     What do you think is going to happen with the BCQ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config.obstacles = ObstacleTypes.wall_object_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), \n",
    "                                          env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou_dev",
   "language": "python",
   "name": "tianshou_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
