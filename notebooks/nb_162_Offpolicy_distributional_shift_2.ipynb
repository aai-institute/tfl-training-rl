{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import warnings\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv\n",
    "from training_rl.offline_rl.scripts.visualizations.utils import snapshot_env\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import RenderMode\n",
    "from training_rl.offline_rl.custom_envs.utils import InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_combined_minari_dataset\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from training_rl.offline_rl.scripts.visualizations.utils import get_state_action_data_and_policy_grid_distributions\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, state_action_histogram\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "import torch\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "import os\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "load_env_variables()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ToDo: this should be load automatically\n",
    "register_grid_envs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook we will deal with another importat property that we should fulfill with a robust offline RL algorithm, the stitching, i.e. the reuse of different trajectories contain in the data to obtain the best trajecroy in our dataset.**\n",
    "\n",
    "\n",
    "The goal will be to reach a target at (7,7) starting from (0,0). We will use again the 8x8 grid environment. Our dataset contains trajectories covering our space of interest but generated for different tasks (note that before we collected data for the same task) . One is a suboptimal policy that moves the agent from (0,0) to (7,0) and the other is a deterministic an optimal one (human expert) that brings the agent from (4,0) to (7,7). We have obviously much more data coming from the suboptimal policy than the expert one as it is cheaper.\n",
    "\n",
    "So we will create the two policies:\n",
    "\n",
    "I  - **Suboptimal expert policy** (behavior_8x8_moves_downwards_within_strip):  moves agent in suboptimal way downwards starting from (0,0) (collect 8000 steps)\n",
    "\n",
    "II - **Optimal expert poilcy**(behavior_8x8_deterministic_4_0_to_7_7): moves agent in the optimal path from (4,0) to (7,7) (collect 300 steps) \n",
    "\n",
    "\n",
    "In this example we will use again as off-policy RL algorithm, the Deep Q-Network (DQN) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our configuration and create the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.vertical_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_IDENTIFIER_I = \"_downwards_\"\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip\n",
    "NUM_STEPS_I = 8000\n",
    "\n",
    "DATA_SET_IDENTIFIER_II = \"_optimal_\"\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "NUM_STEPS_II = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create combined Minari dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "        env_name=ENV_NAME,\n",
    "        dataset_identifiers = (DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "        num_colected_points = (NUM_STEPS_I, NUM_STEPS_II),\n",
    "        behavior_policy_names = (BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "        combined_dataset_identifier = \"_stiching\",\n",
    "        env_2d_grid_initial_config = env_2D_grid_initial_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering behavioral policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_I,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert policy\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_combined_dataset = config_combined_data.data_set_name\n",
    "\n",
    "#Create Buffers with minari datasets\n",
    "buffer_data = load_buffer_minari(name_combined_dataset)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.012))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_NAME = PolicyName.dqn\n",
    "\n",
    "NAME_EXPERT_DATA = name_combined_dataset\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy.pth\"\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config.obstacles = OBSTACLE.obst_free_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "\n",
    "1 - What do you notice? What happens if you increase the collected data? Is it better? \n",
    "\n",
    "2 - Try again with the offline BCQ algorithm as we did in the previous example. What happens now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl_ivan",
   "language": "python",
   "name": "training_rl_ivan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
