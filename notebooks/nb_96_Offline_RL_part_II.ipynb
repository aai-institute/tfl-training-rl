{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*\n",
       "This file is mainly copy-pasta from rise's examples\n",
       "https://github.com/damianavila/RISE/blob/master/examples/rise.css\n",
       "that was further customized for appliedAI purposes\n",
       "*/\n",
       "@import url('https://fonts.googleapis.com/css2?family=Work+Sans:wght@400&display=swap');\n",
       "\n",
       "\n",
       "/*\n",
       "body {\n",
       "    font-family: 'Work Sans', sans-serif !important;\n",
       "    text-transform: initial !important;\n",
       "    letter-spacing: initial !important;\n",
       "    font-weight: 400 !important;\n",
       "    line-height: 1.5 !important;\n",
       "    text-size-adjust: 100% !important;\n",
       "    ‑webkit‑text‑size‑adjust: 100% !important;\n",
       "}\n",
       "\n",
       "\n",
       ".reveal, div.text_cell_render, .md-slide, .sidebar-wrapper {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       ".navbar-default .navbar-nav > li > a {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       ".filename {\n",
       "    font-size: 2.4rem !important;\n",
       "    color: #212529 !important;\n",
       "    font-weight: 600 !important;\n",
       "}\n",
       "\n",
       ".reveal, .md-slide {\n",
       "    color: white !important;\n",
       "}\n",
       "\n",
       "h1, h2 {\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "h3, h4, h5, h6 {\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       ".reveal p, .reveal ol, .reveal dl, .reveal ul,\n",
       "div.text_cell_render {\n",
       "    color: #212529 !important;\n",
       "}\n",
       "\n",
       "/*copied from stackoverflow, better spacing between list items*/\n",
       "li + li {\n",
       "  margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "body.rise-enabled .reveal ol, body.rise-enabled .reveal dl, body.rise-enabled .reveal ul {\n",
       "    margin-left: 0.1em;\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       ".reveal .rendered_html h1:first-child,\n",
       ".reveal .rendered_html h2:first-child,\n",
       ".reveal .rendered_html h3:first-child,\n",
       ".reveal .rendered_html h4:first-child,\n",
       ".reveal .rendered_html h5:first-child {\n",
       "    margin-top: 0.2em;\n",
       "}\n",
       "\n",
       "/*\n",
       ".CodeMirror-lines, .output_text {\n",
       "    font-size: 1.5rem !important;\n",
       "}\n",
       "*/\n",
       "\n",
       "h1.plan, h2.plan, h3.plan {\n",
       "    text-align: center;\n",
       "    padding-bottom: 30px;\n",
       "}\n",
       "\n",
       "ul.plan>li>span.plan-bold {\n",
       "    font-size: 110%;\n",
       "    padding: 4px;\n",
       "    font-weight: bold;\n",
       "    background-color: #eee;\n",
       "}\n",
       "\n",
       "ul.plan>li>ul.subplan>li>span.plan-bold {\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".plan-strike {\n",
       "    opacity: 0.4;\n",
       "/*    text-decoration: line-through; */\n",
       "}\n",
       "\n",
       "div.plan-container {\n",
       "    display: grid;\n",
       "    grid-template-columns: 50% 50%;\n",
       "}\n",
       "\n",
       "/*\n",
       " * this is to void xarray's html output to show the fallback textual representation\n",
       " * see also\n",
       "   * xarray.md and\n",
       "   * https://github.com/damianavila/RISE/issues/594\n",
       " */\n",
       ".reveal pre.xr-text-repr-fallback {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "#toc-header, .toc-item li {\n",
       "    margin: auto !important;\n",
       "    color: #808080 !important;\n",
       "}\n",
       "\n",
       "#toc, #toc-wrapper, .toc-item-num, #toc a, .toc {\n",
       "    margin: auto !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "#toc-wrapper {\n",
       "    top: auto !important;\n",
       "    bottom: auto !important;\n",
       "    margin-top: 2rem !important;\n",
       "    color: #00747b !important;\n",
       "}\n",
       "\n",
       "\n",
       "#rise-header {\n",
       "    margin: 10px;\n",
       "    left: 5%;\n",
       "}\n",
       "\n",
       "#rise-footer {\n",
       "    margin: 10px;\n",
       "    right: 5%;\n",
       "}\n",
       "\n",
       "#rise-backimage {\n",
       "    opacity: 0.70;\n",
       "}\n",
       "\n",
       ".reveal img {\n",
       "    max-width: 100%;\n",
       "}\n",
       "\n",
       "\n",
       ".md-slide.title {\n",
       "    position: relative;\n",
       "    top: -50%;\n",
       "    margin-left: 5%;\n",
       "    font-size: 4em !important;\n",
       "    line-height: 1.6;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%presentation_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "$\\newcommand{\\vect}[1]{{\\mathbf{\\boldsymbol{#1}} }}$\n",
       "$\\newcommand{\\amax}{{\\text{argmax}}}$\n",
       "$\\newcommand{\\P}{{\\mathbb{P}}}$\n",
       "$\\newcommand{\\E}{{\\mathbb{E}}}$\n",
       "$\\newcommand{\\R}{{\\mathbb{R}}}$\n",
       "$\\newcommand{\\Z}{{\\mathbb{Z}}}$\n",
       "$\\newcommand{\\N}{{\\mathbb{N}}}$\n",
       "$\\newcommand{\\C}{{\\mathbb{C}}}$\n",
       "$\\newcommand{\\abs}[1]{{ \\left| #1 \\right| }}$\n",
       "$\\newcommand{\\simpl}[1]{{\\Delta^{#1} }}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_latex_macros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"_static/images/aai-institute-cover.svg\" alt=\"Snow\" style=\"width:100%;\">\n",
    "<div class=\"md-slide title\"> Addressing distributional shift </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing Distributional Shift in Offline RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various approaches, but the core idea is to strike a balance where the policy distribution remains reasonably close to the behavioral one while also improving its performance. This involves introducing some distributional shift to enhance the policy without going out of distribution, all while ensuring that the effective sample size remains large enough to be representative during inference. Achieving this balance is a challenging task and a highly active area of research in the RL community.\n",
    "\n",
    "To attain the aforementioned goal, offline RL algorithms can be classified into three primary categories:\n",
    "\n",
    "**I - Policy constraint**\n",
    "\n",
    "**II - Policy Regularization**\n",
    "\n",
    "**III - Importance sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Policy constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Non-implicit or Direct\n",
    "\n",
    "We have access to the behavior policy, $\\bf \\pi_\\beta$. For instance it could be a suboptimal classical policy (i.e. non RL) or computed from behavioral cloning on a given dataset.\n",
    "\n",
    "As we already have $\\pi_\\beta$ we can constrain the learned and behavioral policy through:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(\\pi(.|s)||\\pi_{\\beta}(.|s)) \\leq \\epsilon\n",
    "\\label{dk_1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"_static/images/96_KL_divergence.png\" alt=\"KL divergence\" width=700cm>\n",
    "<div class=\"slide title\"> Fig.1: DKL divergence </div>\n",
    "\n",
    "with the Kullback-leibler divergence, $D_{KL}$, defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(\\pi(.|s)||\\pi_{\\beta}(.|s)) = \\sum_a \\pi(a|s) log \\frac{\\pi(a|s)}{\\pi_{\\beta}(a|s)} \n",
    "\\label{dkl_2}\n",
    "$$\n",
    "\n",
    "\n",
    "As shown in (ref.1 ) if eq.\\ref{dk_1} is satisfy then we can bound locally the state visitation frequency (to be precise it is the $\\gamma$-discounted state distribution), $d_{\\pi}(s)$ and $d_{\\pi_{\\beta}}(s)$ (induced by $\\pi(a|s)$ and \\pi_{\\beta}(a|s), respectively), by a $\\delta$ which is $O\\left(\\frac{\\epsilon}{{(1 - \\gamma)}^2}\\right)$ . \n",
    "In other words, as you can see in the fig. below the minimization of the DKL divergence will favour the case a) and it will punish situations as in case b)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"_static/images/96_policy_constraint_DKL.png\" alt=\"KL divergence\" width=200%>\n",
    "<div class=\"slide title\"> Fig.2: DKL divergence's effect on out-of-distribution data </div>\n",
    "\n",
    "\n",
    "In summary if the state distributions $d_{\\pi}(s)$ and $d_{\\pi_{\\beta}}(s)$ are close enough around a given state, $s$, the space of states visited during data collection will be similar to the space we will encounter in inference and this will reduce undesired o.o.d actions as in case b) but it will encourage the o.o.d actions as in case a) that are quite important to improve the behavior policy $\\pi_\\beta(a|s)$. Remember that we need the distributional shifht of case b) as typically the support state-action pairs of the optimal policy, $\\pi(a|s)$ will be different (and usually smaller) than the one of the behavior policy $\\pi_\\beta(a|s)$.\n",
    "\n",
    "As we will see now in order to find the support state-action pairs of the optimal policy we will reuse some of the methods we already introduced in the online RL part of the workshop.\n",
    "\n",
    "Policy constraint methods use in general the DKL constraint in an actor-critic like approach in order to find the optimal policy, i.e.:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\hat Q}^{\\pi}_{k+1} \\gets \\arg \\min_Q \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q(s,a) - \\mathcal{B}^{\\pi}_k Q(s,a)\\big)^2\\Big] \\quad \\text{ with } \\quad \\mathcal{B(s,a)}^{\\pi}Q = r(s,a) + {\\gamma}\\mathbb{E}_{s' \\sim D, a' \\sim \\pi}Q(s',a') \n",
    "\\tag{Evaluation}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}}_{k+1}(s, a) \\right] \n",
    "\\tag{Improvement}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "D_{KL}(\\pi(.|s), \\pi_{\\beta}(.|s)) \\leq \\epsilon.\n",
    "\\tag{Constraint}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We could add this constraint as a Lagrange multiplier or sometimes it is absorved in the evaluation an improvement steps:\n",
    "\n",
    "$$\n",
    "{\\hat Q}^{\\pi}_{k+1} \\leftarrow \\arg \\min_Q \\mathbb{E}_{(s,a,s')\\sim D} \\left[\\left( Q(s, a) -  r(s, a) + \\gamma \\mathbb{E}_{a' \\sim\\pi_k(a'|s')}[{\\hat Q}^{\\pi}_k(s', a')] -\\alpha\\gamma D_{KL}(\\pi_k(\\cdot|s'), \\pi_\\beta(\\cdot|s')) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}_{k+1}}(s, a) -\\alpha\\gamma D_{KL}(\\pi_k(\\cdot|s), \\pi_\\beta(\\cdot|s)) \\right] \\\\\n",
    "$$\n",
    "\n",
    "These are not equivalent obviously but they produce similar results with the advantage that the second approach is much easier to implement from a technical point of view. But strictly speaking the first approach is the more rigourous one --> TODO: See in papers if they talk about this!!! --> pagina 20 Levine\n",
    "\n",
    "This approach works quite well but what happens if we need to deviate considerably from the behavior policy as it could happens in realistic situations where the data collected is far from optimal. On those cases the $D_{KL}$ constraint could be too conservative.\n",
    "\n",
    "Let's first analize the simple example in the fig. below:\n",
    "\n",
    "<img src=\"_static/images/96_support_policy_constraint.png\" alt=\"offline_rl_4\" width=150%>\n",
    "\n",
    "In this case there are not o.o.d data as we are constrained to the one dimensional grid where all the states are availables. Note that this is similar to fig.X a) above but with the difference that some of the actions will be much more likely than the others. In this case if we constrained through the DKL, i.e. we constraint the policy probablity distributions to be close we will be imitating the bad behavior of $\\pi_\\beta(a|s)$ and we won't be able to get the optimal policy as seen in fig.c). A smarter chooice instead would be to constraint on the behavior policy support, as seen in the fig. below. \n",
    "\n",
    "<img src=\"_static/images/policy_constraint_vs_support.png\" alt=\"offline_rl_4\" width=500cm>\n",
    "<div class=\"slide title\"> Fig.3: distributional vs. support policy constraint </div>\n",
    "\n",
    "In this way o.o.d. action like the ones in fig.X b) will try to be avoided and actions that are in-distribution or close to it as in FigX. a) will be encourage but without any constraint on their probabilites. In this way the learned probabilities will be determined almost entirely from the evaluation-improvement process. By doing so we can get the optimal policy observed in fig.d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implicit \n",
    "\n",
    "We don't need $\\pi_\\beta$, and we can work directly with our data $D$. This is the situation many times as the lack of data or in complex high dimensional spaces cloning a policy that match the real data distribution could be extremely hard.\n",
    "\n",
    "In this approach you assume that you have a behavioral policy $\\pi_\\beta$ (that will be integrated out later) and so you want to find a better one $\\pi$. What you could do is to maximize the difference reward:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\eta(\\pi) = J(\\pi) - J(\\pi_\\beta) \\quad \\hbox{with} \\quad J (\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}  \\left[ \\sum_{t = 0}^{\\infty} \\gamma^t r (s_t, a_t) \\right] \n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "i.e., given the cumulative reward of your behavior policy, $\\pi_\\beta$, try to increase as much as possible the cumulative reward of the learnt policy, $\\pi$.\n",
    "\n",
    "It can be shown that (1) can be written as this (similar to Trust Region Policy Optimization (TRPO) derivation):\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "\\eta(\\pi) = \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\mathbb{E}_{a \\sim \\pi(a|s)} [A^{\\pi_\\beta}(s, a)] \\\\ \\text{s.t.} \\quad D_{KL}(\\pi(\\cdot|s) || \\pi_\\beta(\\cdot|s) ) \\leq \\epsilon\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "\n",
    "The math to reach eq.3 is a bit involve (see ref.1) but we can have an intuitive understanding of what it is doing. It is important to note that eq.3 can be understood qualitatively as seen in the fig below.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"_static/images/96_difference_reward.png\" alt=\"offline_rl_4\" width=200%>\n",
    "\n",
    "\n",
    "\n",
    "**Note that point 3 implies to find a policy $\\pi(a|s)$ that produces state-action pairs $(s_0,a_0)$ (constrained to be close to dataset distribution through the $D_{KL}$ divergence) that generate trajectories on your dataset with maximum reward. In other words by solving eq.2 we are trying to compute a policy that is close to the optimal one on a given dataset.**\n",
    "\n",
    "\n",
    "At this point eq.3 can be formulated as a constrained optimization problem in a Lagrangian formalism:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "L(\\pi, \\lambda) =  \\mathbb{E}_{s \\sim d^{\\pi}(s)} \\mathbb{E}_{a \\sim \\pi(a|s)} [A^{\\pi_\\beta}(s, a)] + \\lambda \\left( \\epsilon -  D(\\pi(\\cdot|s) || \\pi_\\beta(\\cdot|s)) \\right)\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "and this can be maximized easily so after some algebra we find that the optimal policy, $\\pi^*(a|s)$, is given by:\n",
    "\n",
    "$\n",
    "\\pi^*(a|s) = \\frac{1}{Z(s)} \\pi_\\beta(a|s) \\exp\\left(\\frac{1}{\\lambda} A^{\\pi_\\beta}(s, a)\\right) \n",
    "\\tag{5}.\n",
    "$\n",
    "\n",
    "\n",
    "This is what we expected from our previous intuition of eq.3. What eq.5 says is that (up to a normalizazion constant $\\frac{1}{Z(s)}$) given an state $s$ the optimal policy $\\pi^*(a|s) $ gives a **probability to find an actions $a$ that is proportional to the probability that this action belongs to the dataset ($ \\pi_\\beta(a|s)$) times a factor $\\exp\\left(\\frac{1}{\\lambda} A^{\\pi_\\beta}(s, a)\\right)$ that grows exponential with $A^{\\pi_\\beta}(s, a)$, proportional to the cumulative reward on the dataset, collected from $(s,a)$ (see fig.X). \n",
    "\n",
    "\n",
    "So far this is a theoretical approach but what we can do now to compute $\\pi^*(a|s) $ is, as we do usually, to approximate it by a DNN, $\\pi_\\theta$. To keep our DNN close to the theoretical solution, $\\pi^*$, we can impose that $\\pi_\\theta$ and $\\pi^*$ be close distributions on the dataset, i.e.\n",
    "\n",
    "\n",
    "$$\n",
    "\\pi_\\theta (a|s) = argmin_{\\pi_\\theta} \\mathbb{E}_{s \\sim d\\pi_\\beta(s)} \\left[ D_{KL}(\\pi^*(\\cdot|s) \\, \\Vert \\, \\pi_\\theta(\\cdot|s)) \\right] = \\\\\n",
    "\\arg\\max_{\\pi_\\theta} \\mathbb{E}_{s\\sim d\\pi_\\beta(s)}\\mathbb{E}_{a\\sim\\pi_\\beta(a|s)} \\left[ \\frac{1}{Z(s)} \\log \\pi_\\theta(a|s) \\exp\\left(\\frac{1}{\\lambda} A^{\\pi_\\beta}(s, a)\\right) \\right]\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "\n",
    "In eq.6 we just used the definition of the $D_{KL}$ divergence. Note also that we used the forward $D_{KL}$ divergence, as it is usually done in an stochastical variational inference approach. This trick allow us to get an expectation value on the behavioral policy, that we can now approximate with a sampling on points on our collected \n",
    "dataset, **getting rid of the behavioral policy**, i.e.:\n",
    "\n",
    "$$\n",
    "\\pi_\\theta (a|s) =\n",
    "\\arg\\max_{\\pi_\\theta} \\mathbb{\\sum}_{(s,a)\\sim D} \\left[ \\frac{1}{Z(s)} \\log \\pi_\\theta(a|s) \\exp\\left(\\frac{1}{\\lambda} A^{D}(s, a)\\right) \\right]\n",
    "\\label{AWR}\n",
    "\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy constraint methods are powerful, but they can be often too pessimistic, which is always undesirable. For instance, if we know that a certain state has all actions with zero reward, we should not care about constraining the policy in this state once it can inadvertently affect our neural network approximator while forcing the learned policy to be close to the behavior policy in this irrelevant state. We effectively limit how good of a policy we can learn from our dataset by being too pessimistic. This is something that will happen with the methods presented before when we constrain the learn policy through their probabilites or support. There is an alternative approach to avoid going o.o.d without constrained the policies directly, but instead controlling the o.o.d directly from a Q function perspective and we will explore them in the next section.\n",
    "\n",
    "Also, as we use function approximation on these methods this could produce some issues for instance when we fit an unimodal policy into multimodal data. In that case, policy constraint methods can fail dramatically. However this is not a major issues as we will deal typically with some sort of DNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Policy Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Regularization is an alternative approach to ensuring the robustness of learned value functions, specifically Q-functions. **This approach involves regularizing the value function directly, aiming to prevent overestimation, especially for actions that fall outside the distribution seen during training**.\n",
    "\n",
    "It's versatile, applicable to different RL methods, including actor-critic and Q-learning methods, and doesn't necessitate explicit behavior policy modeling as the previous methods.\n",
    "\n",
    "Perhaps one of the most famous examples is the CQL (Conservative Q-Learning) algorithm that introduces the following constraint as Q-value regularization:\n",
    "\n",
    "\\begin{equation}\n",
    "CCQL_0(D, \\phi) = E_{s\\sim D, a\\sim \\mu(a|s)}[Q_{\\phi}(s, a)]\\ \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<img src=\"_static/images/96_CQL_1.png\" alt=\"offline_rl_4\" width=200%>\n",
    "<div class=\"slide title\"> Fig.4: Policy regularization approach </div>\n",
    "\n",
    "As seen in the fig. above the main idea in policy regularization method is a minimal modification of the evaluation-improvement process:\n",
    "\n",
    "$\\hat{Q}^{k+1}_{\\text{CQL}} \\gets \\hbox{argmin}_\\theta \\left[ \\color{red} {\\alpha\\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\mu}[Q_\\theta(s,a)] } + \\frac{1}{2} \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q_\\theta(s,a) - \\mathcal{B}^{\\pi}Q_\\theta(s,a)\\big)^2\\Big] \\right]. \\tag{9}$\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q_\\theta^{\\hat{\\pi}_{k+1}}(s, a) \\right] \\\\\n",
    "$$\n",
    "\n",
    "The main idea is to choose a new policy $\\mu(a|s)$ that will try to find the actions a that maximize our DNN $Q_\\theta$ values while at the same time we try to minimize the Q function on the $\\theta$ parameter space. This effect will be particularly important in actions that are o.o.d. that are the ones that are overestimate in general as we saw before \n",
    "\n",
    "( ToDo: --> Explain this better and say that in-distribution data is estimate correctly through the evaluation approach that include only data from D)\n",
    "\n",
    "This is what is rigorously shown in the CQL paper where they found that the solution of eq.9 produces a lower bound for $Q(s,a)$. The policy $\\mu$ doesn't need to be proportional to $\\pi(a|s)$ but should be one that tries to always maximizes $Q(s,a)$. There are different chooices as we will se later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short review of some popular offline RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore several key algorithms that aim to address distributional shift issues within offline reinforcement learning. It's worth noting that the field of offline RL is evolving rapidly, and this list is by no means exhaustive. Many of the concepts and strategies employed by these algorithms find applications and improvements in various other approaches.\n",
    "\n",
    "A common approach followed by many algorithms in offline RL involves an actor-critic methodology. Within this framework, there is an iterative process of evaluation and improvement, characterized by:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\hat Q}^{\\pi}_{k+1} \\gets \\arg \\min_Q \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q(s,a) - \\mathcal{B}^{\\pi}_k Q(s,a)\\big)^2\\Big].\n",
    "\\tag{Evaluation}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{B}^{\\pi}Q = r + {\\gamma}\\mathbb{E}_{s' \\sim D, a' \\sim \\pi}Q(s',a') \n",
    "\\tag{Bellman backup op.}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}}_{k+1}(s, a) \\right] \\tag{Improvement}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "So the main idea is to modify the Evaluation/Improvement steps to improve the distributional shift problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Constrained deep Q-learning (BCQ) algorithm\n",
    "\n",
    "The main idea is pictures in the figure below.\n",
    "\n",
    "<img src=\"_static/images/97_BCQ_algo_1.png\" alt=\"offline_rl_4\" width=200%>\n",
    "<div class=\"slide title\"> Fig.5: BCQ approach to offline RL </div>\n",
    "\n",
    "\n",
    "In this methods the policies $\\pi$ and $\\pi_\\beta$ are not constrained through the $D_{KL}$ divergence but still you constraint $\\pi(s)$ to generate similar actions than $\\pi_\\beta(s)$ thorugh a generative mode, in this case a VAE. So this method belongs to the direct policy constraint introduced before.\n",
    "\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_{a_i} Q_\\theta(s, a_i),\n",
    "\\\\ \\{a_i \\sim G_\\omega(s)\\}_{i=1}^n\n",
    "\\tag{10}\n",
    "$$\n",
    "\n",
    "\n",
    "In order to compute the Q-values the BCQ algorithm uses a clipped Double Deep Q-Learning (clipped-DDQ):\n",
    "\n",
    "$$\n",
    "L(\\theta_i, D) = \\mathbb{E}_{ s,a,r,s' \\sim D} \\left[  Q_{\\theta_i}(s,a) - y(r,s') \\right]\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "y(r,s') = r + \\gamma min_{i=1,2} Q_{\\theta_i, targ} (s', a'(s'))\n",
    "$$\n",
    "\n",
    "where the minimum is taken to avoid overestimation of Q-values, a issue that also happens on these kind of methods in online RL. In offline RL as we saw the o.o.d. actions are the ones that will produce typically such overstimations so clipped-DDQ somehow also introduces a control on this issue at the Q-value level, a similar effect that policy regularization methods try to achieve with a lower bound on Q-values.\n",
    "\n",
    "\n",
    "**A few technical details**:\n",
    "\n",
    "The action in eq.10 are clipped with some noise $\\epsilon$ (from here the name clipped) as this also helps to avoid overestimation of Q-values:\n",
    "\n",
    "$$\n",
    "a \\rightarrow clip [a + clip(\\epsilon, -c, c), a_{low}, a_{high}]\n",
    "$$\n",
    "\n",
    "as we allow actions having high Q-values to get some uncertainty in order to make the algorithm explore anyway regions of lower reward to avoid the overestimation effect. \n",
    "\n",
    "\n",
    "Finally, as run a VAE during training is a bit expensive the algorithm introduces a perturbation model $\\xi_\\phi(s, a_i, \\Phi)$, which outputs an adjustment to an action a in the range $[−\\Phi, \\Phi]$. So eq.10 becomes:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_{a_i + \\xi\\phi(s, a_i, \\Phi)} Q_\\theta(s, a_i + \\xi_\\phi(s, a_i, \\Phi)),\n",
    "\\\\ \\{a_i \\sim G_\\omega(s)\\}_{i=1}^n\n",
    "$$\n",
    "\n",
    "\n",
    "Note that if $\\Phi=0$ and $n=1$ the policy will resemble behavioral cloning (TODO: WHY?????????).\n",
    "On the opposite side if d $\\Phi \\rightarrow a_{max} - a_{min}$ and $n \\rightarrow \\infty$, then the algorithm approaches Q-learning, as the policy begins to greedily maximize the value function over the entire action space.\n",
    "\n",
    "**Pros**: As it learns how to generate new actions not include in the dataset\n",
    "it is suitable for small datasets and for unbalanced sets where a few unrepresented actions\n",
    "could be important for the task to be solved.\n",
    "\n",
    "**cons**: As BCQ generated action from a VAE, if the dataset used to train it underrepresents some important actions it could be that the VAE is not able to generate meaningful actions around that state and so the discovery of new or unconventional actions could be hard. This is one of the limitation of constrained policy approaches!\n",
    "\n",
    "\n",
    "Let's give a look to Tianshou BCQ policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conservative Q-Learning (CQL) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CQL follows a pessimistic approach by considering a lower bound of the Q-value. In the paper they show that the solution of:\n",
    "\n",
    "$\\hat{Q}^{k+1}_{\\text{CQL}} \\gets \\hbox{argmin}_Q \\left[ \\color{red} {\\alpha\\big(\\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\mu}[Q(s,a)] - \\mathbb{E}_{s,a \\sim \\mathcal{D}}[Q(s,a)]\\big)} + \\frac{1}{2} \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q(s,a) - \\mathcal{B}^{\\pi}Q(s,a)\\big)^2\\Big] \\right].$\n",
    "\n",
    "for $\\mu = \\pi$ is a lower bound for the Q value.\n",
    "\n",
    "The nice thing about this method is that it can be applied to any Actor Critic method in a few lines of code.\n",
    "\n",
    "CQL Focuses on **conservative value estimation** to provide lower bounds on the expected return of a policy. Aims to reduce overestimation bias and ensure that the policy remains within a safe region of the state-action space. Achieves safe exploration by constructing action sets that cover a broader range of state-action pairs. Well-suited for scenarios where safety is a top priority, as it **reduces the risk of catastrophic actions**.\n",
    "\n",
    "Note that BCQ could be better to discover novel actions and to use the collected data more efficiently but may not guarantee complete safety!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit Q-Learning (IQL) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another clever idea to avoid going out of distribution. Let's revisit the ideas for evaluation improvement, assuming that we only operate with state-action pairs from the dataset in a SARSA-style approach, i.e.:\n",
    "\n",
    "$$\n",
    "{\\hat Q}_{k+1} \\leftarrow \\arg \\min_Q \\mathbb{E}_{(s,a,s',a')\\sim D} \\left[\\left( Q(s, a) -  r(s, a) + \\gamma{\\hat Q}_k(s', a')  \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}_{k+1}}(s, a)  \\right] \\\\\n",
    "$$\n",
    "\n",
    "This is indeed a valid approach. It's important to note that running the evaluation-improvement loop makes sense only once. During evaluation, we compute the $Q$-values of the behavior policy and derive the optimal policy based on those $Q$-values in the improvement step. Further iterations would be futile since we are limited to the fixed dataset.\n",
    "\n",
    "However, this idea often falls short in finding an optimal policy for many real-world problems. Intuitively, if your data is suboptimal, the Q-values derived from that data will also be suboptimal.\n",
    "\n",
    "The core principle of IQL is to utilize a pessimistic Q-value lower bound during evaluation, similar to policy regularization, while also ensuring consistency with in-distribution data. This strategy enables a multi-step process, facilitating multiple evaluation-improvement iterations. With each iteration, a new estimate for Q(s,a) is derived, encouraging a deeper exploration of the Q-functions and enabling the capture of broader correlations.\n",
    "\n",
    "<img src=\"_static/images/96_one_step_vs_multiple_steps.png\" alt=\"offline_rl_4\" width=80%>\n",
    "<div class=\"slide title\"> Fig.6: one vs multiple step approaches.  </div>\n",
    "\n",
    "\n",
    "These are the main steps involved in the IQL approach:\n",
    "\n",
    "$$L_V(\\psi) = E_{(s,a)\\sim D}[L_2^{\\tau}(Q_{\\hat{\\theta}}(s, a) - V_{\\psi}(s))]$$\n",
    "\n",
    "$$L_Q(\\theta) = E_{(s,a,s') \\sim D}\\left[(r(s, a) + \\gamma V_{\\psi}(s') - Q_{\\theta}(s, a))^2\\right]$$\n",
    "\n",
    "and for the policy improvement step, it uses an advantage weighted regression:\n",
    "\n",
    "$$L_\\pi(\\phi) = E_{(s,a)\\sim D} \\left[\\exp(\\beta(Q_{\\hat{\\theta}}(s, a) - V_{\\psi}(s))) \\log \\pi_{\\phi}(a|s)\\right]\n",
    "$$\n",
    "\n",
    "similar to eq.7 . The lower bound used here is the 'expectile' shown in the figure below.\n",
    "\n",
    "\n",
    "<img src=\"_static/images/96_expectile.png\" alt=\"offline_rl_4\" width=80%>\n",
    "<div class=\"slide title\"> Fig.7: Expectile of a two dimenstional random variable.  </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Transformer\n",
    "\n",
    "ToDo: Add theory and show results on blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Schulman et al. 2017 - Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477.pdf)\n",
    "\n",
    "[Kumar et al. 2020 - Conservative Q-Learning for Offline Reinforcement Learning](https://arxiv.org/pdf/2006.04779.pdf)\n",
    "\n",
    "[ Levine et al. 2021 - Offline Reinforcement Learning: Tutorial, Review,\n",
    "and Perspectives on Open Problems ](https://arxiv.org/pdf/2005.01643.pdf)\n",
    "\n",
    "[Peng et al. 2019 - Simple and Scalable Off-Policy Reinforcement Learning](https://arxiv.org/abs/1910.00177)\n",
    "\n",
    "[Nair et al. '2020 - AWAC: Accelerating Online Reinforcement Learning with Offline Datasets](https://arxiv.org/abs/2006.09359)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo LIST: \n",
    "\n",
    "1 - Is IQL added to tianshou??\n",
    "\n",
    "2 - Maybe we could give an exercise about DKL ....\n",
    "\n",
    "ToDo: see Bootstrapping Error Accumulation Reduction (BEAR) and why is not the state of the art ?\n",
    "\n",
    "ToDo: Include the simple exercise in figure above 3?\n",
    "\n",
    "Page 22 Levine: example of support constrain in discrete space...and discussion there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
