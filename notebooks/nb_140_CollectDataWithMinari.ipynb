{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import warnings\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "import gymnasium as gym\n",
    "from training_rl.offline_rl.utils import one_hot_to_integer\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.simple_grid import Custom2DGridEnv\n",
    "import numpy as np\n",
    "from training_rl.offline_rl.scripts.visualizations.utils import snapshot_env\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig, InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv, RenderMode\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from random import random\n",
    "import minari\n",
    "from minari import StepDataCallback, DataCollectorV0\n",
    "from training_rl.offline_rl.utils import delete_minari_data_if_exists\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.utils import generate_compatible_minari_dataset_name\n",
    "from training_rl.offline_rl.scripts.visualizations.utils import get_state_action_data_and_policy_grid_distributions\n",
    "from training_rl.offline_rl.utils import load_buffer_minari, state_action_histogram\n",
    "\n",
    "\n",
    "load_env_variables()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# To get access to the registered environments.\n",
    "register_grid_envs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook, we will create our own suboptimal policy and generate a Minari dataset using the collected data. The primary goal is to become familiar with the Minari library and learn how to collect data in the form of (step, action, reward) using it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will design a suboptimal behavioral policy, denoted as $\\pi_b$, to guide an agent through a 2-dimensional 8x8 grid world. The agent's starting position is at (0, 0), and the objective is to reach a target located at (0, 7). During this process, we will collect data from our policy.**\n",
    "\n",
    "**Notation:**\n",
    "\n",
    "In our grid world environment, the agent's position is represented as $(x_1, x_2)$, where $x_1$ corresponds to the vertical coordinate, and $x_2 corresponds to the horizontal coordinate. Additionally, we've placed obstacles within the environment to increase the task's complexity.\n",
    "\n",
    "Observations are represented as a 64-dimensional vector since we are working with an 8x8 grid. All vector elements are set to zero, except for the position where the agent is located, which is set to one.\n",
    "\n",
    "The action is represented by an integer in the range of [0, 1, 2, 3], each indicating a direction:\n",
    "\n",
    "    0: (-1, 0) - UP\n",
    "    1: (1, 0) - DOWN\n",
    "    2: (0, -1) - LEFT\n",
    "    3: (0, 1) - RIGHT\n",
    "\n",
    "Now, let's proceed to create the environment and take a closer look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have different 2-d grid environments registered.\n",
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "RENDER_MODE = RenderMode.RGB_ARRAY_LIST\n",
    "\n",
    "\n",
    "# Configure the environment initial conditions. We also have different obstacles regidterd\n",
    "# in ObstacleTypes. \n",
    "env_2d_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=ObstacleTypes.verical_object_8x8,\n",
    "    initial_state=(0, 0),\n",
    "    target_state=(0, 7),\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(\n",
    "    gym.make(ENV_NAME, render_mode=RENDER_MODE),\n",
    "    env_config=env_2d_grid_initial_config\n",
    ")\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a suboptimal behavioral policy, $\\pi_b$:\n",
    "\n",
    "    def behavior_suboptimal_policy(state: np.ndarray, env: \n",
    "                                                     Custom2DGridEnv) -> int:\n",
    "\n",
    "        state_index = one_hot_to_integer(state)\n",
    "        state_xy = env.to_xy(state_index)\n",
    "\n",
    "        ...\n",
    "\n",
    "        return action\n",
    "        \n",
    "(Note: In practical scenarios, the environment would not be supplied to the policy. We include it here for simplicity.)\n",
    "\n",
    "This policy should meet the following criteria:\n",
    "\n",
    "1 - Guide the agent from its initial position at (0, 0) to a target located at (0, 7).\n",
    "\n",
    "2 - Incorporate stochastic movement with a restriction of not traversing more than 5 cells \n",
    "    vertically while introducing a bias towards the right direction to make it suboptimal.\n",
    "    \n",
    "Hint: To ensure that everything is working correctly, you can visualize the policy using the **behavior_policy_rendering(...)** function:\n",
    "\n",
    "    behavior_policy_rendering(\n",
    "        env_or_env_name=ENV_NAME,\n",
    "        render_mode=RENDER_MODE,\n",
    "        behavior_policy= behavior_suboptimal_policy,\n",
    "        env_2d_grid_initial_config=env_2d_grid_initial_config,\n",
    "        num_frames=1000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavior_suboptimal_policy(state: np.ndarray, env: Custom2DGridEnv) -> int:\n",
    "\n",
    "    state_index = one_hot_to_integer(state)\n",
    "    state_xy = env.to_xy(state_index)\n",
    "\n",
    "    possible_directions = [0, 1, 2, 3]\n",
    "    weights = [1, 1, 1, 2]\n",
    "\n",
    "    if state_xy[0] == 4:\n",
    "        weights = [2, 1, 2]\n",
    "        possible_directions = [0, 2, 3]\n",
    "\n",
    "    random_directions = random.choices(possible_directions, weights=weights)[0]\n",
    "\n",
    "    return random_directions\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RENDER_MODE,\n",
    "    policy_model= behavior_suboptimal_policy,\n",
    "    env_2d_grid_initial_config=env_2d_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using our suboptimal policy, we will proceed to collect data and generate a custom Minari dataset.\n",
    "\n",
    "But first, let's give a look to the [Minari documentation](https://minari.farama.org/main/content/basic_usage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your own Minari dataset using the behavior_suboptimal_policy.\n",
    "\n",
    "Hint: Remember to use the DataCollectorV0 wrapper provided in the Minari library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER = \"_rl_workshop\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "\n",
    "DATA_SET_NAME += DATA_SET_IDENTIFIER\n",
    "minari_dataset_name = generate_compatible_minari_dataset_name(ENV_NAME, DATA_SET_NAME, VERSION_DATA_SET)\n",
    "\n",
    "delete_minari_data_if_exists(minari_dataset_name)\n",
    "\n",
    "# Usefull for data preprocessing\n",
    "class CustomSubsetStepDataCallback(StepDataCallback):\n",
    "    def __call__(self, env, **kwargs):\n",
    "        step_data = super().__call__(env, **kwargs)\n",
    "        #del step_data[\"observations\"][\"achieved_goal\"]\n",
    "        return step_data\n",
    "\n",
    "env = DataCollectorV0(\n",
    "    env,\n",
    "    step_data_callback=CustomSubsetStepDataCallback,\n",
    "    record_infos=False,\n",
    ")\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "num_steps = 0\n",
    "for _ in range(BUFFER_SIZE):\n",
    "\n",
    "    action = behavior_suboptimal_policy(state, env)\n",
    "\n",
    "    next_state, reward, done, time_out, info = env.step(action)\n",
    "    num_steps += 1\n",
    "\n",
    "    if done or time_out:\n",
    "        state, _ = env.reset()\n",
    "        num_steps=0\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "dataset = minari.create_dataset_from_collector_env(dataset_id=minari_dataset_name, collector_env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the state-action distribution data.\n",
    "\n",
    "First, let's upload the Minari dataset in a Tainshou ReplyBuffer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = \"Grid_2D_8x8_discrete-data_rl_workshop-v0\"\n",
    "data = load_buffer_minari(NAME_EXPERT_DATA)\n",
    "\n",
    "state_action_count_data, _ = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(data, env)\n",
    "\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\")\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution makes sense. From states 40 to 64 (i.e., (5,0) to (7,7)), our policy doesn't collect any data, neither around the obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[ \\[Fu.Justin et. al.\\] D4RL: Datasets for Deep Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "\n",
    "[ MINARI: A dataset API for Offline Reinforcement Learning ](https://minari.farama.org/main/content/basic_usage/) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou_dev",
   "language": "python",
   "name": "tianshou_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
