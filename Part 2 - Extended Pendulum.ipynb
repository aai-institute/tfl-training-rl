{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extended pendulum\n",
    "\n",
    "In this notebook we will dive a bit deeper into RL by training agents on the pendulum environment\n",
    "that we already encountered before and analyze the results.\n",
    "We will be concerned with questions like reward shaping, stability of results,\n",
    "generalization to non-training situations and other issues related to real-world applications of RL.\n",
    "\n",
    "Here we will only look at model-free RL since model-based RL will often require some domain specific\n",
    "algorithms and engineering. Also, the openly available tools for model-based RL are far less mature\n",
    "than for model-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Only needed on colab or on a fresh setup\n",
    "# Won't work on Windows!\n",
    "!pip install -r requirements.txt\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# rendering envs directly in notebook is recommended only on colab\n",
    "in_notebook = \"google.colab\" in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import torch\n",
    "\n",
    "from gym.envs.classic_control import PendulumEnv\n",
    "from gym.wrappers import TimeLimit\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from visualization import demo_model\n",
    "\n",
    "# You might want to switch to a CUDA-enabled runtime for executing this notebook\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The vanilla pendulum\n",
    "\n",
    "Let us start by simply using gym's pendulum as is and training a soft actor critic (an off-policy algorithm) on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def main(\n",
    "    experiment_config: ExperimentConfig,\n",
    "    task: str = \"Ant-v3\",\n",
    "    buffer_size: int = 1000000,\n",
    "    hidden_sizes: Sequence[int] = (256, 256),\n",
    "    actor_lr: float = 1e-3,\n",
    "    critic_lr: float = 1e-3,\n",
    "    gamma: float = 0.99,\n",
    "    tau: float = 0.005,\n",
    "    alpha: float = 0.2,\n",
    "    auto_alpha: bool = False,\n",
    "    alpha_lr: float = 3e-4,\n",
    "    start_timesteps: int = 10000,\n",
    "    epoch: int = 200,\n",
    "    step_per_epoch: int = 5000,\n",
    "    step_per_collect: int = 1,\n",
    "    update_per_step: int = 1,\n",
    "    n_step: int = 1,\n",
    "    batch_size: int = 256,\n",
    "    training_num: int = 1,\n",
    "    test_num: int = 10,\n",
    "):\n",
    "    now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "    log_name = os.path.join(task, \"sac\", str(experiment_config.seed), now)\n",
    "\n",
    "    sampling_config = SamplingConfig(\n",
    "        num_epochs=epoch,\n",
    "        step_per_epoch=step_per_epoch,\n",
    "        num_train_envs=training_num,\n",
    "        num_test_envs=test_num,\n",
    "        buffer_size=buffer_size,\n",
    "        batch_size=batch_size,\n",
    "        step_per_collect=step_per_collect,\n",
    "        update_per_step=update_per_step,\n",
    "        start_timesteps=start_timesteps,\n",
    "        start_timesteps_random=True,\n",
    "    )\n",
    "\n",
    "    env_factory = MujocoEnvFactory(task, experiment_config.seed, sampling_config, obs_norm=False)\n",
    "\n",
    "    experiment = (\n",
    "        SACExperimentBuilder(env_factory, experiment_config, sampling_config)\n",
    "        .with_sac_params(\n",
    "            SACParams(\n",
    "                tau=tau,\n",
    "                gamma=gamma,\n",
    "                alpha=AutoAlphaFactoryDefault(lr=alpha_lr) if auto_alpha else alpha,\n",
    "                estimation_step=n_step,\n",
    "                actor_lr=actor_lr,\n",
    "                critic1_lr=critic_lr,\n",
    "                critic2_lr=critic_lr,\n",
    "            ),\n",
    "        )\n",
    "        .with_actor_factory_default(\n",
    "            hidden_sizes,\n",
    "            continuous_unbounded=True,\n",
    "            continuous_conditioned_sigma=True,\n",
    "        )\n",
    "        .with_common_critic_factory_default(hidden_sizes)\n",
    "        .build()\n",
    "    )\n",
    "    experiment.run(log_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = TimeLimit(PendulumEnv(), max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = SAC(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"logs\")\n",
    "model_name = model.__class__.__name__.lower() + \"_vanilla_pendulum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let us have a quick glance at the parameters of this off-policy model\n",
    "from inspect import getfullargspec\n",
    "\n",
    "getfullargspec(SAC).annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We will now train the model, you can view the training progress in tensorboard\n",
    "# This takes about 10 minutes on a laptop with a GPU\n",
    "model.learn(total_timesteps=12000, log_interval=4, tb_log_name=model_name)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we load the saved model and look at how it performs on the environment\n",
    "model = SAC.load(model_name)\n",
    "demo_model(env, model, num_steps=800, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "As you see, the reward is not easily interpretable. In fact, it is composed of different quantities (do have a look at the source\n",
    "code of the environment, it is also provided below). How would you go about evaluating an agent's performance? Think about an\n",
    "evaluation strategy and put it into code. It might contain the average time needed until the pendulum is stabilized, the average\n",
    "torque per unit time, the average angular distance travelled by the pendulum or other metrics you find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sparse Rewards\n",
    "\n",
    "With the vanilla pendulum, the agent receives rewards continuously based on the angle. The reward also\n",
    "contains information about the torque. What if we tried training with sparse rewards, where we motivate\n",
    "the agent to move a pendulum to a certain angle-range as fast as possible and to leave it there?\n",
    "\n",
    "Since gym environments are not modular, we cannot easily modify the reward. We could follow the strategies\n",
    "outlined before to change the reward (and in a real project this would be the way to go). However, for\n",
    "educational purposes we will instead copy-paste gym's source code and simply modify it according to our needs\n",
    "prior to each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Copy of gym's pendulum that we will modify\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomPendulumEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 30}\n",
    "\n",
    "    def __init__(self, g=10.0, target_angle_range=(-np.pi / 5, np.pi / 5)):\n",
    "        self.target_angle_range = target_angle_range\n",
    "        self.max_speed = 8\n",
    "        self.max_torque = 2.0\n",
    "        self.dt = 0.05\n",
    "        self.g = g\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "        self.viewer = None\n",
    "\n",
    "        high = np.array([1.0, 1.0, self.max_speed], dtype=np.float32)\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-self.max_torque, high=self.max_torque, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, u):\n",
    "        th, thdot = self.state  # th := theta\n",
    "\n",
    "        g = self.g\n",
    "        m = self.m\n",
    "        l = self.l\n",
    "        dt = self.dt\n",
    "\n",
    "        u = np.clip(u, -self.max_torque, self.max_torque)[0]\n",
    "        self.last_u = u  # for rendering\n",
    "        # costs = angle_normalize(th) ** 2 + 0.1 * thdot ** 2 + 0.001 * (u ** 2)\n",
    "\n",
    "        min_th, max_th = self.target_angle_range\n",
    "        angle_cost = 0 if min_th < angle_normalize(th) < max_th else 1\n",
    "        costs = angle_cost\n",
    "\n",
    "        newthdot = (\n",
    "            thdot\n",
    "            + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3.0 / (m * l ** 2) * u) * dt\n",
    "        )\n",
    "        newth = th + newthdot * dt\n",
    "        newthdot = np.clip(newthdot, -self.max_speed, self.max_speed)\n",
    "\n",
    "        self.state = np.array([newth, newthdot])\n",
    "        return self._get_obs(), -costs, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        high = np.array([np.pi, 1])\n",
    "        self.state = self.np_random.uniform(low=-high, high=high)\n",
    "        self.last_u = None\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, thetadot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), thetadot])\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(500, 500)\n",
    "            self.viewer.set_bounds(-2.2, 2.2, -2.2, 2.2)\n",
    "            rod = rendering.make_capsule(1, 0.2)\n",
    "            rod.set_color(0.8, 0.3, 0.3)\n",
    "            self.pole_transform = rendering.Transform()\n",
    "            rod.add_attr(self.pole_transform)\n",
    "            self.viewer.add_geom(rod)\n",
    "            axle = rendering.make_circle(0.05)\n",
    "            axle.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(axle)\n",
    "            fname = \"clockwise.png\"\n",
    "            self.img = rendering.Image(fname, 1.0, 1.0)\n",
    "            self.imgtrans = rendering.Transform()\n",
    "            self.img.add_attr(self.imgtrans)\n",
    "\n",
    "        self.viewer.add_onetime(self.img)\n",
    "        self.pole_transform.set_rotation(self.state[0] + np.pi / 2)\n",
    "        if self.last_u:\n",
    "            self.imgtrans.scale = (-self.last_u / 2, np.abs(self.last_u) / 2)\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "\n",
    "\n",
    "def angle_normalize(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparse_env = TimeLimit(CustomPendulumEnv(), max_episode_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = SAC(\"MlpPolicy\", sparse_env, verbose=1, tensorboard_log=\"logs\")\n",
    "\n",
    "# This takes about 10 minutes on a laptop with a GPU\n",
    "model_name = model.__class__.__name__.lower() + \"_sparse_pendulum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=12000, log_interval=4, tb_log_name=model_name)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = SAC.load(model_name)\n",
    "demo_model(sparse_env, model, num_steps=800, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transferring to perturbed environments\n",
    "\n",
    "The environment assumes a fixed mass. What if were to apply the same agent on an env with a different mass?\n",
    "Note that planning algorithms a la MPC would have no problem with this at all, their performance would not\n",
    "go down as long as mass is included in the dynamics-model.\n",
    "\n",
    "Not so for the \"real RL\" agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env.env.m = 0.2\n",
    "\n",
    "demo_model(env, model, num_steps=400, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pendulum is balanced upright but an excessive amount of torque is being applied constantly.\n",
    "How could we improve this situation?\n",
    "\n",
    "We will try the following: we randomize the pendulum's mass at reset of episodes and also add mass to the observations.\n",
    "For that we will again follow the *bad-practice* and modify the environment by overriding methods directly.\n",
    "Don't do this in a real project! Part of the reason for doing it here is to highlight how cumbersome and fragile such\n",
    "a software design becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class VariableMassPendulum(PendulumEnv):\n",
    "    def __init__(self, mass_range=(0.1, 1.5)):\n",
    "        super().__init__()\n",
    "        m_min, m_max = mass_range\n",
    "        high = np.array([1.0, 1.0, self.max_speed, m_max], dtype=np.float32)\n",
    "        low = -high\n",
    "        low[-1] = m_min\n",
    "        self.observation_space = spaces.Box(low=-high, high=high, dtype=np.float32)\n",
    "\n",
    "        self.mass_range = mass_range\n",
    "        self.m = np.random.uniform(*mass_range)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = list(super()._get_obs())\n",
    "        obs.append(self.m)\n",
    "        return np.array(obs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.m = np.random.uniform(*self.mass_range)\n",
    "        return super().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "var_mass_env = TimeLimit(VariableMassPendulum(), max_episode_steps=100)\n",
    "model = SAC(\"MlpPolicy\", var_mass_env, verbose=1, tensorboard_log=\"logs\")\n",
    "model_name = model.__class__.__name__.lower() + \"_var_mass_pendulum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This takes about 12 minutes on a laptop with a GPU\n",
    "model.learn(total_timesteps=15000, log_interval=4, tb_log_name=model_name)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = SAC.load(model_name)\n",
    "demo_model(var_mass_env, model, num_steps=400, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us see how this new agent behaves with low masses. We can look into that by configuring a degenerate range such\n",
    "that always the same mass is \"sampled\" at reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "var_mass_env.env.mass_range = (0.3, 0.3)\n",
    "demo_model(var_mass_env, model, num_steps=400, in_notebook=in_notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "There are many possibilities to extend the experiments done above. You could try:\n",
    "\n",
    "    1. Also changing l and g and adding them to the observation.\n",
    "    2. Normalizing all observations to lie within 0 and 1 (or at least between -1 and 1)\n",
    "    3. What if we could not observe the angular velocity? Remove the velocity from the observation.\n",
    "       This renders the decision process non-Markovian and partially observed.\n",
    "       However, adding a single past observation is sufficient to restore the Markov property.\n",
    "       Add a history of previous observations and actions to the environment. You can use\n",
    "       gym's `FrameStack` and `FlattenObservation` wrappers for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
