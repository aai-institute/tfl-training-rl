{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from examples.offline_RL_workshop import load_env_variables\n",
    "import minari\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from examples.offline_RL_workshop.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from examples.offline_RL_workshop.custom_envs.utils import Grid2DInitialConfig\n",
    "from examples.offline_RL_workshop.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_minari_datasets, MinariDatasetConfig\n",
    "from examples.offline_RL_workshop.generate_custom_minari_datasets.utils import generate_compatible_minari_dataset_name, \\\n",
    "    get_dataset_name_2D_grid\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import CustomEnv\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import RenderMode\n",
    "from examples.offline_RL_workshop.offline_policies.policy_registry import PolicyName\n",
    "from examples.offline_RL_workshop.offline_trainings.offline_training import offline_training\n",
    "from examples.offline_RL_workshop.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from examples.offline_RL_workshop.utils import state_action_histogram\n",
    "from examples.offline_RL_workshop.visualizations.utils import get_state_action_data_and_policy_grid_distributions\n",
    "from examples.offline_RL_workshop.custom_envs.utils import InitialConfigCustom2DGridEnvWrapper\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import gymnasium as gym\n",
    "from examples.offline.utils import load_buffer_minari\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_rendering import render_rgb_frames, snapshot_env\n",
    "import cv2\n",
    "import torch\n",
    "from examples.offline_RL_workshop.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "import os\n",
    "from examples.offline_RL_workshop.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from examples.offline_RL_workshop.utils import compare_state_action_histograms\n",
    "from tianshou.data import Collector\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_rendering import behavior_policy_rendering\n",
    "from examples.offline_RL_workshop.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from examples.offline_RL_workshop.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import create_minari_config_from_dict\n",
    "from examples.offline_RL_workshop.utils import delete_minari_data_if_exists\n",
    "from minari import combine_datasets\n",
    "from copy import copy\n",
    "from examples.offline_RL_workshop.load_env_variables import load_env_variables\n",
    "\n",
    "load_env_variables()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ToDo: this should be load automatically\n",
    "register_grid_envs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook we will deal with another importat property that we should fulfill with a robust offline RL algorithm, the stiching, i.e. the reuse of different trajectories contain in the data to obtain the best trajecroy in our dataset.**\n",
    "\n",
    "\n",
    "The goal will be to reach a target at (7,7) starting from (0,0). We will use again the 8x8 grid environment. Our dataset contains trajectories covering our space of interest but generated for different tasks. One is a suboptimal policy that moves the agent from (0,0) to (7,0) and the other is a determinitic an optimal one (human expert) that brings the agent from (4,0) into (7,7). We have obviously much more data coming from the suboptial policy than the expert one as it is cheaper.\n",
    "\n",
    "So we will create the two policies:\n",
    "\n",
    "I  - **Suboptimal expert policy**:  moves agent in suboptimal way downwards starting from (0,0) (collect 8000 steps)\n",
    "\n",
    "II - **Optimal expert poilcy**: moves agent in the optimal path from (4,0) to (7,7) (collect 300 steps) \n",
    "\n",
    "\n",
    "In this example we will use again as off-policy RL algorithm, the Deep Q-Network (DQN) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our configuration and create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip\n",
    "NUM_STEPS_I = 3000\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "NUM_STEPS_II = 300\n",
    "\n",
    "OFFLINE_POLICY = PolicyName.dqn\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.verical_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to our environment and both policies configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "behavior_policy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_I,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert policy\n",
    "behavior_policy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE:\n",
    "\n",
    "Create both minari datasets and combine them into a single one. This is what you will need to do in realistic situations!\n",
    "\n",
    "To combine the datasets you will need to use:\n",
    "\n",
    "combined_dataset = combine_datasets(\n",
    "    minari_datasets, new_dataset_id=name_combined_dataset\n",
    ")\n",
    "\n",
    "with minari_datasets a list with your two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER_I = \"_move_downwards\"\n",
    "DATA_SET_IDENTIFIER_II = \"_move_deterministic\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "\n",
    "# Create metadata config for set I\n",
    "minari_dataset_config_I = create_minari_config_from_dict(\n",
    "env_name=ENV_NAME,\n",
    "dataset_name=DATA_SET_NAME,\n",
    "data_set_identifier=DATA_SET_IDENTIFIER_I,\n",
    "version_dataset=VERSION_DATA_SET,\n",
    "num_steps=NUM_STEPS_I,\n",
    "behavior_policy_name=BEHAVIOR_POLICY_I,\n",
    "env_2d_grid_initial_config=env_2D_grid_initial_config\n",
    ")\n",
    "\n",
    "# Create metadata config for set II\n",
    "minari_dataset_config_II = create_minari_config_from_dict(\n",
    "env_name=ENV_NAME,\n",
    "dataset_name=DATA_SET_NAME,\n",
    "data_set_identifier=DATA_SET_IDENTIFIER_II,\n",
    "version_dataset=VERSION_DATA_SET,\n",
    "num_steps=NUM_STEPS_II,\n",
    "behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "env_2d_grid_initial_config=env_2D_grid_initial_config\n",
    ")\n",
    "\n",
    "create_minari_datasets(minari_dataset_config_I)\n",
    "create_minari_datasets(minari_dataset_config_II)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now combine both datasets in a single one as we will do in a realistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINARI_DATASET_I = \"Grid_2D_8x8_discrete-data_verical_object_8x8_start_0_0_target_7_7_move_downwards-v0\"\n",
    "MINARI_DATASET_II = \"Grid_2D_8x8_discrete-data_verical_object_8x8_start_0_0_target_7_7_move_deterministic-v0\"\n",
    "NAME_COMBINED_DATASET = \"combined_data_sets_vertical_obstacle\"\n",
    "\n",
    "name_combined_dataset = generate_compatible_minari_dataset_name(\n",
    "    env_name=ENV_NAME,\n",
    "    data_set_name=NAME_COMBINED_DATASET,\n",
    "    version=\"V0\"\n",
    ")\n",
    "\n",
    "delete_minari_data_if_exists(name_combined_dataset)\n",
    "\n",
    "list_dataset_names = [MINARI_DATASET_I, MINARI_DATASET_II]\n",
    "minari_datasets = [\n",
    "    minari.load_dataset(dataset_id) for dataset_id in list_dataset_names\n",
    "]\n",
    "\n",
    "combined_dataset = combine_datasets(\n",
    "    minari_datasets, new_dataset_id=name_combined_dataset\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of episodes in dataset I:{len(minari_datasets[0])}, in dataset I:{len(minari_datasets[1])} and  \"\n",
    "      f\"in the combined dataset: {len(combined_dataset)}\")\n",
    "\n",
    "\n",
    "# Create metadata for the combined dataset (we can reuse the metadata of set I for simplicity)\n",
    "minari_combined_dataset = MinariDatasetConfig.load_from_file(MINARI_DATASET_I)\n",
    "minari_combined_dataset.num_steps = NUM_STEPS_I + NUM_STEPS_II\n",
    "minari_combined_dataset.data_set_name = name_combined_dataset\n",
    "minari_combined_dataset.save_to_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to the state-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Buffers with minari datasets\n",
    "buffer_data = load_buffer_minari(name_combined_dataset)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", inset_pos_xy=(-0.1, -0.005))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the DQN policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = name_combined_dataset\n",
    "\n",
    "# The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.cql\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's restore the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy_final.pth\"\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config.initial_state=(0,5)\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(\n",
    "    gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), \n",
    "    env_config=env_2D_grid_initial_config#\n",
    ")\n",
    "\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to the policy state-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES_FOR_STATISTICS = 100 # as more episodes the better\n",
    "\n",
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES_FOR_STATISTICS,\n",
    "    logits_sampling=True,\n",
    ")\n",
    "\n",
    "# plots\n",
    "state_action_histogram(state_action_count_data)\n",
    "state_action_histogram(state_action_count_policy)\n",
    "compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_collector = Collector(policy, env, exploration_noise=EXPLORATION_NOISE)\n",
    "#final_collector.collect(n_episode=20, render=1 / 35)\n",
    "\n",
    "#ToDo: Sole error in DQN visualization !!!!\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "1 - DQN cannot find the optimal solution encoded in a few expert data episodes in the policy II.\n",
    "2 - DQN produces as expected a drastic reduction of the effective dataset dimension. This is fine in this simple case but as mentioned before it could be that under-represented states-actions in the dataset will appear during inference .... ????? Clarify this.\n",
    "3 - ....."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou_dev",
   "language": "python",
   "name": "tianshou_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
