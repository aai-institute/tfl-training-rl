{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imitation Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imitation learning is a supervice learning approach focuses on learning policies or behaviors by observing and imitating expert demonstrations**. Instead of learning from trial and error, imitation learning leverages existing expert knowledge to train agents. \n",
    "\n",
    "This makes this algorithms appealing as **you don't need to create a reward function for your task** like in situatons where the manual approach becomes essential because creating a reward function directly is not feasible, such as when training a self-driving vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiet imitation learning algorithm is call BC (Behavioral Cloning) and is just supervised learning on the collected expert data, i.e.:\n",
    "\n",
    "$$ D = \\{(s_0, a_0), (s_1, a_1), \\ldots, (s_T, a_T^o)\\} \\quad \\tag{Dataset} $$\n",
    "\n",
    "$$ L_{BC}(\\theta) = \\frac{1}{2} \\left(\\pi_\\theta(s_t) - a_t\\right)^2 \\tag{Cost function}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are improve versions of BC like DAgger (Dataset Aggregation) where after BC the policy is rollout and if new states appear a new feedback is ask to the human expert. This could produce a huge improvement but it could be quite expensive.\n",
    "\n",
    "Pros and cons of these models:\n",
    "\n",
    "**pros**: If you have expert dataset and you are not worry about safety (i.e. unexpected policy behavior in unknown states) this could be a a fast approach. \n",
    "\n",
    "**cons**: In general we don't have access to expert data so this is one of the main issues, but even if we have we will have problems related with distributional shift between our clone policy and the provided dataset. We will se this in a moment in an exercise. Also many of the properties of the Minari datasets (see notebook ??) that could appear in reality cannot be handled with simple imitation learning approaches, like for instance the stiching propery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other interesting methods that combine imitation learning and the offline RL methods we will introduce later. Typically, they involve two steps:\n",
    "\n",
    "1 - Modeling data distribution (Imitation learning).\n",
    "\n",
    "2 - Applying offline RL for planning.\n",
    "\n",
    "In the first step, they use more sophisticated techniques for cloning, such as Transformers to generate new trajectories or normalizing flows to fit the state-action data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[ Ross et. al. 2012 - A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning](https://arxiv.org/abs/1011.0686)\n",
    "\n",
    "[Janner et.al. 2021 - Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)\n",
    "\n",
    "[Prudencio et. al.' 2023 - A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems ](https://arxiv.org/pdf/2203.01387.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
