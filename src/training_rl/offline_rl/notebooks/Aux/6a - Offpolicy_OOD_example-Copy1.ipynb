{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T18:31:12.951579Z",
     "start_time": "2023-10-18T18:31:12.948679Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'training_rl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtraining_rl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moffline_rl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_env_variables\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mminari\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtraining_rl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moffline_rl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbehavior_policies\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbehavior_policy_registry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BehaviorPolicyType\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'training_rl'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from training_rl.offline_rl import load_env_variables\n",
    "import minari\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_minari_datasets, MinariDatasetConfig\n",
    "\n",
    "\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "\n",
    "\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from training_rl.offline_rl.utils import state_action_histogram\n",
    "from training_rl.offline_rl.custom_envs.utils import InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import register_grid_envs\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "import os\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from tianshou.data import Collector\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "\n",
    "from training_rl.offline_rl.utils import delete_minari_data_if_exists\n",
    "from minari import combine_datasets\n",
    "from copy import copy\n",
    "\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "\n",
    "load_env_variables()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ToDo: this should be load automatically\n",
    "register_grid_envs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will see what happens if we want to apply off-policy RL from a given dataset. Our policy will be a simple random walk in one dimension, that starting from (0,0) tries to reach the target at (3,7). So as we move in one dimension the the highest reward state is not present in our dataset.\n",
    "\n",
    "We will use in this example as off-policy RL policy, the Deep Q-Network (DQN) algorithm, that we introduced in the online RL section. As the DQN agent cannot interact with the environment we will feed the collected data through a ReplyBuffer, similarly as we did before in the imitation learning example.\n",
    "\n",
    "Again we will use the 8x8 grid environment but without any obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our configuration and create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T18:32:44.068445Z",
     "start_time": "2023-10-18T18:32:43.902574Z"
    }
   },
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.scripts.visualizations.utils import snapshot_env\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv, RenderMode\n",
    "\n",
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "BEHAVIOR_POLICY = BehaviorPolicyType.random\n",
    "NUM_COLLECTED_POINTS = 6000\n",
    "\n",
    "OFFLINE_POLICY = PolicyName.dqn\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.vertical_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (3, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to our environment and policy configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as we did for imitation learning before:\n",
    "\n",
    "    1 - Collect a minari dataset\n",
    "    2 - Create a reply Tianshou buffer\n",
    "    3 - train a DQN policy on the generated dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER = \"_exercise_6_a\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "\n",
    "# Create metadata config for set I\n",
    "minari_dataset_config = create_minari_config_from_dict(\n",
    "env_name=ENV_NAME,\n",
    "dataset_name=DATA_SET_NAME,\n",
    "data_set_identifier=DATA_SET_IDENTIFIER,\n",
    "version_dataset=VERSION_DATA_SET,\n",
    "num_steps=NUM_COLLECTED_POINTS,\n",
    "behavior_policy_name=BEHAVIOR_POLICY,\n",
    "env_2d_grid_initial_config=env_2D_grid_initial_config\n",
    ")\n",
    "\n",
    "create_minari_datasets(minari_dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to the state-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = \"Grid_2D_8x8_discrete-data_horizontal_line_object_8x8_start_0_0_target_3_7_exercise_6_a-v0\" \n",
    "\n",
    "#Create Buffers with minari datasets\n",
    "buffer_data = load_buffer_minari(NAME_EXPERT_DATA)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", \n",
    "                       inset_pos_xy=(0.0, -0.03))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the DQN policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = NAME_EXPERT_DATA\n",
    "\n",
    "# The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.dqn\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to the policy state-action distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2:\n",
    "\n",
    "Analyze the state-action policy distribution and try to make sense of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy_final.pth\"\n",
    "NUM_EPISODES = 10 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=False,\n",
    ")\n",
    "\n",
    "# plots\n",
    "#state_action_histogram(state_action_count_data)\n",
    "state_action_histogram(state_action_count_policy, inset_pos_xy=None)\n",
    "#compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy.set_eps(0.01)\n",
    "#final_collector = Collector(policy, env, exploration_noise=EXPLORATION_NOISE)\n",
    "#final_collector.collect(n_episode=20,q render=1 / 35)\n",
    "\n",
    "#ToDo: Sole error in DQN visualization !!!!\n",
    "\n",
    "env_2D_grid_initial_config.initial_state=(0,0)\n",
    "env_2D_grid_initial_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "1 - DQN goes totally out of distribution. It start to generate action that bring the system to states not included in our dataset. Remember that in general in off-policy algorithms :\n",
    "\n",
    "$$ J (\\phi) = \\mathbb{E}_{{s,a,s'}\\sim D} [r(s, a) + \\gamma \\mathbb{E}_{a'\\sim \\pi_{\\text{off}}(\\cdot|s)} [Q_{\\pi_{\\phi}}(s', a') - Q_{\\pi_{\\phi}}(s, a)]^2 $$\n",
    "\n",
    "$a'$ won't be in the dataset and so this will move the agent to states not observed in the data with possibly negative consequences!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl_v1",
   "language": "python",
   "name": "training_rl_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
