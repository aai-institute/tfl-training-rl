{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Source Datasets for offline RL\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of offline RL or imitation learning is to learn a policy from a fixed dataset. This approach has gained significant attention because it allows RL methods to utilize vast, pre-collected datasets, somewhat similar to how large datasets have propelled advances in supervised learning.**\n",
    "\n",
    "Challenges:\n",
    "\n",
    "- Existing benchmarks designed for online RL are not well-suited for the offline RL \n",
    "  setting.\n",
    "- RL benchmarks relying on data generated by partially-trained agents or environment-\n",
    "  dependent are not robust.\n",
    "\n",
    "These challenges makes it difficult to accurately measure progress in offline RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MINARI Dataset \n",
    "(previouly called D4RL from UC Berkeley/Google Brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library is emerging as the standard in the field. While previously reliant on D4RL, the community is currently transitioning to Minari. Its scope is to fill the gap of representative datasets in in offline RL, by introducing open-source datasets explicitly crafted for the offline setting. These datasets are shaped by the critical properties that real-world applications of offline RL demand. Minari provides datasets collected with random, medium, and expert policies in some environments, allowing us to evaluate whether an algorithm can extract meaning from noise. \n",
    "\n",
    "In particular the provided datasets focus mainly on the following properties that appear often in realistic situations:\n",
    "\n",
    "\n",
    "1 - **Narrow and biased data distributions**: e.g. from deterministic policies: Narrow datasets may arise in human demonstrations or in hand-crafted policies.\n",
    "\n",
    "2 - **Undirected and multitask data**: Undirected in the sense that is not directed towards the specific task one is trying to accomplish. E.g.: recording user interactions on the internet or recording videos of a car for autonomous driving. The main purpose it to test how good is the offline agebt to be used for \"trajectory stitching\", i.e. combining trajectories from different tasks to achieve new objectives, instead of searching for out-of-distribution trajectories.\n",
    "\n",
    "<img src=\"Images/stiching.png\" alt=\"stich_traj\" style=\"width:200px;\">\n",
    "\n",
    "3 - **Sparse rewards**: Sparse rewards are challenging in online settings due to their close correlation with exploration. In offline RL, we exclusively explore within the dataset, making it an ideal framework to study the algorithm's response to sparse rewards.\n",
    "Note that crafting effective rewards can be challenging, and overly complex rewards may inadvertently push solutions towards suboptimal outcomes. In contrast, designing sparse rewards is often more straightforward as it merely involves specifying the task's success criteria, making it an attractive property to work with.\n",
    "\n",
    "ToDo: Add a sparse reward exercise\n",
    "\n",
    "4 - **Suboptimal data**: Give a clear task the data could not contain any optimal trajectory so this is a realistic scenario in general and still the offline agent should be able to find a suboptimal solution.\n",
    "\n",
    "5 - **Non-representable behavior policies**: non-Markovian behavior policies, and partial observability. For instance, if the data is collected with a classical control algorithm that have access to a window of previous states.\n",
    "\n",
    "ToDo: USE NON-MARKOVIAN POLICY BEHAVIOR --> PD CONTROLLER\n",
    "\n",
    "6 - **Realistic domains**: Different Mujoco tasks as robot manipulation or multi-tasking.\n",
    "\n",
    "Let's give a look to [Minari](https://minari.farama.org/main/content/basic_usage/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Unplugged dataset \n",
    "(Deep Mind - Google Brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It includes a nice set of tasks, but the crucial point is that all datasets come from behavior policies trained online, so the collected data may not be representative of realistic situations where human experts and non-RL policies are typically used to collect data. Additionally, most of the data comes from medium to expert policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open X-Embodiment Repository (October 2023)\n",
    "(Partners from 33 academic labs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library introduced the **Open X-Embodiment Repository** that includes a dataset with 22 different robot types for **X-embodiment learning**, i.e. to learn from diverse and large-scale datasets from multiple robots for better transfer learning and improved generalization.\n",
    "\n",
    "[Let's give a look](https://www.deepmind.com/blog/scaling-up-learning-across-many-different-robot-types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "\n",
    "In most cases, offline RL methods typically train a policy with specific hyperparameters for a fixed number of steps. They then assess their quality by using the policy from the last iteration in an online evaluation. We will follow this approach in this workshop as it will allow us to focus on understanding concepts rather than fine-tuning optimization.\n",
    "\n",
    "However, it's worth noting that alternative methods in the realm of offline RL, known as **Offline Policy Evaluation (OPE)**, exist. These approaches aim to evaluate policy performance directly from the data without any interaction with the environment, which can be a realistic scenario. Although we won't delve into these methods in detail, it's important to acknowledge their existence. **It's worth noting that these methods are still considered somewhat unreliable, and there is ongoing progress in this area**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[ \\[Fu.Justin et. al. '2021 \\] D4RL: Datasets for Deep Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "\n",
    "[ MINARI: A dataset API for Offline Reinforcement Learning ](https://minari.farama.org/main/content/basic_usage/) \n",
    "\n",
    "[ C. Gulcehre et al. '2021, â€œRL unplugged: A suite of benchmarks for offline\n",
    "reinforcement learning](https://arxiv.org/abs/2006.13888)\n",
    "\n",
    "[ \\[A. Padalkar et. al. '2023 \\] Open X-Embodiment: Robotic Learning Datasets and RT-X Models\\ ](https://robotics-transformer-x.github.io/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
