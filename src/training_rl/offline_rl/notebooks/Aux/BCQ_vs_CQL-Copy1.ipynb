{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ivan/Documents/GIT_PROJECTS/tfl-training-rl/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: pandas in /home/ivan/anaconda3/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ivan/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ivan/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/ivan/anaconda3/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ivan/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "%%capture\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext training_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
      "Requirement already satisfied: pandas in /home/ivan/anaconda3/lib/python3.8/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ivan/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/ivan/anaconda3/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ivan/anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ivan/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mminari\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbehavior_policies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbehavior_policy_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BehaviorPolicyType\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_2d_grid_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobstacles_2D_grid_register\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ObstacleTypes\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grid2DInitialConfig\n",
      "File \u001b[0;32m~/Documents/GIT_PROJECTS/tfl-training-rl/src/training_rl/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnb_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TflWorkshopMagic\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ipython_extension\u001b[39m(ipython):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124;03m\"\"\"Defining this function here allows to load the TflWorkshopMagic extension\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    by using %load_ext training_rl\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT_PROJECTS/tfl-training-rl/src/training_rl/nb_utils.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequence\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccsr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote_storage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RemoteStorage\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmagic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Magics, line_magic, magics_class\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import warnings\n",
    "import minari\n",
    "from training_rl.offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from training_rl.offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from training_rl.offline_rl.custom_envs.utils import Grid2DInitialConfig\n",
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_minari_datasets, MinariDatasetConfig\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import CustomEnv\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import RenderMode\n",
    "from training_rl.offline_rl.offline_policies.policy_registry import PolicyName\n",
    "from training_rl.offline_rl.offline_trainings.offline_training import offline_training\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from training_rl.offline_rl.custom_envs.utils import InitialConfigCustom2DGridEnvWrapper\n",
    "from training_rl.offline_rl.custom_envs.custom_envs_registration import register_grid_envs\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from training_rl.offline_rl.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "import os\n",
    "from training_rl.offline_rl.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from training_rl.offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from training_rl.offline_rl.utils import delete_minari_data_if_exists\n",
    "from minari import combine_datasets\n",
    "from training_rl.offline_rl.load_env_variables import load_env_variables\n",
    "\n",
    "load_env_variables()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ToDo: this should be load automatically\n",
    "register_grid_envs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restore our combined dataset of our previous notebook made of data collected from an optimal policy moving the agent from (4,0) to the target (7,7) and the other one given by a suboptimal stochastival trajectory bringing the agent from (0,0) to (7,0).\n",
    "\n",
    "Our task is to create an optimal polica that be able to move the agent from (0,0) to (7,7). This was impossible for DQN as we saw before but as we saw in the theory BCQ and CQL are able to use the data in a much more efficient way so let's see if this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's restore the data and the environment from our previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicyType.behavior_8x8_moves_downwards_within_strip\n",
    "NUM_STEPS_I = 3000\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "#BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_deterministic_4_0_to_7_7\n",
    "NUM_STEPS_II = 1000\n",
    "\n",
    "OFFLINE_POLICY = PolicyName.dqn\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.verical_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (7, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "env_2D_grid_initial_config.initial_state=(4,0)\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_minari_config\n",
    "\n",
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER_I = \"_move_downwards\"\n",
    "DATA_SET_IDENTIFIER_II = \"_move_deterministic\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "\n",
    "# Create metadata config for set I\n",
    "minari_dataset_config_I = create_minari_config(\n",
    "env_name=ENV_NAME,\n",
    "dataset_name=DATA_SET_NAME,\n",
    "data_set_identifier=DATA_SET_IDENTIFIER_I,\n",
    "version_dataset=VERSION_DATA_SET,\n",
    "num_steps=NUM_STEPS_I,\n",
    "behavior_policy_name=BEHAVIOR_POLICY_I,\n",
    "env_2d_grid_initial_config=env_2D_grid_initial_config\n",
    ")\n",
    "\n",
    "# Create metadata config for set II\n",
    "minari_dataset_config_II = create_minari_config(\n",
    "env_name=ENV_NAME,\n",
    "dataset_name=DATA_SET_NAME,\n",
    "data_set_identifier=DATA_SET_IDENTIFIER_II,\n",
    "version_dataset=VERSION_DATA_SET,\n",
    "num_steps=NUM_STEPS_II,\n",
    "behavior_policy_name=BEHAVIOR_POLICY_II,\n",
    "env_2d_grid_initial_config=env_2D_grid_initial_config\n",
    ")\n",
    "\n",
    "create_minari_datasets(minari_dataset_config_I)\n",
    "create_minari_datasets(minari_dataset_config_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_rl.offline_rl.generate_custom_minari_datasets.utils import generate_compatible_minari_dataset_name\n",
    "\n",
    "MINARI_DATASET_I = \"Grid_2D_8x8_discrete-data_verical_object_8x8_start_0_0_target_7_7_move_downwards-v0\"\n",
    "MINARI_DATASET_II = \"Grid_2D_8x8_discrete-data_verical_object_8x8_start_0_0_target_7_7_move_deterministic-v0\"\n",
    "NAME_COMBINED_DATASET = \"combined_data_sets_vertical_obstacle\"\n",
    "\n",
    "name_combined_dataset = generate_compatible_minari_dataset_name(\n",
    "    env_name=ENV_NAME,\n",
    "    data_set_name=NAME_COMBINED_DATASET,\n",
    "    version=\"V0\"\n",
    ")\n",
    "\n",
    "delete_minari_data_if_exists(name_combined_dataset)\n",
    "\n",
    "list_dataset_names = [MINARI_DATASET_I, MINARI_DATASET_II]\n",
    "minari_datasets = [\n",
    "    minari.load_dataset(dataset_id) for dataset_id in list_dataset_names\n",
    "]\n",
    "\n",
    "combined_dataset = combine_datasets(\n",
    "    minari_datasets, new_dataset_id=name_combined_dataset\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of episodes in dataset I:{len(minari_datasets[0])}, in dataset I:{len(minari_datasets[1])} and  \"\n",
    "      f\"in the combined dataset: {len(combined_dataset)}\")\n",
    "\n",
    "\n",
    "# Create metadata for the combined dataset (we can reuse the metadata of set I for simplicity)\n",
    "minari_combined_dataset = MinariDatasetConfig.load_from_file(MINARI_DATASET_I)\n",
    "minari_combined_dataset.num_steps = NUM_STEPS_I + NUM_STEPS_II\n",
    "minari_combined_dataset.data_set_name = name_combined_dataset\n",
    "minari_combined_dataset.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = name_combined_dataset\n",
    "\n",
    "# The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.dqn\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy_final.pth\"\n",
    "NUM_EPISODES = 10 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianshou_dev",
   "language": "python",
   "name": "tianshou_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
