{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx because it changed.\n",
      "[1/1] Cythonizing /home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "performance hint: /home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx:67:5: Exception check on 'c_warning_callback' will always require the GIL to be acquired.\n",
      "Possible solutions:\n",
      "\t1. Declare the function as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
      "\t2. Use an 'int' return type on the function to allow an error code to be returned.\n",
      "performance hint: /home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx:104:5: Exception check on 'c_error_callback' will always require the GIL to be acquired.\n",
      "Possible solutions:\n",
      "\t1. Declare the function as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.\n",
      "\t2. Use an 'int' return type on the function to allow an error code to be returned.\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    See c_warning_callback, which is the C wrapper to the user defined function\n",
      "    '''\n",
      "    global py_warning_callback\n",
      "    global mju_user_warning\n",
      "    py_warning_callback = warn\n",
      "    mju_user_warning = c_warning_callback\n",
      "                       ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "/home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx:92:23: Cannot assign type 'void (const char *) except * nogil' to 'void (*)(const char *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'void (const char *) except * nogil'.\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "    See c_warning_callback, which is the C wrapper to the user defined function\n",
      "    '''\n",
      "    global py_error_callback\n",
      "    global mju_user_error\n",
      "    py_error_callback = err_callback\n",
      "    mju_user_error = c_error_callback\n",
      "                     ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "/home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx:127:21: Cannot assign type 'void (const char *) except * nogil' to 'void (*)(const char *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to type 'void (const char *) except * nogil'.\n"
     ]
    },
    {
     "ename": "CompileError",
     "evalue": "/home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_RL_workshop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RenderMode\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_RL_workshop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_policies\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolicyName\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_RL_workshop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_trainings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_training\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m offline_training\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mfrom examples.offline_RL_workshop.offline_trainings.policy_config_data_class import TrainedPolicyConfig\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mfrom examples.offline_RL_workshop.utils import state_action_histogram\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mregister_grid_envs()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GIT_PROJECTS/Tianshou/tianshou/examples/offline_RL_workshop/offline_trainings/offline_training.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_buffer_minari\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_RL_workshop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_grid_envs\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moffline_RL_workshop\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_envs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InitialConfigCustom2DGridEnvWrapper\n",
      "File \u001b[0;32m~/Documents/GIT_PROJECTS/Tianshou/tianshou/examples/offline/utils.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, Dict\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/d4rl/__init__.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m _ERROR_MESSAGE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhand_manipulation_suite\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpointmaze\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/d4rl/locomotion/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ant\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md4rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlocomotion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m maze_env\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mregister(\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    id='antmaze-umaze-v0',\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/d4rl/locomotion/ant.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cymj, ignore_mujoco_warnings, functions, MujocoException\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m const\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmujoco_py\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmjrenderpool\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MjRenderPool\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/builder.py:504\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m__fun\n\u001b[1;32m    503\u001b[0m mujoco_path \u001b[38;5;241m=\u001b[39m discover_mujoco()\n\u001b[0;32m--> 504\u001b[0m cymj \u001b[38;5;241m=\u001b[39m \u001b[43mload_cython_ext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmujoco_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Trick to expose all mj* functions from mujoco in mujoco_py.*\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdict2\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/builder.py:110\u001b[0m, in \u001b[0;36mload_cython_ext\u001b[0;34m(mujoco_path)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImport error. Trying to rebuild mujoco_py.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m         cext_so_path \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m         mod \u001b[38;5;241m=\u001b[39m load_dynamic_ext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcymj\u001b[39m\u001b[38;5;124m'\u001b[39m, cext_so_path)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/builder.py:226\u001b[0m, in \u001b[0;36mMujocoExtensionBuilder.build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 226\u001b[0m     built_so_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     new_so_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_so_file_path()\n\u001b[1;32m    228\u001b[0m     move(built_so_file_path, new_so_file_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/builder.py:278\u001b[0m, in \u001b[0;36mLinuxCPUExtensionBuilder._build_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 278\u001b[0m     so_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# Removes absolute paths to libraries. Allows for dynamic loading.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     fix_shared_library(so_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibmujoco210.so\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibmujoco210.so\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/builder.py:239\u001b[0m, in \u001b[0;36mMujocoExtensionBuilder._build_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    235\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Distribution({\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_ext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    238\u001b[0m     })\n\u001b[0;32m--> 239\u001b[0m     dist\u001b[38;5;241m.\u001b[39mext_modules \u001b[38;5;241m=\u001b[39m \u001b[43mcythonize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     dist\u001b[38;5;241m.\u001b[39minclude_dirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    241\u001b[0m     dist\u001b[38;5;241m.\u001b[39mcmdclass \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuild_ext\u001b[39m\u001b[38;5;124m'\u001b[39m: custom_build_ext}\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/Cython/Build/Dependencies.py:1154\u001b[0m, in \u001b[0;36mcythonize\u001b[0;34m(module_list, exclude, nthreads, aliases, quiet, force, language, exclude_failures, show_all_warnings, **options)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m to_compile:\n\u001b[0;32m-> 1154\u001b[0m         \u001b[43mcythonize_one\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exclude_failures:\n\u001b[1;32m   1157\u001b[0m     failed_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/training_rl/lib/python3.11/site-packages/Cython/Build/Dependencies.py:1321\u001b[0m, in \u001b[0;36mcythonize_one\u001b[0;34m(pyx_file, c_file, fingerprint, quiet, options, raise_on_failure, embedded_metadata, full_module_name, show_all_warnings, progress)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_failures:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[0;32m-> 1321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CompileError(\u001b[38;5;28;01mNone\u001b[39;00m, pyx_file)\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(c_file):\n\u001b[1;32m   1323\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(c_file)\n",
      "\u001b[0;31mCompileError\u001b[0m: /home/ivan/anaconda3/envs/training_rl/lib/python3.11/site-packages/mujoco_py/cymj.pyx"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "from examples.offline_RL_workshop import load_env_variables\n",
    "import minari\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_registry import BehaviorPolicyType\n",
    "from examples.offline_RL_workshop.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from examples.offline_RL_workshop.custom_envs.utils import Grid2DInitialConfig\n",
    "from examples.offline_RL_workshop.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import \\\n",
    "    create_minari_datasets, MinariDatasetConfig\n",
    "from examples.offline_RL_workshop.generate_custom_minari_datasets.utils import generate_compatible_minari_dataset_name, \\\n",
    "    get_dataset_name_2D_grid\n",
    "\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import CustomEnv\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import RenderMode\n",
    "from examples.offline_RL_workshop.offline_policies.policy_registry import PolicyName\n",
    "\n",
    "\n",
    "from examples.offline_RL_workshop.offline_trainings.offline_training import offline_training\n",
    "\n",
    "'''\n",
    "from examples.offline_RL_workshop.offline_trainings.policy_config_data_class import TrainedPolicyConfig\n",
    "from examples.offline_RL_workshop.utils import state_action_histogram\n",
    "from examples.offline_RL_workshop.visualizations.utils import get_state_action_data_and_policy_grid_distributions\n",
    "from examples.offline_RL_workshop.custom_envs.utils import InitialConfigCustom2DGridEnvWrapper\n",
    "from examples.offline_RL_workshop.custom_envs.custom_envs_registration import register_grid_envs\n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "from examples.offline.utils import load_buffer_minari\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_rendering import render_rgb_frames, snapshot_env\n",
    "import cv2\n",
    "import torch\n",
    "from examples.offline_RL_workshop.offline_trainings.policy_config_data_class import get_trained_policy_path\n",
    "import os\n",
    "from examples.offline_RL_workshop.offline_trainings.restore_policy_model import restore_trained_offline_policy\n",
    "from examples.offline_RL_workshop.utils import compare_state_action_histograms\n",
    "from tianshou.data import Collector\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_rendering import behavior_policy_rendering\n",
    "from examples.offline_RL_workshop.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from examples.offline_RL_workshop.generate_custom_minari_datasets.generate_minari_dataset_grid_envs import create_minari_config_from_dict\n",
    "from examples.offline_RL_workshop.utils import delete_minari_data_if_exists\n",
    "from minari import combine_datasets\n",
    "from copy import copy\n",
    "from examples.offline_RL_workshop.behavior_policies.behavior_policy_rendering import render_rgb_frames, snapshot_env\n",
    "from examples.offline_RL_workshop.load_env_variables import load_env_variables\n",
    "\n",
    "load_env_variables()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# ToDo: this should be load automatically\n",
    "register_grid_envs()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will see what happens if we want to apply off-policy RL from a given dataset. Our policy will be a simple random walk in one dimension, that starting from (0,0) tries to reach the target at (3,7). So as we move in one dimension the the highest reward state is not present in our dataset.\n",
    "\n",
    "We will use in this example as off-policy RL policy, the Deep Q-Network (DQN) algorithm, that we introduced in the online RL section. As the DQN agent cannot interact with the environment we will feed the collected data through a ReplyBuffer, similarly as we did before in the imitation learning example.\n",
    "\n",
    "Again we will use the 8x8 grid environment but without any obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our configuration and create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = CustomEnv.Grid_2D_8x8_discrete\n",
    "BEHAVIOR_POLICY = BehaviorPolicyType.random\n",
    "NUM_COLLECTED_POINTS = 6000\n",
    "\n",
    "OFFLINE_POLICY = PolicyName.dqn\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = ObstacleTypes.horizontal_line_object_8x8\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (3, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to our environment and policy configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suboptimal policy\n",
    "behavior_policy_rendering(\n",
    "    env_or_env_name=ENV_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    behavior_policy_name=BEHAVIOR_POLICY,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as we did for imitation learning before:\n",
    "\n",
    "    1 - Collect a minari dataset\n",
    "    2 - Create a reply Tianshou buffer\n",
    "    3 - train a DQN policy on the generated dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = \"data\"\n",
    "DATA_SET_IDENTIFIER = \"_exercise_6_a\"\n",
    "VERSION_DATA_SET = \"v0\"\n",
    "\n",
    "# Create metadata config for set I\n",
    "minari_dataset_config = create_minari_config_from_dict(\n",
    "env_name=ENV_NAME,\n",
    "dataset_name=DATA_SET_NAME,\n",
    "data_set_identifier=DATA_SET_IDENTIFIER,\n",
    "version_dataset=VERSION_DATA_SET,\n",
    "num_steps=NUM_COLLECTED_POINTS,\n",
    "behavior_policy_name=BEHAVIOR_POLICY,\n",
    "env_2d_grid_initial_config=env_2D_grid_initial_config\n",
    ")\n",
    "\n",
    "create_minari_datasets(minari_dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to the state-action distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = \"Grid_2D_8x8_discrete-data_horizontal_line_object_8x8_start_0_0_target_3_7_exercise_6_a-v0\" \n",
    "\n",
    "#Create Buffers with minari datasets\n",
    "buffer_data = load_buffer_minari(NAME_EXPERT_DATA)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "state_action_histogram(state_action_count_data, title=\"State-Action data distribution\", \n",
    "                       inset_pos_xy=(0.0, -0.03))\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the DQN policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_EXPERT_DATA = NAME_EXPERT_DATA\n",
    "\n",
    "# The model policy to be trained.\n",
    "POLICY_NAME = PolicyName.dqn\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "UPDATE_PER_EPOCH = 100\n",
    "\n",
    "# After every epoch we will collect some test statistics from the policy from NUMBER_TEST_ENVS independent envs.\n",
    "NUMBER_TEST_ENVS = 1\n",
    "EXPLORATION_NOISE = True\n",
    "SEED = None #1626\n",
    "\n",
    "# TrainedPolicyConfig is a handy object that will help us to deal with the policy configuration data.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=NAME_EXPERT_DATA,\n",
    "    policy_name=POLICY_NAME,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    device=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training\n",
    "offline_training(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    update_per_epoch=UPDATE_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a look to the policy state-action distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE 2:\n",
    "\n",
    "Analyze the state-action policy distribution and try to make sense of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_FILE = \"policy_final.pth\"\n",
    "NUM_EPISODES = 10 # as more episodes the better\n",
    "\n",
    "# restore a policy with the same configuration as the one we trained.\n",
    "policy = restore_trained_offline_policy(offline_policy_config)\n",
    "# load the weights\n",
    "name_expert_data = offline_policy_config.name_expert_data\n",
    "log_name = os.path.join(name_expert_data, POLICY_NAME)\n",
    "log_path = get_trained_policy_path(log_name)\n",
    "policy.load_state_dict(torch.load(os.path.join(log_path, POLICY_FILE), map_location=\"cpu\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute statistics\n",
    "state_action_count_data, state_action_count_policy = \\\n",
    "    get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, \n",
    "    env, \n",
    "    policy, \n",
    "    num_episodes=NUM_EPISODES,\n",
    "    logits_sampling=False,\n",
    ")\n",
    "\n",
    "# plots\n",
    "#state_action_histogram(state_action_count_data)\n",
    "state_action_histogram(state_action_count_policy, inset_pos_xy=None)\n",
    "#compare_state_action_histograms(state_action_count_data, state_action_count_policy)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy.set_eps(0.01)\n",
    "#final_collector = Collector(policy, env, exploration_noise=EXPLORATION_NOISE)\n",
    "#final_collector.collect(n_episode=20,q render=1 / 35)\n",
    "\n",
    "#ToDo: Sole error in DQN visualization !!!!\n",
    "\n",
    "env_2D_grid_initial_config.initial_state=(0,0)\n",
    "env_2D_grid_initial_config.obstacles = ObstacleTypes.obst_free_8x8\n",
    "env = InitialConfigCustom2DGridEnvWrapper(gym.make(ENV_NAME, render_mode=RenderMode.RGB_ARRAY_LIST), env_config=env_2D_grid_initial_config)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY_LIST,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=1000,\n",
    "    imitation_policy_sampling=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "1 - DQN goes totally out of distribution. It start to generate action that bring the system to states not included in our dataset. Remember that in general in off-policy algorithms :\n",
    "\n",
    "$$ J (\\phi) = \\mathbb{E}_{{s,a,s'}\\sim D} [r(s, a) + \\gamma \\mathbb{E}_{a'\\sim \\pi_{\\text{off}}(\\cdot|s)} [Q_{\\pi_{\\phi}}(s', a') - Q_{\\pi_{\\phi}}(s, a)]^2 $$\n",
    "\n",
    "$a'$ won't be in the dataset and so this will move the agent to states not observed in the data with possibly negative consequences!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_rl",
   "language": "python",
   "name": "training_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
