{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of some famous offline RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore several key algorithms that aim to address distributional shift issues within offline reinforcement learning. It's worth noting that the field of offline RL is evolving rapidly, and this list is by no means exhaustive. Many of the concepts and strategies employed by these algorithms find applications and improvements in various other approaches.\n",
    "\n",
    "A common approach followed by many algorithms in offline RL involves an actor-critic methodology. Within this framework, there is an iterative process of evaluation and improvement, characterized by:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\hat Q}^{\\pi}_{k+1} \\gets \\arg \\min_Q \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q(s,a) - \\mathcal{B}^{\\pi}_k Q(s,a)\\big)^2\\Big].\n",
    "\\tag{Evaluation}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{B}^{\\pi}Q = r + {\\gamma}\\mathbb{E}_{s' \\sim D, a' \\sim \\pi}Q(s',a')\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}}_{k+1}(s, a) \\right] \\tag{Improvement}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "So the main idea is to modified the Evaluation/Improvement steps to improve the distributional shift problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Batch Constrained deep Q-learning (BCQ) algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BCQ algorithm tries to solve the problem of distributional shift, and in particular the issues mentioned before during the Q-value evaluation process, i.e.: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\pi_{k+1} \\leftarrow \\arg \\max_{\\pi} \\mathbb{E}_{s\\sim D} \\left[ \\mathbb{E}_{a \\sim\\pi(a|s)} Q^{\\hat{\\pi}}_{k+1}(s, a) \\right] \\tag{Improvement}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where out of distribution actions $a'$ could be overestimated. \n",
    "\n",
    "The BCQ algorithm propose to tackle this problem by train a Variational Autoencoder (VAE) $G_\\omega(s')$ in the given dataset in order to generate in (1) reasonable actions $a'$ (i.e. with a high likelihood that will belong to the probability distribution describing our dataset), given the state $s'$ (thus BCQ belongs to the direct constraint policy algorithms). So the idea is to generate $n$ potential actions for $s'$ and select the one that maximizes the Q-value.\n",
    "\n",
    "In order to be general enough BCQ propose to perturbate the generated actions by a perturbated actor that learns how to change the $n$ actions within a range $[-\\Phi, \\Phi]$ \n",
    "through a function $\\xi_\\phi(s, a_i, \\Phi)$ trained in order to maximize the Q function (this also makes the algorithm more optimal as we don't need to generate too many VAE samples). In other words BCQ algorithm proposes for the improvement step (The details about the evaluation are not so relevant here, but you are welcome to explore the original paper for more details):\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\pi(s) = \\arg\\max_{a_i + \\xi\\phi(s, a_i, \\Phi)} Q_\\theta(s, a_i + \\xi_\\phi(s, a_i, \\Phi)), \n",
    "\\\\ \\{a_i \\sim G_\\omega(s)\\}_{i=1}^n \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi \\leftarrow \\underset{\\phi}{\\arg\\max} \\sum_{a \\sim G_{\\omega}(s)} Q_{\\theta_1}(s, a + \\xi_\\phi(s, a, \\Phi))\n",
    "$$\n",
    "\n",
    "Note that if $\\Phi=0$ and $n=1$ the policy will resemble behavioral clonning.\n",
    "On the oposite side if d $\\Phi \\rightarrow a_{max} - a_{min}$ and $n \\rightarrow \\infty$, then the algorithm approaches Q-learning, as the policy begins to greedily maximize the value function over the entire action space.\n",
    "\n",
    "\n",
    "## Discrete BCQ: Short descritpion -> Simplifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**: As it learns how to generates new actions not include in the dataset \n",
    "it is suitable for small datasets and for unbalanced sets where a few unrepresented actions\n",
    "could be important for the task to be solved. \n",
    "\n",
    "**cons**: As BCQ generated action from a VAE, if the dataset used to trian it underrepresent some important actions it could be that the VAE is not able to generate meaningful actions around that state and so the discover of new or unconventional actions could be hard. This is one of the limitation of contrained policy approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo: Exercises where BCQ beats DQN!!!\n",
    "\n",
    "BCQ for realistic case!!! --> MINARI robotic hand manipulation maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conservative Q-Learning (CQL) algorithm\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CQL follows a pesimitic approach by considering a lower bound of the Q-value. In the paper they show that the solution of:\n",
    "\n",
    "$\\hat{Q}^{k+1}_{\\text{CQL}} \\gets \\hbox{argmin}_Q \\left[ \\color{red} {\\alpha\\big(\\mathbb{E}_{s \\sim \\mathcal{D}, a \\sim \\mu}[Q(s,a)] - \\mathbb{E}_{s,a \\sim \\mathcal{D}}[Q(s,a)]\\big)} + \\frac{1}{2} \\mathbb{E}_{s,a \\sim \\mathcal{D}} \\Big[\\big(Q(s,a) - \\mathcal{B}^{\\pi}Q(s,a)\\big)^2\\Big] \\right].$\n",
    "\n",
    "for $\\mu = \\pi$ is a lower bound for the Q value. \n",
    "\n",
    "The nice thing about this method is that it can be applied to any Actor Critic method in a few lines of code.  \n",
    "\n",
    "CQL Focuses on **conservative value estimation** to provide lower bounds on the expected return of a policy. Aims to reduce overestimation bias and ensure that the policy remains within a safe region of the state-action space. Achieves safe exploration by constructing action sets that cover a broader range of state-action pairs. Well-suited for scenarios where safety is a top priority, as it **reduces the risk of catastrophic actions**.\n",
    "\n",
    "Note that BCQ could be better to discover novel actions an maybe uses the collected data more efficiently but may not guarantee complete safety!.\n",
    "\n",
    "Link to excercise!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "IQL\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
